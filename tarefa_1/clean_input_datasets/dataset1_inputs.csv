ID	Text
D1-1	The cell cycle, or cell-division cycle, is the sequential series of events that take place in a cell that causes it to divide into two daughter cells. These events include the growth of the cell, duplication of its DNA (DNA replication) and some of its organelles, and subsequently the partitioning of its cytoplasm, chromosomes and other components into two daughter cells in a process called cell division. In eukaryotic cells (having a cell nucleus) including animal, plant, fungal, and protist cells, the cell cycle is divided into two main stages: interphase, and the M phase that includes mitosis and cytokinesis.
D1-2	The cell cycle is the process by which a cell grows, duplicates its DNA, and divides into two daughter cells. It is essential for growth, tissue repair, and reproduction. The cycle consists of four main phases. In G₁ phase, the cell grows, produces proteins, and prepares for DNA replication. During the S phase, DNA is replicated to ensure that each daughter cell receives an identical copy. The G2 phase follows, where the cell continues growing and checks for DNA damage before proceeding to division. Finally, in the M phase, the cell undergoes mitosis, where chromosomes are separated, and cytokinesis, where the cytoplasm splits.
D1-3	Photons, in many atomic models in physics, are particles which transmit light. In other words, light is carried over space by photons. Photon is an elementary particle that is its own antiparticle. In quantum mechanics each photon has a characteristic quantum of energy that depends on frequency: A photon associated with light at a higher frequency will have more energy (and be associated with light at a shorter wavelength).Photons have a rest mass of 0 (zero). However, Einstein's theory of relativity says that they do have a certain amount of momentum. Before the photon got its name, Einstein revived the proposal that light is separate pieces of energy (particles). These particles came to be known as photons.
D1-4	A photon is a fundamental particle of light and other electromagnetic radiation. It has no mass, no electric charge, and always moves at the speed of light (299,792,458 m/s in a vacuum). Photons carry energy and momentum, which depend on their wavelength or frequency. Higher frequency photons (like X-rays) have more energy, while lower frequency photons (like radio waves) have less. Their energy is given by Planck’s equation:E = h f, where E is energy, h is Planck’s constant, and f is frequency.Photons behave as both particles and waves (wave-particle duality), meaning they can interfere, diffract, and be absorbed/emitted like particles. They are responsible for vision, photosynthesis, solar power, and many quantum phenomena.
D1-5	According to the theory of plate tectonics, Earth's lithosphere, its rigid outer shell, is broken into sixteen larger and several smaller plates. These move continuously at a slow pace, due to convection in the underlying ductile mantle, and most volcanic activity on Earth takes place along plate boundaries, where plates are converging (and lithosphere is being destroyed) or are diverging (and new lithosphere is being created). During the development of geological theory, certain concepts that allowed the grouping of volcanoes in time, place, structure and composition have developed that ultimately have had to be explained in the theory of plate tectonics.
D1-6	The theory of plate tectonics explains that Earth’s lithosphere is divided into moving plates that float on the semi-fluid asthenosphere. These movements shape the planet, causing earthquakes, volcanic activity, and mountain formation. The theory builds on continental drift, proposed by Alfred Wegener, and is supported by evidence from seafloor spreading, fossil distribution, and geological formations. Plates move apart at divergent boundaries, collide at convergent boundaries, and slide past each other at transform boundaries. Their movement is driven by mantle convection, gravity, and Earth’s rotation, constantly reshaping the surface. This theory is essential for understanding natural disasters and the geological evolution of continents.
D1-7	Thalidomide is a pharmaceutical drug, first prepared in 1957 in Germany, prescribed for treating morning sickness in pregnant women. The drug was discovered to be teratogenic, causing serious genetic damage to early embryonic growth and development, leading to limb deformation in babies. Several proposed mechanisms of teratogenicity involve different biological functions for the (R)- and (S)-thalidomide enantiomers.In the human body, however, thalidomide undergoes racemization: even if only one of the two enantiomers is administered as a drug, the other enantiomer is produced as a result of metabolism. Thalidomide is currently used for the treatment of other diseases, notably cancer and leprosy. Strict regulations and controls have been implemented to avoid its use by pregnant women and prevent developmental deformities.
D1-8	Thalidomide is a drug that was first developed in the 1950s as a sedative and later prescribed to pregnant women for morning sickness. However, it caused severe birth defects, including missing or malformed limbs, when taken during pregnancy. The tragedy affected thousands of babies and led to stricter drug regulations worldwide. Despite its harmful past, thalidomide was later found to have anti-inflammatory and immunomodulatory properties. Today, it is used under strict controls to treat multiple myeloma, leprosy-related inflammation, and some autoimmune diseases. Due to its risks, it is only prescribed under a controlled program to prevent use during pregnancy. Its history remains a major lesson in pharmaceutical safety.
D1-9	Kepler published his first two laws about planetary motion in 1609, having found them by analyzing the astronomical observations of Tycho Brahe.Kepler's third law was published in 1619. Kepler had believed in the Copernican model of the Solar System, which called for circular orbits, but he could not reconcile Brahe's highly precise observations with a circular fit to Mars' orbit – Mars coincidentally having the highest eccentricity of all planets except Mercury. His first law reflected this discovery. In 1621, Kepler noted that his third law applies to the four brightest moons of Jupiter. Godefroy Wendelin also made this observation in 1643.
D1-10	Kepler’s laws of planetary motion are three fundamental principles that describe the motion of planets around the Sun. These laws were formulated by the German astronomer Johannes Kepler in the early 17th century, building on the detailed observations of the astronomer Tycho Brahe. Kepler’s work came at a time when the heliocentric model (Sun-centered solar system) proposed by Copernicus was still controversial. Kepler initially struggled with the Copernican model, but after inheriting Brahe’s precise astronomical data, he was able to make groundbreaking discoveries. Kepler’s first law, the Law of Ellipses, stated that planets move in elliptical orbits with the Sun at one focus. The second, the Law of Equal Areas, explained that planets sweep out equal areas in equal times.
D1-11	"The basic idea of biological evolution is that populations and species of organisms change over time. Today, when we think of evolution, we are likely to link this idea with one specific person: the British naturalist Charles Darwin. In the 1850s, Darwin wrote an influential and controversial book called On the Origin of Species. In it, he proposed that species evolve (or, as he put it, undergo ""descent with modification""), and that all living things can trace their descent to a common ancestor. Darwin also suggested a mechanism for evolution: natural selection, in which heritable traits that help organisms survive and reproduce become more common in a population over time."
D1-12	Biological evolution, according to Charles Darwin, refers to the process by which species of organisms change over time through variations in traits that are passed down from one generation to the next. Darwin’s theory of evolution by natural selection suggests that individuals within a species show variation in their characteristics. These variations can affect their ability to survive and reproduce in their environment. Those individuals with traits that give them a survival or reproductive advantage are more likely to pass those traits on to their offspring, while less advantageous traits are gradually eliminated. Over long periods, this process leads to the accumulation of beneficial traits in the population, resulting in the adaptation of species to their environment.
D1-13	Stoichiometry rests upon the very basic laws that help to understand it better, i.e., law of conservation of mass, the law of definite proportions (i.e., the law of constant composition), the law of multiple proportions and the law of reciprocal proportions. In general, chemical reactions combine in definite ratios of chemicals. Since chemical reactions can neither create nor destroy matter, nor transmute one element into another, the amount of each element must be the same throughout the overall reaction. For example, the number of atoms of a given element X on the reactant side must equal the number of atoms of that element on the product side, whether or not all of those atoms are actually involved in a reaction.
D1-14	Stoichiometry is the branch of chemistry that deals with the calculation of reactants and products in chemical reactions. It is based on the concept that matter is conserved during a reaction, meaning the quantity of reactants used equals the quantity of products formed. Stoichiometry involves using balanced chemical equations to determine the molar ratios between substances, allowing the calculation of how much of each reactant is needed and how much product will be formed. This includes conversions between moles, mass, and volume. Stoichiometric calculations are essential for understanding reaction yields, determining the limiting reactant, and ensuring efficient use of materials in chemical processes.
D1-15	General relativity is a theory of gravitation developed by Einstein in the years 1907–1915. The development of general relativity began with the equivalence principle, under which the states of accelerated motion and being at rest in a gravitational field (for example, when standing on the surface of the Earth) are physically identical. The upshot of this is that free fall is inertial motion: an object in free fall is falling because that is how objects move when there is no force being exerted on them, instead of this being due to the force of gravity as is the case in classical mechanics.
D1-16	General relativity, also known as the general theory of relativity, is a geometric theory of gravitation developed by Albert Einstein between 1907 and 1915. It provides a unified description of gravity as a property of spacetime, which is a four-dimensional continuum combining space and time. The theory posits that massive objects cause a curvature in spacetime, and this curvature influences the motion of objects, effectively manifesting as gravity. Einstein developed general relativity to address limitations in Newton's law of universal gravitation, particularly in explaining anomalies like the precession of Mercury's orbit. The theory has been extensively tested and confirmed through observations of gravitational waves, the bending of light around massive objects, and the behavior of objects in strong gravitational fields
D1-17	The recommended number of eggs to consume per week varies depending on several factors, including individual health, dietary preferences, and overall dietary patterns. It's important to note that while eggs are a nutritious food, they are also relatively high in dietary cholesterol. According to general guidelines, including those provided by the American Heart Association and the Dietary Guidelines for Americans, consuming up to seven eggs per week is considered reasonable and can be part of a healthy diet for most people. However, it's important to consider the overall context of your diet, including other sources of cholesterol and saturated fat, as well as your individual health goals and any specific dietary restrictions or considerations you may have.
D1-18	The question of how many eggs one should eat per day is often debated due to concerns about cholesterol and cardiovascular health. Current research suggests that consuming up to three eggs per day can be part of a healthy diet for most people, particularly young, healthy adults. Consuming up to three eggs per day has been shown to increase HDL (good) cholesterol and improve the LDL/HDL ratio, which are favorable changes for cardiovascular health. For most healthy individuals, consuming up to three eggs per day can be beneficial, improving cholesterol profiles and providing essential nutrients without increasing cardiovascular risk. However, individuals with specific health conditions, such as cardiovascular disease or diabetes, should consider their overall dietary patterns.
D1-19	There seems to be some merit to having a single daily glass of red wine, it seems to have something called resveratrol, which acts as an antioxidant and is thought to be good for your heart. Additionally, Italian researchers found that moderate beer drinkers had a 42 percent lower risk of heart disease compared to non-drinkers. For maximum protection, keep your consumption to one pint—at around 5 percent alcohol by volume—a day, the researchers say. Basically, alcohol, in moderation, appears to lower risks of heart and other cardiovascular disease.But this is in moderation, mind you. Just as with everything else, the only difference between medicine and poison is the dosage. A beer a day is good; twelve is not.
D1-20	No level of alcohol consumption is considered entirely safe for health. According to the World Health Organization (WHO), even low levels of alcohol use can increase the risk of certain cancers and other health problems. While some guidelines suggest limits to reduce risks, these guidelines are based on minimizing risk rather than ensuring safety. Recent research indicates that even moderate drinking may increase the risk of chronic diseases and premature death compared to abstaining. Therefore, the safest approach is to avoid alcohol altogether. Alcohol consumption significantly impacts mental health, often exacerbating existing conditions and contributing to new ones. Alcohol initially acts as a depressant, reducing inhibitions and creating a temporary sense of relaxation or confidence.
D1-21	There are three major compounds of life: proteins; lipids and carbohydrates. The perception of sweet taste, mainly associated with advantageous food, has had an important evolutionary influence on different physiological regulation mechanisms. During human development, sugar was always luxury. In 1885 Constantin Fahlberg produced the first artificial sweetener, saccharin, and the scientific establishment was surprised by its extreme sweetness. Significant to this discovery was the fact that sweet taste became affordable to poor people. Following the commercial success of artificial sweeteners, a battle between the sugar and sweetener industries began. Saccharin was claimed to be carcinogenic in rats. However, it was later shown that saccharin is neither toxic nor carcinogenic in normal amounts, yet its reputation remains tarnished.
D1-22	Artificial sweeteners are low- or zero-calorie sugar substitutes used to sweeten foods and beverages without the added calories of sugar. Common examples include aspartame, sucralose, saccharin, and stevia (a natural alternative). They are widely used in diet sodas, sugar-free gum, and diabetic-friendly products. While they provide sweetness without spiking blood sugar levels, they remain controversial. Studies show they are safe in moderation, but concerns exist about potential long-term effects on gut health and metabolism. Some people experience sensitivity to specific sweeteners. Ultimately, artificial sweeteners can be helpful for reducing calorie intake, but their impact varies widely from person to person.
D1-23	Recent findings on the ecology, etiology and pathology of coral pathogens, host resistance mechanisms, previously unknown disease/syndromes and the global nature of coral reef diseases have increased our concern about the health and future of coral reef communities. Much of what has been discovered in the past 4 years is presented in this special issue. Among the significant findings, the role that various Vibrio species play in coral disease and health, the composition of the ‘normal microbiota’ of corals, and the possible role of viruses in the disease process are important additions to our knowledge.  reefs and a major cause of reef deterioration.
D1-24	Coral reefs are among the most biodiverse and vital ecosystems on the planet, supporting millions of marine species and providing essential services like coastal protection and livelihoods for humans. However, their health is rapidly declining due to climate change, ocean acidification, overfishing, and pollution. Rising sea temperatures cause coral bleaching, where corals expel the algae they depend on for energy, often leading to mass die-offs. Ocean acidification, driven by increased CO₂ absorption, weakens coral skeletons, further threatening reef stability. The future of coral reefs depends on urgent conservation efforts. Strategies like reducing carbon emissions, establishing marine protected areas, and coral restoration projects (e.g., coral farming) offer hope. Research into resilient coral species and reef rehabilitation techniques is also promising.
D1-25	Although the earliest life seems to have arisen quite soon in Earth’s history, less than 800 million years after its formation, simple forms continued for 1.6 billion years until the multicellular Francevillian biota developed and quickly died out about 2.1 billion years ago. After that, although multicellular forms were in existence from about 1.7 billion years ago, it was not until the Cambrian ‘explosion’ from 540–500 million years ago that multicellular life expanded and prospered. This is essentially what we know. But fossilisation is rare and early life has resulted in tiny traces  difficult to interpret. At present there are believed to be about one trillion species on Earth, less than one per cent of those which have ever lived.
D1-26	"The question of how life appeared on Earth is one of science’s greatest mysteries. While there is no definitive answer, scientists propose various theories based on evidence and hypotheses.VPrimordial Soup Theory: This idea suggests that life began in a ""soup"" of organic molecules in early Earth's oceans. Energy from lightning, volcanic activity, or UV radiation may have triggered chemical reactions, forming basic building blocks of life like amino acids and nucleotides. Experiments like the 1953 Miller-Urey experiment support this theory by showing that organic molecules can form under simulated early Earth conditions. Hydrothermal Vent Hypothesis: Some researchers believe life began near deep-sea hydrothermal vents, where hot, mineral-rich water provided the energy and raw materials for life."
D1-27	"We investigate the problem about the reason for the significant sub-centurial (50-55 and 60-65 yr) and quasi-centurial (120-130 yr) climatic oscillations. The 50-55 and 60-65 yr cycles are clearly detectable in numerous global and regional climatic parameters, i.e. in the temperatures of the Northern Hemisphere and World Ocean surface, in the tree rings widths, in the atmosphere concentration of CO2, etc. Searching for analogues of these cycles in the solar activity we study the connections between various types of solar, geophysical and climatic cycles. In this Paper I we analyze time series of residual variations of the Northern Hemisphere (AD 1610-1979) and World Ocean surface (AD 18561995) temperatures in respect to the corresponding regression models ""sunspot activity – temperature""."
D1-28	Significant climatic oscillations on sub-centurial (50–55 years, 60–65 years) and quasi-centurial (120–130 years) timescales are thought to arise from natural variability in Earth's climate system, driven by a combination of internal processes and external forcing mechanisms. The causes of these oscillations include: Ocean-Atmosphere Interactions Atlantic Multidecadal Oscillation (AMO) and Pacific Decadal Oscillation (PDO) are large-scale oceanic patterns of variability that operate on multidecadal timescales (50–70 years). These affect global heat distribution, weather patterns, and long-term climate trends. Solar Variability Variations in solar activity, such as the Gleissberg Cycle (~80–120 years), impact Earth's climate. Changes in solar irradiance affect atmospheric dynamics, ocean circulation, and surface temperatures.
D1-29	The COVID-19 was detected in Wuhan, central China’s Hubei Province, but that doesn’t mean the novel coronavirus had originated from China. It may have originated from somewhere else. To determine the origin of an unknown virus requires scientific, meticulous research, instead of random speculation. The source of the virus is a scientific issue and requires a professional investigation. Without evidence supported by data or research, no one can claim where it had originated from. Connecting a virus with a region may cause xenophobic behavior. The pandemic influenza of 2009 originated in North America, but it was not called the “North America flu.” That is why the World Health Organization named the novel coronavirus “COVID-19,” which avoids stigmatizing a particularly region.
D1-30	The origin of the COVID-19 pandemic, caused by SARS-CoV-2, remains uncertain, with two main theories under discussion. The dominant hypothesis suggests a natural origin through zoonotic spillover, where the virus jumped from bats, its likely reservoir, to humans, possibly via an intermediate species such as pangolins. This theory links the early outbreak to a seafood market in Wuhan, China, where live wild animals were sold. Another theory suggests an accidental leak from a laboratory, like the Wuhan Institute of Virology, which studies bat coronaviruses, though no direct evidence supports this claim. While most scientists favor the natural origin hypothesis, limited data and restricted early access make the exact source of the virus difficult to confirm.
D1-31	An experimental demonstration of a novel all-optical technique for loading ion traps, that has particular application to microtrap architectures, is presented. The technique is based on photo-ionisation of an atomic beam created by pulsed laser ablation of a calcium target, and provides improved temporal control compared to traditional trap loading methods. Ion loading rates as high as 125 ions per second have so far been observed. Also described are observations of trap loading where Rydberg state atoms are photo-ionised by the ion Doppler cooling laser.
D1-32	This research paper presents an all-optical ion-loading technique for scalable microtrap architectures. The technique utilizes a combination of optical tweezers and laser cooling to trap and cool individual ions, which can then be loaded into a microtrap. This approach eliminates the need for complex and expensive ion-loading mechanisms, allowing for the creation of more compact and scalable microtraps. The paper also explores the performance of the microtrap using this technique, demonstrating its potential for use in a variety of applications in quantum computing and precision measurement.
D1-33	We demonstrate that bistability of the nuclear spin polarization in optically pumped semiconductor quantum dots is a general phenomenon possible in dots with a wide range of parameters. In experiment, this bistability manifests itself via the hysteresis behavior of the electron Zeeman splitting as a function of either pump power or external magnetic field. In addition, our theory predicts that the nuclear polarization can strongly influence the charge dynamics in the dot leading to bistability in the average dot charge.
D1-34	"The research paper titled ""Bistability of optically-induced nuclear spin orientation in quantum dots"" investigates the behavior of nuclear spins in quantum dots under various optical and magnetic conditions. The primary subject of the study is the bistability of nuclear spin orientation, which refers to the ability of the system to exist in two stable states with different spin orientations. The researchers found that by controlling the optical and magnetic fields, they could induce and manipulate the bistability of nuclear spin orientation in the quantum dots. They also discovered that the duration of the spin polarization was dependent on the strength of the magnetic field. The study's main conclusion is that the bistability of nuclear spin orientation in quantum dots can be controlled and utilized for various practical applications, such as quantum information processing and spin-based electronics."
D1-35	We investigate some fundamental features of a class of non-linear relativistic lagrangian field theories with kinetic self-coupling. We focus our attention upon theories admitting static, spherically symmetric solutions in three space dimensions which are finite-energy and stable. We determine general conditions for the existence and stability of these non-topological soliton solutions. In particular, we perform a linear stability analysis that goes beyond the usual Derrick-like criteria. On the basis of these considerations we obtain a complete characterization of the soliton-supporting members of the aforementioned class of non-linear field theories. We then classify the family of soliton-supporting theories according to the central and asymptotic behaviors of the soliton field, and provide illustrative explicit examples of models belonging to each of the corresponding sub-families. In the present work we restrict most of our considerations to one and many-components scalar models. We show that in these cases the finite-energy static spherically symmetric solutions are stable against charge-preserving perturbations, provided that the vacuum energy of the model vanishes and the energy density is positive definite. We also discuss briefly the extension of the present approach to models involving other types of fields, but a detailed study of this more general scenario will be addressed in a separate publication.
D1-36	This research paper investigates the existence and properties of non-topological solitons in field theories with kinetic self-coupling. Such solitons are localized, stable configurations of the field that do not rely on topological considerations for their stability. The study is conducted by solving the equations of motion for the field and analyzing the resulting solutions. The paper explores the conditions under which non-topological solitons can arise and examines their properties, such as their energy, size, and stability. The results of this research can have significant implications for various fields of physics, including particle physics and condensed matter physics.
D1-37	The Galactic center (GC) is a dense and chaotic region filled with unusual sources, such as intense star forming regions, dense star clusters, nonthermal radio filaments, and a massive black hole. The proximity of the GC region makes it an ideal place to study the unusual processes that tend to manifest themselves in Galactic nuclei. This thesis uses single-dish and interferometric radio continuum, radio recombination line, polarized radio continuum, and mid-IR observations to study the wide variety of physical processes seen in the GC region on physical scales from 0.1 to 100 parsecs. These observations provide one of the most sensitive studies of the radio continuum emission in the central 500 parsecs. I also study the properties of nonthermal radio filaments, which can constrain their origin and the structure of the magnetic field in the GC region. The presence of massive star clusters and a massive black hole suggest that starburst and AGN phenomena can manifest themselves in our Galaxy. This thesis explores this possibility by studying a 150-pc-tall, shell-like structure called the Galactic center lobe (GCL). Our observations examine the spectral index, dust emission, polarized continuum emission, and ionized gas throughout the GCL. I find strong evidence supporting the idea that the GCL is a true three-dimensional shell located in the GC region with nested layers of ionized, magnetized, and mid-IR--emitting components. I compare the physical conditions of the GCL to proposed models for its origin and find best agreement with starburst outflows seen in other galaxies, yet consistent with the currently observed pressure and star formation rate in the central tens of parsecs of our Galaxy. (abridged)
D1-38	This research paper provides an overview of recent surveys of the Galactic Center and their implications for understanding the nature of the Galactic Center lobe. The paper highlights key discoveries related to the morphology, kinematics, and energetics of the lobe, and discusses their relevance for models of the Milky Way's central region. The authors conclude that the lobe is likely a result of past accretion events and feedback from the central black hole, and suggest that future observations will help to refine our understanding of its properties and evolution.
D1-39	This document reports on a series of experimental and theoretical studies conducted to assess the astro-particle physics potential of three future large-scale particle detectors proposed in Europe as next generation underground observatories. The proposed apparatus employ three different and, to some extent, complementary detection techniques: GLACIER (liquid Argon TPC), LENA (liquid scintillator) and MEMPHYS (\WC), based on the use of large mass of liquids as active detection media. The results of these studies are presented along with a critical discussion of the performance attainable by the three proposed approaches coupled to existing or planned underground laboratories, in relation to open and outstanding physics issues such as the search for matter instability, the detection of astrophysical- and geo-neutrinos and to the possible use of these detectors in future high-intensity neutrino beams.
D1-40	This research paper presents the scientific case and prospects for large underground, liquid-based detectors for astro-particle physics in Europe. The paper discusses the potential for these detectors to make groundbreaking discoveries in the fields of neutrino and dark matter research, as well as their ability to contribute to the study of supernovae, solar neutrinos, and geoneutrinos. The authors conclude that these detectors are crucial for advancing our understanding of the universe and recommend continued investment in their development and operation.
D1-41	In this paper, the equations of motion for geodesics in the neutral rotating Black Ring metric are derived and the separability of these equations is considered. The bulk of the paper is concerned with sets of solutions where the geodesic equations can be examined analytically - specifically geodesics confined to the axis of rotation, geodesics restricted to the equatorial plane, and geodesics that circle through the centre of the ring. The geodesics on the rotational axis behave like a particle in a potential well, while the geodesics confined to the equatorial plane mimic those of the Schwarzschild metric. It is shown that it is impossible to have circular orbits that pass through the ring, but some numerical results are presented which suggest that it is possible to have bound orbits that circle through the ring.
D1-42	This paper investigates particle motion in the rotating black ring metric, a solution to the equations of General Relativity describing a five-dimensional black hole with a ring topology around its event horizon. The authors derive the equations of motion for test particles moving in this metric and analyze their behavior in various regimes, including geodesic motion, circular orbits, and chaotic trajectories. They find that the black ring metric exhibits several interesting features, such as the existence of stable circular orbits in the plane of rotation and the appearance of chaotic behavior for particles with nonzero angular momentum. These findings provide new insights into the dynamics of black hole spacetimes and may have implications for astrophysical phenomena such as accretion disks and binary black hole mergers.
D1-43	A recent radio survey of globular clusters has increased the number of millisecond pulsars drastically. M28 is now the globular cluster with the third largest population of known pulsars, after Terzan 5 and 47 Tuc. This prompted us to revisit the archival Chandra data on M28 to evaluate whether the newly discovered millisecond pulsars find a counterpart among the various X-ray sources detected in M28 previously. The radio position of PSR J1824-2452H is found to be in agreement with the position of CXC 182431-245217 while some faint unresolved X-ray emission near to the center of M28 is found to be coincident with the millisecond pulsars PSR J1824-2452G, J1824-2452J, J1824-2452I and J1824-2452E.
D1-44	We reconstruct a rational Lax matrix of size R+1 from its spectral curve (the desingularization of the characteristic polynomial) and some additional data. Using a twisted Cauchy--like kernel (a bi-differential of bi-weight (1-nu,nu)) we provide a residue-formula for the entries of the Lax matrix in terms of bases of dual differentials of weights nu and 1-nu respectively. All objects are described in the most explicit terms using Theta functions. Via a sequence of ``elementary twists'', we construct sequences of Lax matrices sharing the same spectral curve and polar structure and related by conjugations by rational matrices. Particular choices of elementary twists lead to construction of sequences of Lax matrices related to finite--band recurrence relations (i.e. difference operators) sharing the same shape. Recurrences of this kind are satisfied by several types of orthogonal and biorthogonal polynomials. The relevance of formulae obtained to the study of the large degree asymptotics for these polynomials is indicated.
D1-45	"The paper ""Effective inverse spectral problem for rational Lax matrices and applications"" presents a new method for solving the inverse spectral problem for rational Lax matrices. The method is based on the theory of algebraic curves and uses a combination of geometric and analytical techniques. The authors apply this method to two important problems in mathematical physics: the nonlinear Schr dinger equation and the Korteweg-de Vries equation. They show that the new method provides an effective and efficient way to solve these problems, and that it can be used to obtain new insights into the behavior of these equations. The main findings of the paper include a new formula for the spectral curve of the rational Lax matrix, a new approach to computing the Baker-Akhiezer function for the nonlinear Schr dinger equation, and a new method for finding the soliton solutions of the Korteweg-de Vries equation. The authors conclude that the new method has the potential to become a powerful tool for solving a wide range of problems in mathematical physics."
D1-46	In this paper, the bit energy requirements of training-based transmission over block Rayleigh fading channels are studied. Pilot signals are employed to obtain the minimum mean-square-error (MMSE) estimate of the channel fading coefficients. Energy efficiency is analyzed in the worst case scenario where the channel estimate is assumed to be perfect and the error in the estimate is considered as another source of additive Gaussian noise. It is shown that bit energy requirement grows without bound as the snr goes to zero, and the minimum bit energy is achieved at a nonzero snr value below which one should not operate. The effect of the block length on both the minimum bit energy and the snr value at which the minimum is achieved is investigated. Flash training schemes are analyzed and shown to improve the energy efficiency in the low-snr regime. Energy efficiency analysis is also carried out when peak power constraints are imposed on pilot signals.
D1-47	"The research paper titled ""An Energy Efficiency Perspective on Training for Fading Channels"" focuses on the tradeoff between energy efficiency and channel estimation accuracy in wireless communication systems. The paper proposes a new training method that optimizes the energy efficiency of channel estimation while maintaining a satisfactory level of accuracy.The key findings of the research are that traditional training methods consume a significant amount of energy and do not necessarily result in the highest accuracy. The proposed method reduces energy consumption by adjusting the number of pilot symbols transmitted, while maintaining an acceptable level of accuracy.The paper concludes that the proposed method can significantly improve the energy efficiency of wireless communication systems, especially in scenarios where energy is a limiting factor. It also highlights the importance of considering energy efficiency in the design of wireless communication systems."
D1-48	The low-snr capacity of M-ary PSK transmission over both the additive white Gaussian noise (AWGN) and fading channels is analyzed when hard-decision detection is employed at the receiver. Closed-form expressions for the first and second derivatives of the capacity at zero snr are obtained. The spectral-efficiency/bit-energy tradeoff in the low-snr regime is analyzed by finding the wideband slope and the bit energy required at zero spectral efficiency. Practical design guidelines are drawn from the information-theoretic analysis. The fading channel analysis is conducted for both coherent and noncoherent cases, and the performance penalty in the low-power regime for not knowing the channel is identified.
D1-49	In this paper, pilot-assisted transmission over Gauss-Markov Rayleigh fading channels is considered. A simple scenario, where a single pilot signal is transmitted every T symbols and T-1 data symbols are transmitted in between the pilots, is studied. First, it is assumed that binary phase-shift keying (BPSK) modulation is employed at the transmitter. With this assumption, the training period, and data and training power allocation are jointly optimized by maximizing an achievable rate expression. Achievable rates and energy-per-bit requirements are computed using the optimal training parameters. Secondly, a capacity lower bound is obtained by considering the error in the estimate as another source of additive Gaussian noise, and the training parameters are optimized by maximizing this lower bound.
D1-50	This research paper focuses on optimizing training sequences for Gauss-Markov Rayleigh fading channels. The study proposes a new algorithm for designing training sequences based on the maximum likelihood estimation of channel parameters. The algorithm is compared with existing methods and is shown to provide better performance in terms of channel estimation accuracy. The study concludes that the proposed algorithm can significantly enhance the performance of wireless communication systems operating in fading channels.
D1-51	In this paper, the error performance of on-off frequency shift keying (OOFSK) modulation over fading channels is analyzed when the receiver is equipped with multiple antennas. The analysis is conducted in two cases: the coherent scenario where the fading is perfectly known at the receiver, and the noncoherent scenario where neither the receiver nor the transmitter knows the fading coefficients. For both cases, the maximum a posteriori probability (MAP) detection rule is derived and analytical probability of error expressions are obtained. The effect of fading correlation among the receiver antennas is also studied. Simulation results indicate that for sufficiently low duty cycle values, lower probability of error values with respect to FSK signaling are achieved. Equivalently, when compared to FSK modulation, OOFSK with low duty cycle requires less energy to achieve the same probability of error, which renders this modulation a more energy efficient transmission technique.
D1-52	This research paper presents a performance analysis for multichannel reception of Orthogonal Offset Frequency Shift Keying (OOFSK) signaling. The main theme of the study is to investigate the impact of the number of channels on the system's performance in terms of bit error rate (BER) and throughput. The researchers conducted simulations using different numbers of channels and varying signal-to-noise ratios (SNRs). The results show that increasing the number of channels improves the system's performance, particularly at low SNRs. However, the improvement in BER saturates beyond a certain number of channels. Furthermore, the study also reveals that increasing the number of channels may not necessarily lead to an increase in throughput. The research provides valuable insights into the design and optimization of multichannel communication systems using OOFSK signaling.
D1-53	In this paper, the performance of signaling strategies with high peak-to-average power ratio is analyzed in both coherent and noncoherent fading channels. Two recently proposed modulation schemes, namely on-off binary phase-shift keying and on-off quaternary phase-shift keying, are considered. For these modulation formats, the optimal decision rules used at the detector are identified and analytical expressions for the error probabilities are obtained. Numerical techniques are employed to compute the error probabilities. It is concluded that increasing the peakedness of the signals results in reduced error rates for a given power level and hence improve the energy efficiency.
D1-54	This research paper analyses the error probability of peaky signaling over fading channels. The study considers the effects of fading on the peaky signaling, and derives the closed-form expressions for the error probability of the peaky signaling over Rayleigh and generalized fading channels. The results show that the error probability of the peaky signaling is significantly affected by the channel fading. It is also found that the performance of the peaky signaling is improved by increasing the peakiness factor. The findings of this study provide insights for the design of reliable communication systems over fading channels using peaky signaling.
D1-55	The effective Lagrangian of electromagnetic fields at the cubic order in field strength has been considered. This generalized Lagrangian is motivated by electrodynamics on non-commutative spaces. We find the canonical and symmetrical energy-momentum tensors and show that they possess non-zero traces. The propagation of a linearly polarized laser beam in the external transverse magnetic field is investigated. We evaluate the induced ellipticity which allows us to obtain a constraint on parameters introduced from the PVLAS experimental data.
D1-56	The effective Lagrangian at cubic order in electromagnetic fields and vacuum birefringence is a topic of significant interest in the field of quantum electrodynamics. This research paper aims to explore the theoretical framework for deriving the effective Lagrangian at cubic order and its implications for the phenomenon of vacuum birefringence. The paper will provide a detailed analysis of the relevant mathematical and physical concepts, including the use of Feynman diagrams and perturbative expansions. The paper will also discuss the experimental evidence for vacuum birefringence and its potential applications in testing fundamental physics theories. Overall, this research paper contributes to the ongoing efforts to understand the fundamental properties of the universe through the study of quantum electrodynamics.
D1-57	A Brownian Motor is a nanoscale or molecular device that combines the effects of thermal noise, spatial or temporal asymmetry, and directionless input energy to drive directed motion. Because of the input energy, Brownian motors function away from thermodynamic equilibrium and concepts such as linear response theory, fluctuation dissipation relations, and detailed balance do not apply. The {\em generalized} fluctuation-dissipation relation, however, states that even under strongly thermodynamically non-equilibrium conditions the ratio of the probability of a transition to the probability of the time-reverse of that transition is the exponent of the change in the internal energy of the system due to the transition. Here, we derive an extension of the generalized fluctuation dissipation theorem for a Brownian motor for the ratio between the probability for the motor to take a forward step and the probability to take a backward step.
D1-58	"The research paper titled ""Symmetry Relations for Trajectories of a Brownian Motor"" explores the properties of a Brownian motor, which is a device that can convert thermal energy into mechanical work. The authors investigate the symmetry properties of the motor's trajectory using mathematical models and simulations. The main finding of the study is that the motor's trajectory exhibits a remarkable symmetry, which is related to the underlying symmetries of the motor's structure. The authors also show that this symmetry can be broken by external perturbations, which can lead to changes in the motor's efficiency. Overall, the study provides new insights into the fundamental properties of Brownian motors and their potential applications in nanotechnology and biophysics."
D1-59	We study Casimir forces on the partition in a closed box (piston) with perfect metallic boundary conditions. Related closed geometries have generated interest as candidates for a repulsive force. By using an optical path expansion we solve exactly the case of a piston with a rectangular cross section, and find that the force always attracts the partition to the nearest base. For arbitrary cross sections, we can use an expansion for the density of states to compute the force in the limit of small height to width ratios. The corrections to the force between parallel plates are found to have interesting dependence on the shape of the cross section. Finally, for temperatures in the range of experimental interest we compute finite temperature corrections to the force (again assuming perfect boundaries).
D1-60	This research paper investigates the Casimir forces in a piston geometry at zero and finite temperatures. We study the effect of temperature on the Casimir force, which is the force that arises between two closely spaced parallel plates due to the fluctuations of the electromagnetic field. We use the scattering approach to calculate the Casimir force for different piston geometries, including a rectangular piston and a cylindrical piston. Our results show that the Casimir force decreases with increasing temperature and that the geometry of the piston has a significant effect on the magnitude of the force. We also investigate the role of surface roughness on the Casimir force and find that it can have a significant impact on the force at low temperatures. These findings have important implications for the design and optimization of nanoscale devices that rely on the Casimir force for their operation.
D1-61	We extend recent work that included the effect of pressure forces to derive the precession rate of eccentric accretion discs in cataclysmic variables to the case of double degenerate systems. We find that the logical scaling of the pressure force in such systems results in predictions of unrealistically high primary masses. Using the prototype AM CVn as a calibrator for the magnitude of the effect, we find that there is no scaling that applies consistently to all the systems in the class. We discuss the reasons for the lack of a superhump period to mass ratio relationship analogous to that known for SU UMa systems and suggest that this is because these secondaries do not have a single valued mass-radius relationship. We highlight the unreliability of mass-ratios derived by applying the SU UMa expression to the AM CVn binaries.
D1-62	Inspired by a recent astro-ph posting, I propose a creation of an Alternative History astro-ph archive (althistastro-ph). Such an archive would serve as a final resting place for the various telescope (and possibly other) proposals that were not successful. As we all know, from both submitting proposals and also from serving on various time allocation committees, many excellent proposals ``do not make it''. Creating such an AltHist archive would serve many goals, including venting the frustration of the authors and also providing possible amusement for the readers. These are worthy goals, but they alone would not warrant creating such an archive. The truly useful role of AltHistAstro-ph archive would be to match astronomers with unappreciated ideas with other astronomers with underutilized resources, hopefully leading in some cases to resurrection of old proposals and resulting publications in the regular astro-ph archive. Given the possible danger of a low signal-to-noise and possible confusion, a creation of a separate archive seems like a good idea, although it should be noted that low signal-to-noise is achieved on astro-ph quite often already. Finally, I include my own excellent, but rejected (twice), HST proposal, as an example of a potential AltHistAstro-ph posting.
D1-63	The field of astronomy is constantly evolving with new discoveries and advancements in technology. However, the limited funding and resources allocated to the astronomical community have hindered its progress. In this research paper, a modest proposal is presented to address this issue. The proposal suggests a collaborative effort between government agencies, private organizations, and individual donors to establish a sustainable funding model for the astronomical community. The paper also discusses the potential benefits of this proposal, including increased research opportunities and a more comprehensive understanding of the universe. Furthermore, the proposal emphasizes the need for transparency and accountability in the distribution of funds. The research paper concludes by acknowledging the challenges associated with implementing such a proposal but highlights the importance of investing in the advancement of astronomy for the betterment of society as a whole.
D1-64	Aims: In the context of space interferometry missions devoted to the search of exo-Earths, this paper investigates the capabilities of new single mode conductive waveguides at providing modal filtering in an infrared and monochromatic nulling experiment; Methods: A Michelson laser interferometer with a co-axial beam combination scheme at 10.6 microns is used. After introducing a Pi phase shift using a translating mirror, dynamic and static measurements of the nulling ratio are performed in the two cases where modal filtering is implemented and suppressed. No additional active control of the wavefront errors is involved. Results: We achieve on average a statistical nulling ratio of 2.5e-4 with a 1-sigma upper limit of 6e-4, while a best null of 5.6e-5 is obtained in static mode. At the moment, the impact of external vibrations limits our ability to maintain the null to 10 to 20 seconds.; Conclusions: A positive effect of SM conductive waveguide on modal filtering has been observed in this study. Further improvement of the null should be possible with proper mechanical isolation of the setup.
D1-65	This research paper presents a mid-infrared laser light nulling experiment using single-mode conductive waveguides. The objective of this study is to investigate the feasibility of nulling laser light at mid-infrared wavelengths using waveguides. The experiment was conducted using a CO2 laser and a single-mode conductive waveguide made of silicon. The results of the experiment show that the waveguide successfully nulls the laser light at mid-infrared wavelengths. The study concludes that single-mode conductive waveguides can be an effective approach for nulling laser light at mid-infrared wavelengths, which has important implications for applications in astronomy and remote sensing.
D1-66	"Semiclassical reasoning suggests that the process by which an object collapses into a black hole and then evaporates by emitting Hawking radiation may destroy information, a problem often referred to as the black hole information paradox. Further, there seems to be no unique prediction of where the information about the collapsing body is localized. We propose that the latter aspect of the paradox may be a manifestation of an inconsistent self-reference in the semiclassical theory of black hole evolution. This suggests the inadequacy of the semiclassical approach or, at worst, that standard quantum mechanics and general relavity are fundamentally incompatible. One option for the resolution for the paradox in the localization is to identify the G\""odel-like incompleteness that corresponds to an imposition of consistency, and introduce possibly new physics that supplies this incompleteness. Another option is to modify the theory in such a way as to prohibit self-reference. We discuss various possible scenarios to implement these options, including eternally collapsing objects, black hole remnants, black hole final states, and simple variants of semiclassical quantum gravity."
D1-67	In this paper we outline several points of view on the interplay between discrete and continuous wavelet transforms; stressing both pure and applied aspects of both. We outline some new links between the two transform technologies based on the theory of representations of generators and relations. By this we mean a finite system of generators which are represented by operators in Hilbert space. We further outline how these representations yield sub-band filter banks for signal and image processing algorithms.
D1-68	This research paper compares the discrete wavelet transform (DWT) and continuous wavelet transform (CWT) for signal analysis. The study analyzes the performance of both transforms in terms of time and frequency resolution, computational complexity, and noise robustness. The results show that CWT provides better time-frequency resolution compared to DWT, but at a higher computational cost. However, DWT is found to be more robust to noise. The study concludes that the choice of transform depends on the specific application and the trade-off between resolution and computational cost.
D1-69	We propose a generic test for models in which gravity is modified to do away with dark matter. These models tend to have gravitons couple to a different metric than ordinary matter. A strong test of such models comes from comparing the arrival time of the gravitational wave pulse from a cosmological event such as a supernova with the arrival times of the associated pulses of neutrinos and photons. For SN 1987a we show that the gravity wave would have arrived 5.3 days after the neutrino pulse.
D1-70	This research paper presents a generic test for modified gravity models that emulate dark matter. The study focuses on the galactic dynamics of dwarf spheroidal galaxies and utilizes the Jeans equation to derive the velocity dispersion profiles. The authors compare the predictions of modified gravity models to those of dark matter models and find that some modified gravity models can accurately reproduce the observed velocity dispersion profiles without the need for dark matter. However, the study also highlights the limitations of the generic test and emphasizes the need for further testing and refinement of modified gravity models. The results suggest that modified gravity models have the potential to provide an alternative explanation for the observed galactic dynamics, but more research is needed to fully understand their implications.
D1-71	"We study the IR-through-UV interstellar extinction curves towards 328 Galactic B and late-O stars. We use a new technique which employs stellar atmosphere models in lieu of unreddened ""standard"" stars. This technique is capable of virtually eliminating spectral mismatch errors in the curves. It also allows a quantitative assessment of the errors and enables a rigorous testing of the significance of relationships between various curve parameters, regardless of whether their uncertainties are correlated. Analysis of the curves gives the following results: (1) In accord with our previous findings, the central position of the 2175 A extinction bump is mildly variable, its width is highly variable, and the two variations are unrelated. (2) Strong correlations are found among some extinction properties within the UV region, and within the IR region. (3) With the exception of a few curves with extreme (i.e., large) values of R(V), the UV and IR portions of Galactic extinction curves are not correlated with each other. (4) The large sightline-to-sightline variation seen in our sample implies that any average Galactic extinction curve will always reflect the biases of its parent sample. (5) The use of an average curve to deredden a spectral energy distribution (SED) will result in significant errors, and a realistic error budget for the dereddened SED must include the observed variance of Galactic curves. While the observed large sightline-to-sightline variations, and the lack of correlation among the various features of the curves, make it difficult to meaningfully characterize average extinction properties, they demonstrate that extinction curves respond sensitively to local conditions. Thus, each curve contains potentially unique information about the grains along its sightline."
D1-72	"The research paper titled ""An Analysis of the Shapes of Interstellar Extinction Curves. V. The IR-Through-UV Curve Morphology"" focuses on analyzing the shapes of interstellar extinction curves. The primary theme of this research is to study the morphology of IR-through-UV extinction curves and investigate their variations in different environments. The research findings suggest that the shape of the extinction curve depends on the physical properties of the interstellar environment, such as the size distribution of dust grains and the abundance of heavy elements. The conclusions drawn from this research provide valuable insights into the nature and evolution of interstellar dust and the interstellar medium."
D1-73	Using the Rossi X-ray Timing Explorer (RossiXTE), astronomers have discovered that disk-accreting neutron stars with weak magnetic fields produce three distinct types of high-frequency X-ray oscillations. These oscillations are powered by release of the binding energy of matter falling into the strong gravitational field of the star or by the sudden nuclear burning of matter that has accumulated in the outermost layers of the star. The frequencies of the oscillations reflect the orbital frequencies of gas deep in the gravitational field of the star and/or the spin frequency of the star. These oscillations can therefore be used to explore fundamental physics, such as strong-field gravity and the properties of matter under extreme conditions, and important astrophysical questions, such as the formation and evolution of millisecond pulsars. Observations using RossiXTE have shown that some two dozen neutron stars in low-mass X-ray binary systems have the spin rates and magnetic fields required to become millisecond radio-emitting pulsars when accretion ceases, but that few have spin rates above about 600 Hz. The properties of these stars show that the paucity of spin rates greater than 600 Hz is due in part to the magnetic braking component of the accretion torque and to the limited amount of angular momentum that can be accreted in such systems. Further study will show whether braking by gravitational radiation is also a factor. Analysis of the kilohertz oscillations has provided the first evidence for the existence of the innermost stable circular orbit around dense relativistic stars that is predicted by strong-field general relativity. It has also greatly narrowed the possible descriptions of ultradense matter.
D1-74	Accreting Neutron Stars in Low-Mass X-Ray Binary Systems are of great interest in astrophysics due to the extreme conditions they present. In this research paper, we investigate the properties of these systems using observational data and theoretical models. We focus on the accretion process and its effects on the neutron star, as well as the emission properties of the X-ray radiation. Our results show that the accretion rate plays a crucial role in determining the behavior of these systems, with higher rates leading to more intense X-ray emission and potentially triggering thermonuclear explosions. We also find evidence for the existence of a boundary layer between the accretion disk and the neutron star, which can affect the observed X-ray spectra. Our study provides insight into the physics of accreting neutron stars and highlights the importance of continued observational and theoretical investigations in this field.
D1-75	We compute the high-frequency emission and absorption noise in a fractional quantum Hall effect (FQHE) sample at arbitrary temperature. We model the edges of the FQHE as chiral Luttinger liquids (LL) and we use the non-equilibrium perturbative Keldysh formalism. We find that the non-symmetrized high frequency noise contains important signatures of the electron-electron interactions that can be used to test the Luttinger liquid physics, not only in FQHE edge states, but possibly also in other one-dimensional systems such as carbon nanotubes. In particular we find that the emission and absorption components of the excess noise (defined as the difference between the noise at finite voltage and at zero voltage) are different in an interacting system, as opposed to the non-interacting case when they are identical. We study the resonance features which appear in the noise at the Josephson frequency (proportional to the applied voltage), and we also analyze the effect of the distance between the measurement point and the backscattering site. Most of our analysis is performed in the weak backscattering limit, but we also compute and discuss briefly the high-frequency noise in the tunneling regime.
D1-76	The fractional quantum Hall effect (FQHE) has been a topic of intense research in condensed matter physics due to its exotic properties and potential applications in quantum computing. In this paper, we investigate the emission and absorption noise in the FQHE system using numerical simulations and theoretical analysis. Specifically, we focus on the role of edge states and disorder in generating noise in the FQHE regime. Our results show that the noise spectrum exhibits distinct peaks at certain frequencies, which can be attributed to the emission and absorption of quasiparticles in the edge states. We also find that disorder can significantly enhance the noise level at low frequencies, while the edge states dominate the high-frequency noise. Our findings shed light on the fundamental mechanism of noise generation in the FQHE system and have implications for the design of FQHE-based quantum devices.
D1-77	The existence of magnetosonic solitons in dusty plasmas is investigated. The nonlinear magnetohydrodynamic equations for a warm dusty magnetoplasma are thus derived. A solution of the nonlinear equations is presented. It is shown that, due to the presence of dust, static structures are allowed. This is in sharp contrast to the formation of the so called shocklets in usual magnetoplasmas. A comparatively small number of dust particles can thus drastically alter the behavior of the nonlinear structures in magnetized plasmas.
D1-78	This research paper investigates the formation and propagation of magnetosonic solitons in a dusty plasma slab. Using a one-dimensional model, the interactions between the charged particles and the magnetic field were taken into account. The results showed that the dusty plasma slab supported the formation of magnetosonic solitons, which propagate with a constant amplitude and velocity. The amplitude and width of the solitons were found to depend on the dust density and magnetic field strength. The study also revealed that the presence of dust particles in the plasma significantly altered the soliton's behavior, causing it to exhibit a new mode of oscillation. These findings have important implications for understanding the dynamics of dusty plasmas and could be useful in developing new technologies for plasma-based devices.
D1-79	Very recently, it has been shown that thermal noise and its artificial versions (Johnson-like noises) can be utilized as an information carrier with peculiar properties therefore it may be proper to call this topic Thermal Noise Informatics. Zero Power (Stealth) Communication, Thermal Noise Driven Computing, and Totally Secure Classical Communication are relevant examples. In this paper, while we will briefly describe the first and the second subjects, we shall focus on the third subject, the secure classical communication via wire. This way of secure telecommunication utilizes the properties of Johnson(-like) noise and those of a simple Kirchhoff's loop. The communicator is unconditionally secure at the conceptual (circuit theoretical) level and this property is (so far) unique in communication systems based on classical physics. The communicator is superior to quantum alternatives in all known aspects, except the need of using a wire. In the idealized system, the eavesdropper can extract zero bit of information without getting uncovered. The scheme is naturally protected against the man-in-the-middle attack. The communication can take place also via currently used power lines or phone (wire) lines and it is not only a point-to-point communication like quantum channels but network-ready. Tests have been carried out on a model-line with ranges beyond the ranges of any known direct quantum communication channel and they indicate unrivalled signal fidelity and security performance. This simple device has single-wire secure key generation/sharing rates of 0.1, 1, 10, and 100 bit/second for copper wires with diameters/ranges of 21 mm / 2000 km, 7 mm / 200 km, 2.3 mm / 20 km, and 0.7 mm / 2 km, respectively and it performs with 0.02% raw-bit error rate (99.98 % fidelity).
D1-80	"The paper ""Thermal noise informatics"" explores the potential of using thermal noise to achieve secure communication through a wire, zero-power communication, and thermal noise driven computing. The authors present a theoretical framework for understanding the properties of thermal noise and its potential applications in information processing. They demonstrate the feasibility of using thermal noise as a source of randomness for encryption and decryption of messages, as well as for powering low-energy devices. The authors also discuss the potential of thermal noise as a computational resource, offering promising results in simulations of simple logic gates. Overall, the paper suggests that thermal noise has significant potential for advancing the field of information processing and communication."
D1-81	An iterative method for recovering the bulk information in asymptotically AdS spacetimes is presented. We consider zero energy spacelike geodesics and their relation to the entanglement entropy in three dimensions to determine the metric in certain symmetric cases. A number of comparisons are made with an alternative extraction method presented in arXiv:hep-th/0609202, and the two methods are then combined to allow metric recovery in the most general type of static, spherically symmetric setups. We conclude by extracting the mass and density profiles for a toy model example of a gas of radiation in (2+1)-dimensional AdS.
D1-82	This research paper focuses on the numerical extraction of metrics in the Anti-de Sitter/Conformal Field Theory (AdS/CFT) correspondence. The AdS/CFT correspondence is a theoretical framework that relates two seemingly different physical theories - gravity in a spacetime called Anti-de Sitter space and a quantum field theory on the boundary of that spacetime. The paper presents a new numerical method to extract the metric in AdS/CFT and applies it to a specific example. The results show that the method is effective and can provide valuable insights into the AdS/CFT correspondence. The research concludes that this numerical approach can be applied to other scenarios in AdS/CFT and can be useful in understanding the interplay between gravity and quantum field theory.
D1-83	We report quantitative relations between corruption level and economic factors, such as country wealth and foreign investment per capita, which are characterized by a power law spanning multiple scales of wealth and investments per capita. These relations hold for diverse countries, and also remain stable over different time periods. We also observe a negative correlation between level of corruption and long-term economic growth. We find similar results for two independent indices of corruption, suggesting that the relation between corruption and wealth does not depend on the specific measure of corruption. The functional relations we report have implications when assessing the relative level of corruption for two countries with comparable wealth, and for quantifying the impact of corruption on economic growth and foreign investments.
D1-84	This research paper explores the quantitative relations between corruption and economic factors. By analyzing data from various countries, the study finds that corruption has a significant negative impact on economic growth, foreign direct investment, and government spending. Moreover, the research shows that corruption is correlated with income inequality and poverty. The study concludes that reducing corruption is crucial for promoting economic development and improving the standard of living for people in developing countries.
D1-85	In this article we calculate the electrical conductivity in QED using the 2PI effective action. We use a modified version of the usual 2PI effective action which is defined with respect to self-consistent solutions of the 2-point functions. We show that the green functions obtained from this modified effective action satisfy ward identities and that the conductivity obtained from the kubo relation is gauge invariant. We work to 3-loop order in the modified 2PI effective action and show explicitly that the resulting expression for the conductivity contains the square of the amplitude that corresponds to all binary collision and production processes.
D1-86	This research paper presents a study of the electrical conductivity of quantum electrodynamics (QED) using the 2PI effective action. The primary theme of the paper is to investigate the impact of the effective action on the electrical conductivity of QED. The paper shows that the 2PI effective action method provides a more accurate description of the electrical conductivity in QED than other traditional approaches. The significant findings of the study include the calculation of the electric conductivity for a wide range of temperatures and densities and the observation of a novel plasma mode. The research concludes that the 2PI effective action is a powerful tool for exploring the electrical conductivity of QED and can provide valuable insights into the physics of strongly interacting systems.
D1-87	Cosmic shear constrains cosmology by exploiting the apparent alignments of pairs of galaxies due to gravitational lensing by intervening mass clumps. However galaxies may become (intrinsically) aligned with each other, and with nearby mass clumps, during their formation. This effect needs to be disentangled from the cosmic shear signal to place constraints on cosmology. We use the linear intrinsic alignment model as a base and compare it to an alternative model and data. If intrinsic alignments are ignored then the dark energy equation of state is biased by ~50 per cent. We examine how the number of tomographic redshift bins affects uncertainties on cosmological parameters and find that when intrinsic alignments are included two or more times as many bins are required to obtain 80 per cent of the available information. We investigate how the degradation in the dark energy figure of merit depends on the photometric redshift scatter. Previous studies have shown that lensing does not place stringent requirements on the photometric redshift uncertainty, so long as the uncertainty is well known. However, if intrinsic alignments are included the requirements become a factor of three tighter. These results are quite insensitive to the fraction of catastrophic outliers, assuming that this fraction is well known. We show the effect of uncertainties in photometric redshift bias and scatter. Finally we quantify how priors on the intrinsic alignment model would improve dark energy constraints.
D1-88	This research paper presents a study on the constraints of dark energy from cosmic shear power spectra. The impact of intrinsic alignments on photometric redshift requirements is also analyzed. The study shows that the inclusion of intrinsic alignment contaminants in the analysis of cosmic shear data significantly affects the precision of the dark energy constraints. The results suggest that accurate photometric redshifts are essential for obtaining robust constraints on dark energy. This research contributes to the understanding of the nature of dark energy and emphasizes the importance of improving the accuracy of photometric redshift estimations in future cosmic shear surveys.
D1-89	A Brownian time process is a Markov process subordinated to the absolute value of an independent one-dimensional Brownian motion. Its transition densities solve an initial value problem involving the square of the generator of the original Markov process. An apparently unrelated class of processes, emerging as the scaling limits of continuous time random walks, involve subordination to the inverse or hitting time process of a classical stable subordinator. The resulting densities solve fractional Cauchy problems, an extension that involves fractional derivatives in time. In this paper, we will show a close and unexpected connection between these two classes of processes, and consequently, an equivalence between these two families of partial differential equations.
D1-90	"The research paper titled ""Brownian subordinators and fractional Cauchy problems"" studies the behavior of Brownian subordinators, which are stochastic processes that describe the distribution of waiting times between two events. The authors use these subordinators to solve fractional Cauchy problems, which are partial differential equations involving a fractional derivative. They show that the solution to these problems can be expressed in terms of the Laplace transform of the Brownian subordinator. The authors also provide numerical simulations to illustrate the behavior of the solution in various scenarios. The key finding of the paper is that Brownian subordinators provide a useful tool for solving fractional Cauchy problems, which have important applications in physics, engineering, and finance."
D1-91	Binary microlensing light curves have a variety of morphologies. Many are indistinguishable from point lens light curves. Of those that deviate from the point lens form, caustic crossing light curves have tended to dominate identified binary lens events. Other distinctive signatures of binary lenses include significant asymmetry, multiple peaks, and repeating events. We have quantified, using high resolution simulations, the theoretically expected relative numbers of each type of binary lens event, based on its measurable characteristics. We find that a microlensing survey with current levels of photometric uncertainty and sampling should find at least as many non-caustic crossing binary lens events as caustic crossing events; in future surveys with more sensitive photometry, the contribution of distinctive non-caustic crossing events will be even greater. To try to explain why caustic crossing light curves appear to be so dominant among the published binary lensing events, we investigate the influence of several physical effects, including blending, sampling rate, and various binary populations.
D1-92	This research paper aims to explore the properties of binary microlensing light curves beyond the conventional caustic crossings. The study will investigate the various features of light curves that result from binary microlensing, including the effects of the source trajectory, the source size, and the binary geometry. The research will utilize a combination of numerical simulations and statistical analyses to identify patterns in the light curves and explore the underlying physical processes. By examining the properties of binary microlensing light curves beyond the caustic crossings, this study will contribute to a deeper understanding of the dynamics of binary systems and their potential applications in astrophysics.
D1-93	The radiation hardness of silicon charged particle sensors is compared with single crystal and polycrystalline diamond sensors, both experimentally and theoretically. It is shown that for Si- and C-sensors, the NIEL hypothesis, which states that the signal loss is proportional to the Non-Ionizing Energy Loss, is a good approximation to the present data. At incident proton and neutron energies well above 0.1 GeV the radiation damage is dominated by the inelastic cross section, while at non-relativistic energies the elastic cross section prevails. The smaller inelastic nucleon-Carbon cross section and the light nuclear fragments imply that at high energies diamond is an order of magnitude more radiation hard than silicon, while at energies below 0.1 GeV the difference becomes significantly smaller.
D1-94	This research paper compares the radiation hardness of diamond and silicon sensors. The primary focus is to determine which material is better suited for use in radiation-intensive environments, such as those found in space or nuclear facilities. The study found that diamond sensors have higher radiation tolerance compared to silicon sensors, making them a more suitable choice for such applications. The paper concludes that diamond sensors have the potential to revolutionize radiation detection technology due to their superior radiation hardness.
D1-95	The topological insulator is an electronic phase stabilized by spin-orbit coupling that supports propagating edge states and is not adiabatically connected to the ordinary insulator. In several ways it is a spin-orbit-induced analogue in time-reversal-invariant systems of the integer quantum Hall effect (IQHE). This paper studies the topological insulator phase in disordered two-dimensional systems, using a model graphene Hamiltonian introduced by Kane and Mele as an example. The nonperturbative definition of a topological insulator given here is distinct from previous efforts in that it involves boundary phase twists that couple only to charge, does not refer to edge states, and can be measured by pumping cycles of ordinary charge. In this definition, the phase of a Slater determinant of electronic states is determined by a Chern parity analogous to Chern number in the IQHE case. Numerically we find, in agreement with recent network model studies, that the direct transition between ordinary and topological insulators that occurs in band structures is a consequence of the perfect crystalline lattice. Generically these two phases are separated by a metallic phase, which is allowed in two dimensions when spin-orbit coupling is present. The same approach can be used to study three-dimensional topological insulators.
D1-96	The research paper explores the concept of topological insulators (TI) beyond the Brillouin zone through the use of Chern parity. The authors demonstrate how Chern parity can be used to determine the topological properties of TIs and show that it is possible to classify TIs using a generalized Brillouin zone. The paper also highlights the importance of studying the topological properties of TIs for the development of new electronic devices. Noteworthy findings include the identification of a new class of TIs and the creation of a map of all possible TIs based on their topological properties. Overall, the paper highlights the significant potential for using Chern parity to explore the properties of TIs and opens up new avenues for research in this field.
D1-97	We report an experimental realization of one-way quantum computing on a two-photon four-qubit cluster state. This is accomplished by developing a two-photon cluster state source entangled both in polarization and spatial modes. With this special source, we implemented a highly efficient Grover's search algorithm and high-fidelity two qubits quantum gates. Our experiment demonstrates that such cluster states could serve as an ideal source and a building block for rapid and precise optical quantum computation.
D1-98	This research paper presents experimental realization of one-way quantum computing with two-photon four-qubit cluster states. The study demonstrates the creation and manipulation of cluster states, as well as the implementation of various quantum gates. The results suggest that the four-qubit cluster state is a promising resource for one-way quantum computing. This study lays the foundation for future research in the field of quantum computing and provides a step towards practical applications of quantum information processing.
D1-99	At the air/water interface, 4,-8-alkyl[1,1,-biphenyl]-4-carbonitrile (8CB) domains with different thicknesses coexist in the same Langmuir film, as multiple bilayers on a monolayer. The edge dislocation at the domain boundary leads to line tension, which determines the domain shape and dynamics. By observing the domain relaxation process starting from small distortions, we find that the line tension is linearly dependent on the thickness difference between the coexisting phases in the film. Comparisons with theoretical treatments in the literature suggest that the edge dislocation at the boundary locates near the center of the film, which means that the 8CB multilayers are almost symmetric with respect to the air/water interface.
D1-100	The research paper investigates the line tension and structure of smectic liquid crystal multilayers at the air-water interface. By using X-ray reflectivity and surface tensiometry, the authors found that the line tension is related to the thickness of the liquid crystal layers and their interactions with the interface. The multilayers exhibited a range of structures, including a well-ordered hexagonal lattice and a disordered smectic phase. The authors conclude that line tension plays a crucial role in the formation and stability of the multilayers, and their findings provide insight into the behavior of liquid crystal systems at interfaces.
