{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e117c95a",
      "metadata": {},
      "source": [
        "# Robustly Optimized BERT Pretraining Approach (RoBERTa) Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f7f3362",
      "metadata": {},
      "source": [
        "```md\n",
        "@authors: Grupo 03\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "00553163",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Notebook Imports\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import shutil\n",
        "\n",
        "from tensorflow import keras\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from enum import Enum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "7cee9d6c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model run enum\n",
        "class ModelRunMode(Enum):\n",
        "    \"\"\"\n",
        "    Enumeration of Model Run Mode.\n",
        "    \"\"\"\n",
        "    TRAIN           = \"train\"           # Train Mode\n",
        "    CLASSIFY        = \"classify\"        # Classify Mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "fc6f6268",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model run mode\n",
        "# Options: \n",
        "#   ModelRunMode.TRAIN.value            (Train the model)\n",
        "#   ModelRunMode.CLASSIFY.value         (Classify data)\n",
        "mode = ModelRunMode.CLASSIFY.value\n",
        "# Prefix for saving the model files\n",
        "model_prefix = \"llm_roberta_model\"\n",
        "file_path = \"llm_roberta_model_weights\"\n",
        "separator_char = \"\\t\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "d6adb728",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters cell\n",
        "if mode == ModelRunMode.TRAIN.value:\n",
        "    # TRAIN mode: Set parameters for training\n",
        "    input_csv = \"../tarefa_1/test_input_dataset/merged_inputs.csv\"                                  # CSV file with training inputs (ID, Text)\n",
        "    output_csv = \"../tarefa_1/test_output_dataset/merged_outputs.csv\"                               # CSV file with training outputs (ID, Label)\n",
        "    test_size = 0.3                                                                                 # Proportion of the dataset to use as test data\n",
        "    random_state=42                                                                                 # Seed for reproducible dataset splitting\n",
        "elif mode == ModelRunMode.CLASSIFY.value:\n",
        "    # CLASSIFY mode: Set parameters for classification\n",
        "    input_csv = \"../tarefa_3/classify_input_datasets/submission3_inputs.csv\"                        # CSV file with texts for prediction (ID, Text)\n",
        "    output_csv = \"../tarefa_3/classify_output_datasets/submission3_outputs_llm_roberta_model.csv\"   # CSV file to store prediction result\n",
        "else:\n",
        "    print(\"The selected option is not valid. Options: \\\"train\\\" or \\\"classify\\\"!\")\n",
        "    SystemExit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "7713c68b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method to load and merge two datasets by ID column\n",
        "def merge_data_by_id(input_csv, output_csv, sep=\"\\t\"):\n",
        "    df_in = pd.read_csv(input_csv, sep=sep)\n",
        "    df_out = pd.read_csv(output_csv, sep=sep)\n",
        "\n",
        "    # Remove duplicates or NaNs if needed\n",
        "    df_in.dropna(subset=[\"ID\", \"Text\"], inplace=True)\n",
        "    df_out.dropna(subset=[\"ID\", \"Label\"], inplace=True)\n",
        "    df_in.drop_duplicates(subset=[\"ID\"], inplace=True)\n",
        "    df_out.drop_duplicates(subset=[\"ID\"], inplace=True)\n",
        "\n",
        "    df_merged = pd.merge(df_in, df_out, on=\"ID\", how=\"inner\")\n",
        "    return df_merged\n",
        "\n",
        "# Method for text cleaning\n",
        "def text_cleaning(text):\n",
        "        # Remove URLs\n",
        "        text = re.sub(r'http[s]?://\\S+', \"\", text)\n",
        "        # Remove HTML tags\n",
        "        text = re.sub(r\"<[^>]*>\", \"\", text)\n",
        "        # Remove common LaTeX commands\n",
        "        text = re.sub(r\"\\\\[a-zA-Z]+(\\{.*?\\})?\", \"\", text)\n",
        "        # Remove email addresses\n",
        "        text = re.sub(r'\\S+@\\S+', \"\", text)\n",
        "        # Return cleaned text\n",
        "        return text\n",
        "\n",
        "# Method to convert labels to binary\n",
        "def convert_labels_to_binary_and_text(df_merged):\n",
        "    df_merged[\"Label\"] = df_merged[\"Label\"].str.lower().str.strip()\n",
        "    y = np.where(df_merged[\"Label\"] == \"ai\", 1, 0)\n",
        "    texts = df_merged[\"Text\"].tolist()\n",
        "    return y, texts\n",
        "\n",
        "# Method to plot the learning curves\n",
        "def plot_learning_curves(history):\n",
        "    # Loss\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    if 'val_loss' in history.history:\n",
        "        plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "    plt.title(\"Loss\")\n",
        "    plt.legend()\n",
        "    \n",
        "    # Accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='Train Acc')\n",
        "    if 'val_accuracy' in history.history:\n",
        "        plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
        "    plt.title(\"Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Method to check label distribution\n",
        "def check_label_distribution(df_merged):\n",
        "    label_counts = df_merged[\"Label\"].value_counts(dropna=False)\n",
        "    print(\"Label distribution:\\n\", label_counts)\n",
        "\n",
        "# Method to print the first 5 cleaned texts\n",
        "def debug_text_cleaning(df_merged):\n",
        "    for i in range(min(5, len(df_merged))):\n",
        "        print(df_merged[\"Text\"].iloc[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "98f21a18",
      "metadata": {},
      "outputs": [],
      "source": [
        "# BERT Model training function\n",
        "def train_roberta(input_csv, output_csv, model_ckpt=\"roberta-base\", output_dir=file_path, model_prefix= model_prefix, num_train_epochs=3, batch_size=8, test_size=0.2, learning_rate=1e-3, random_state=48):\n",
        "    # Load data\n",
        "    df_merged = merge_data_by_id(input_csv, output_csv, sep=separator_char)\n",
        "\n",
        "    # Check label distribution\n",
        "    check_label_distribution(df_merged=df_merged)\n",
        "\n",
        "    # Text cleaning\n",
        "    df_merged[\"Text\"] = df_merged[\"Text\"].apply(text_cleaning)\n",
        "\n",
        "    # Print the first 5 cleaned texts\n",
        "    debug_text_cleaning(df_merged)\n",
        "\n",
        "    # Convert label/text\n",
        "    labels, texts = convert_labels_to_binary_and_text(df_merged)\n",
        "\n",
        "    # Split entire dataset into train_val vs test\n",
        "    X_train_texts, X_val_texts, y_train, y_val = train_test_split(texts, labels, test_size=test_size, random_state=random_state, stratify=labels)\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "\n",
        "    # Tokenize without truncation/padding to compute lengths\n",
        "    raw_train_encodings = tokenizer(X_train_texts, add_special_tokens=True, truncation=False, padding=False)\n",
        "    token_lengths = [len(seq) for seq in raw_train_encodings[\"input_ids\"]]\n",
        "\n",
        "    # Calculate max_length based on the 90th percentile\n",
        "    max_length = int(np.percentile(token_lengths, 90))\n",
        "\n",
        "    # Tokenize with truncation and padding using the calculated max_length\n",
        "    train_encodings = tokenizer(X_train_texts, truncation=True, padding=\"max_length\", max_length=max_length)\n",
        "    val_encodings = tokenizer(X_val_texts, truncation=True, padding=\"max_length\", max_length=max_length)\n",
        "\n",
        "    # Convert to tf.data.Dataset\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train))\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings), y_val))\n",
        "\n",
        "    # Batch the datasets\n",
        "    train_dataset = train_dataset.batch(batch_size)\n",
        "    val_dataset = val_dataset.batch(batch_size)\n",
        "\n",
        "    # Load the model & set high dropout rates for regularization\n",
        "    model = TFAutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=2)\n",
        "    model.config.hidden_dropout_prob = 0.2\n",
        "    model.config.attention_probs_dropout_prob = 0.2\n",
        "    \n",
        "    # Initially freeze base RoBERTa layers so only the classifier head trains\n",
        "    model.roberta.trainable = False\n",
        "    \n",
        "    # Learning rate schedule & optimizer with gradient clipping\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate=learning_rate,\n",
        "        decay_steps=1000,\n",
        "        decay_rate=0.9,\n",
        "        staircase=True\n",
        "    )\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, clipnorm=1.0)\n",
        "    \n",
        "    # Label smoothing in the loss function\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
        "    \n",
        "    # Callbacks\n",
        "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)\n",
        "    \n",
        "    # Train\n",
        "    history = model.fit(\n",
        "        train_dataset,\n",
        "        validation_data=val_dataset,\n",
        "        epochs=num_train_epochs,\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    val_loss, val_acc = model.evaluate(val_dataset)\n",
        "    print(f\"\\nValidation Accuracy: {val_acc:.4f} | Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Display model sumary\n",
        "    model.summary()\n",
        "\n",
        "    # Plot training curves\n",
        "    plot_learning_curves(history)\n",
        "\n",
        "    # If the output_dir already exists, remove it entirely\n",
        "    if os.path.exists(output_dir):\n",
        "        shutil.rmtree(output_dir)\n",
        "\n",
        "    # Now recreate the folder\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Construct the file paths\n",
        "    model_path = os.path.join(output_dir, f\"{model_prefix}_model\")\n",
        "    tokenizer_path = os.path.join(output_dir, f\"{model_prefix}_tokenizer\")\n",
        "    config_path = os.path.join(output_dir, f\"{model_prefix}_config.json\")\n",
        "\n",
        "    # Save the model weights\n",
        "    print(\"Saving model to:\", model_path)\n",
        "    model.save_pretrained(model_path)\n",
        "\n",
        "    # Save the model tokenizer\n",
        "    print(\"Saving tokenizer to:\", tokenizer_path)\n",
        "    tokenizer.save_pretrained(tokenizer_path)\n",
        "\n",
        "    # Save configuration\n",
        "    print(\"Save configuration to:\", config_path)\n",
        "    config_data = {\n",
        "        \"model_ckpt\": model_ckpt,\n",
        "        \"num_train_epochs\": num_train_epochs,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"max_length\": max_length\n",
        "    }\n",
        "    \n",
        "    with open(config_path, \"w\") as f:\n",
        "        json.dump(config_data, f)\n",
        "\n",
        "    # Print end message\n",
        "    print(f\"RoBERTa Model, tokenizer and configuration stored under {output_dir}. Finished Training!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "43fa4fd7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Classification function\n",
        "def classify_roberta(input_csv, output_csv, output_dir=\"llm_roberta_model_weights\", separator_char=separator_char):\n",
        "    # Construct the file paths\n",
        "    model_path = os.path.join(output_dir, f\"{model_prefix}_model\")\n",
        "    tokenizer_path = os.path.join(output_dir, f\"{model_prefix}_tokenizer\")\n",
        "    config_path = os.path.join(output_dir, f\"{model_prefix}_config.json\")\n",
        "\n",
        "    # Load the model\n",
        "    print(f\"Loading model from: {output_dir}\")\n",
        "    model = TFAutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "    \n",
        "    # Load the tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "\n",
        "    # Loading configuration\n",
        "    print(\"Loading configuration from:\", config_path)\n",
        "    with open(config_path, \"r\") as f:\n",
        "         config_data = json.load(f)\n",
        "\n",
        "    # Retrieve the saved configuration max_len\n",
        "    max_length = config_data[\"max_length\"]\n",
        "\n",
        "    # Read the input CSV\n",
        "    df_input = pd.read_csv(input_csv, sep=separator_char)\n",
        "    if \"ID\" not in df_input.columns or \"Text\" not in df_input.columns:\n",
        "        raise ValueError(\"Input CSV must have 'ID' and 'Text' columns for classification.\")\n",
        "    \n",
        "    # Extract texts\n",
        "    texts = df_input[\"Text\"].astype(str).tolist()\n",
        "    \n",
        "    # Tokenize\n",
        "    encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length)\n",
        "    \n",
        "    # Create tf.data.Dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(dict(encodings))\n",
        "    dataset = dataset.batch(16)\n",
        "    \n",
        "    # Predict probabilities (logits => sigmoid)\n",
        "    outputs = model.predict(dataset)\n",
        "\n",
        "    if isinstance(outputs, dict) and \"logits\" in outputs:\n",
        "        logits = outputs[\"logits\"]\n",
        "    else:\n",
        "        logits = outputs\n",
        "    \n",
        "    # Convert logits to probabilities\n",
        "    probs = tf.nn.sigmoid(logits[:, 1]).numpy()\n",
        "    \n",
        "    # Threshold => \"AI\" vs \"Human\"\n",
        "    pred_bin = (probs >= 0.5).astype(int)\n",
        "    pred_labels = np.where(pred_bin == 1, \"AI\", \"Human\")\n",
        "    \n",
        "    # Save result\n",
        "    df_out = pd.DataFrame({\"ID\": df_input[\"ID\"], \"Label\": pred_labels})\n",
        "    df_out.to_csv(output_csv, sep=separator_char, index=False)\n",
        "    print(f\"Predictions saved to {output_csv}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "e633c6f6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model from: llm_roberta_model_weights\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at llm_roberta_model_weights\\llm_roberta_model_model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading configuration from: llm_roberta_model_weights\\llm_roberta_model_config.json\n",
            "7/7 [==============================] - 3s 103ms/step\n",
            "Predictions saved to ../tarefa_3/classify_output_datasets/submission3_outputs_llm_roberta_model.csv\n"
          ]
        }
      ],
      "source": [
        "if mode == ModelRunMode.TRAIN.value:\n",
        "    # Check if Tensorflow is listing available GPUs (if not, continue with CPU)\n",
        "    print(\"Tensorflow List of GPUs:\", tf.config.list_physical_devices('GPU'))\n",
        "    \n",
        "    # Train model\n",
        "    train_roberta(input_csv=input_csv, output_csv=output_csv, model_ckpt=\"roberta-base\", output_dir=file_path, model_prefix=model_prefix, num_train_epochs=5, batch_size=16, test_size=test_size, learning_rate=5e-5, random_state=random_state)\n",
        "\n",
        "if mode == ModelRunMode.CLASSIFY.value:\n",
        "    # Classification\n",
        "    classify_roberta(input_csv=input_csv, output_csv=output_csv, output_dir=file_path, separator_char=separator_char)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
