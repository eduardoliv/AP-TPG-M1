{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Input, RNN, Dropout, Bidirectional\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow.keras.backend as ops\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 120\n",
    "n = 30000  # Set the number of most frequent words to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model run enum\n",
    "class ModelRunMode(Enum):\n",
    "    \"\"\"\n",
    "    Enumeration of Model Run Mode.\n",
    "    \"\"\"\n",
    "    TRAIN           = \"train\"           # Train Mode\n",
    "    CLASSIFY        = \"classify\"        # Classify Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model run mode\n",
    "# Options: \n",
    "#   ModelRunMode.TRAIN.value            (Train the model)\n",
    "#   ModelRunMode.CLASSIFY.value         (Classify data)\n",
    "mode = ModelRunMode.CLASSIFY.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters cell\n",
    "if mode == ModelRunMode.CLASSIFY.value:\n",
    "    # CLASSIFY mode: Set parameters for classification\n",
    "    input_csv = \"classify_input_datasets/dataset3_inputs.csv\"               # CSV file with texts for prediction (ID, Text)\n",
    "    output_csv = \"classify_output_datasets/dataset3_outputs_lstm_model.csv\"  # CSV file to store prediction result\n",
    "elif mode == ModelRunMode.TRAIN.value:\n",
    "    ...\n",
    "else:\n",
    "    print(\"The selected option is not valid. Options: \\\"train\\\" or \\\"classify\\\"!\")\n",
    "    SystemExit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining global fuctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text, stopwords = True):\n",
    "    def normalize(text):\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove numbers, special characters, e o caractere '\n",
    "        text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "        # Replace multiple spaces with a single space\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        # Add start and end sequence tokens\n",
    "        # text = 'startseq ' + \" \".join([word for word in text.split() if len(word) > 1]) + ' endseq'\n",
    "        return text\n",
    "\n",
    "    def remove_stopwords(text):\n",
    "        stopwords = [\n",
    "        \"the\", \"of\", \"and\", \"in\", \"to\", \"is\", \"a\", \"that\", \"for\", \"are\", \"on\", \"with\", \n",
    "        \"as\", \"at\", \"by\", \"from\", \"this\", \"it\", \"an\", \"be\", \"or\", \"which\", \"was\", \"were\"\n",
    "        ]\n",
    "        text = ' '.join([word for word in text.split() if word not in stopwords])\n",
    "        return text\n",
    "    \n",
    "    text = normalize(text)\n",
    "    if stopwords:\n",
    "        text = remove_stopwords(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, texts, n_words=None):\n",
    "        self.n_words = n_words\n",
    "        self.texts = texts\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = set()\n",
    "        self.create_index()\n",
    "        \n",
    "\n",
    "    def create_index(self):\n",
    "        word_counter = Counter()\n",
    "        for caption in self.texts:\n",
    "            for word in caption.split():\n",
    "                word_counter[word] += 1\n",
    "\n",
    "        # Sort words by frequency and alphabetically for ties\n",
    "        most_common = word_counter.most_common(self.n_words) if self.n_words else word_counter.items()\n",
    "        self.vocab = [word for word, _ in sorted(most_common, key=lambda x: (-x[1], x[0]))]\n",
    "\n",
    "        # Add special tokens\n",
    "        self.word2idx['<pad>'] = 0\n",
    "        self.word2idx['<unk>'] = 1\n",
    "        for index, word in enumerate(self.vocab):\n",
    "            self.word2idx[word] = index + 2\n",
    "\n",
    "        for word, index in self.word2idx.items():\n",
    "            self.idx2word[index] = word\n",
    "\n",
    "\n",
    "    def encode(self, caption):\n",
    "        tokens = []\n",
    "        for word in caption.split():\n",
    "            if word in self.word2idx:\n",
    "                tokens.append(self.word2idx[word])\n",
    "            else:\n",
    "                tokens.append(self.word2idx['<unk>'])\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return ' '.join([self.idx2word.get(token, '<unk>') for token in tokens])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, tokenizer,max_length=120):\n",
    "\n",
    "    X = df['Text']\n",
    "    # y = df['Label'].apply(lambda x: 0 if x == 0 else 1)\n",
    "    y = df['Label']\n",
    "    X = X.apply(lambda x: tokenizer.encode(x))\n",
    "    X = pad_sequences(X, maxlen=max_length, padding='pre')\n",
    "    # y = to_categorical(y)[:,1]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN MODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening and cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('../tarefa_1/clean_input_datasets/gpt_vs_human_data_set_inputs.csv',sep='\\t', index_col=False)['Text']\n",
    "y = pd.read_csv('../tarefa_1/clean_output_datasets/gpt_vs_human_data_set_outputs.csv',sep='\\t', index_col=False)['Label']\n",
    "df = pd.concat([X, y], axis=1)\n",
    "\n",
    "df['Text'] = df['Text'].str.replace('\\n', ' ')\n",
    "df['Text'] = df['Text'].astype(str)\n",
    "df['Label'] = df['Label'].apply(lambda x: 1 if x == \"AI\" else 0)\n",
    "df['Text'] = df['Text'].apply(clean)\n",
    "#shuffle\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_1 = pd.read_csv('../tarefa_1/clean_input_datasets/dataset1_enh_inputs.csv',sep='\\t', index_col=False)['Text']\n",
    "y_val_1 = pd.read_csv('../tarefa_1/clean_output_datasets/dataset1_enh_outputs.csv',sep='\\t', index_col=False)['Label']\n",
    "df_val_1 = pd.concat([X_val_1, y_val_1], axis=1)\n",
    "df_val_1['Text'] = df_val_1['Text'].str.replace('\\n', ' ')\n",
    "df_val_1['Text'] = df_val_1['Text'].astype(str)\n",
    "df_val_1['Label'] = df_val_1['Label'].apply(lambda x: 1 if x == \"AI\" else 0)\n",
    "df_val_1['Text'] = df_val_1['Text'].apply(clean)\n",
    "df_val_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.read_csv('dataset2_disclosed.csv', sep=';')\n",
    "val_df['Text'] = val_df['Text'].str.replace('\\n', ' ')\n",
    "val_df['Text'] = val_df['Text'].astype(str)\n",
    "val_df['Label'] = val_df['Label'].apply(lambda x: 1 if x == \"AI\" else 0)\n",
    "val_df['Text'] = val_df['Text'].apply(clean)\n",
    "val_df = val_df[['Text', 'Label']]\n",
    "val_df = pd.concat([val_df, df_val_1], axis=0).reset_index(drop=True)\n",
    "val_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(df['Text'], n)\n",
    "vocab_size = len(tokenizer)\n",
    "print('Number of words in the vocabulary:', len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = prepare_data(df, tokenizer, max_length)\n",
    "X_val, y_val = prepare_data(val_df, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_val[-1])\n",
    "print(tokenizer.decode(X_val[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tune function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "def hyperparameter_optimization_lstm(train_ds, validation_ds, \n",
    "                                     epochs_list, batch_size_list, \n",
    "                                     learning_rate_list, lstm_units_list,\n",
    "                                     embed_dim_list, \n",
    "                                     dropout_list, n_iter=10):\n",
    "\n",
    "    keras.backend.clear_session()\n",
    "    best_acc = 0.0\n",
    "    best_params = {}\n",
    "\n",
    "    # Prepare random combinations\n",
    "    param_combinations = []\n",
    "    for _ in range(n_iter):\n",
    "        param_combinations.append({\n",
    "            'epochs': random.choice(epochs_list),\n",
    "            'batch_size': random.choice(batch_size_list),\n",
    "            'learning_rate': random.choice(learning_rate_list),\n",
    "            'lstm_units': random.choice(lstm_units_list),\n",
    "            'dropout_rate': random.choice(dropout_list),\n",
    "            'embed_dim': random.choice(embed_dim_list)\n",
    "        })\n",
    "\n",
    "    # Extract input shape and number of classes\n",
    "    input_shape = train_ds.element_spec[0].shape\n",
    "    print(\"Input Shape:\", input_shape)\n",
    "\n",
    "    for i,params in enumerate(param_combinations):\n",
    "        print(f\"\\nIteration {i+1}/{n_iter}\", end=' ')\n",
    "        # Unpack parameters\n",
    "        epochs = params['epochs']\n",
    "        batch_size = params['batch_size']\n",
    "        learning_rate = params['learning_rate']\n",
    "        lstm_units = params['lstm_units']\n",
    "        dropout_rate = params['dropout_rate']\n",
    "        embed_dim = params['embed_dim']\n",
    "\n",
    "        # Build LSTM model\n",
    "        inputs = keras.Input(shape=(input_shape[1],))  # Assuming (timesteps, features)\n",
    "\n",
    "        x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
    "\n",
    "        for units in lstm_units[:-1]:\n",
    "            x = layers.Bidirectional(layers.LSTM(units, return_sequences=True))(x)\n",
    "            x = layers.Dropout(dropout_rate)(x)\n",
    "        \n",
    "        x = layers.Bidirectional(layers.LSTM(lstm_units[-1]))(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "\n",
    "        outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=0, min_lr=1e-6)\n",
    "        ]\n",
    "\n",
    "        model.compile(optimizer=optimizer,\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        # Train the model\n",
    "        history = model.fit(train_ds, epochs=epochs, batch_size=batch_size,\n",
    "                            verbose=0, validation_data=validation_ds, callbacks=callbacks)\n",
    "\n",
    "        # Get validation accuracy\n",
    "        val_acc = max(history.history.get('val_accuracy', [0]))\n",
    "        print(f\"Validation Accuracy: {val_acc:.4f}\", end=' ')\n",
    "        print(f\"Train Accuracy: {max(history.history.get('accuracy', [0])):.4f}\")\n",
    "\n",
    "        # Update best model if improved\n",
    "        if val_acc > best_acc:\n",
    "            best_model = model\n",
    "            best_acc = val_acc\n",
    "            best_params = params\n",
    "\n",
    "    print(\"\\nBest Hyperparameters Found:\", best_params)\n",
    "    print(f\"Best Accuracy: {best_acc:.4f}\")\n",
    "    return best_params, best_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == ModelRunMode.TRAIN.value:\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(64)\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(64)\n",
    "\n",
    "    # Hyperparameter search\n",
    "    epochs_list = [10, 20, 30]\n",
    "    batch_size_list = [64]\n",
    "    learning_rate_list = [1e-2, 1e-3, 1e-4]\n",
    "    lstm_units_list = [[64, 64], [128, 128], [256, 256]]\n",
    "    embed_dim_list = [50, 10, 150]\n",
    "    dropout_list = [0.3, 0.5, 0.7]\n",
    "    hiperparams, model = hyperparameter_optimization_lstm(train_ds, val_ds,\n",
    "                                                        epochs_list, batch_size_list,\n",
    "                                                        learning_rate_list, lstm_units_list,\n",
    "                                                        embed_dim_list, dropout_list, n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == ModelRunMode.TRAIN.value:\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == ModelRunMode.TRAIN.value:\n",
    "    model.save('lstm_model_weights/best_lstm_model.h5')\n",
    "    # save params and tokenizer\n",
    "    import pickle\n",
    "    with open('lstm_model_weights/best_lstm_model_params.pkl', 'wb') as f:\n",
    "        pickle.dump(hiperparams, f)\n",
    "\n",
    "    with open('lstm_model_weights/tokenizer_lstm.pkl', 'wb') as f:\n",
    "        pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFY MODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_test(X, tokenizer,max_length=120):\n",
    "    X = X.apply(lambda x: tokenizer.encode(x))\n",
    "    X = pad_sequences(X, maxlen=max_length, padding='pre')\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == ModelRunMode.CLASSIFY.value:\n",
    "    X_test = pd.read_csv(input_csv,sep=';', index_col=False)['Text']\n",
    "    ids = pd.read_csv(input_csv,sep=';', index_col=False)['ID']\n",
    "    X_test = X_test.str.replace('\\n', ' ')\n",
    "    X_test = X_test.astype(str)\n",
    "    X_test = X_test.apply(clean)\n",
    "    print(\"CSV loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == ModelRunMode.CLASSIFY.value:\n",
    "    import pickle\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    # open best params and tokenizer\n",
    "    with open('lstm_model_weights/best_lstm_model_params.pkl', 'rb') as f:\n",
    "        hiperparams = pickle.load(f)\n",
    "\n",
    "    with open('lstm_model_weights/tokenizer_lstm.pkl', 'rb') as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "\n",
    "    # print(hiperparams)\n",
    "    # print(tokenizer)\n",
    "\n",
    "    max_length = 120\n",
    "    input_shape = (max_length,)\n",
    "    vocab_size = len(tokenizer)\n",
    "    epochs = hiperparams['epochs']\n",
    "    batch_size = hiperparams['batch_size']\n",
    "    learning_rate = hiperparams['learning_rate']\n",
    "    lstm_units = hiperparams['lstm_units']\n",
    "    dropout_rate = hiperparams['dropout_rate']\n",
    "    embed_dim = hiperparams['embed_dim']\n",
    "\n",
    "    # Build LSTM model\n",
    "    inputs = keras.Input(shape=input_shape)  # Assuming (timesteps, features)\n",
    "\n",
    "    x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
    "\n",
    "    for units in lstm_units[:-1]:\n",
    "        x = layers.Bidirectional(layers.LSTM(units, return_sequences=True))(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = layers.Bidirectional(layers.LSTM(lstm_units[-1]))(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.load_weights('lstm_model_weights/best_lstm_model.h5')\n",
    "    # print(model.summary())\n",
    "    print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == ModelRunMode.CLASSIFY.value:\n",
    "    X_test = prepare_data_test(X_test, tokenizer, max_length)\n",
    "    print(\"X_test shape:\", X_test.shape)\n",
    "    preds = model.predict(X_test)\n",
    "    preds = [1 if pred > 0.5 else 0 for pred in preds]\n",
    "    print(np.unique(preds, return_counts=True))\n",
    "    preds_str = ['AI' if pred == 1 else 'Human' for pred in preds]\n",
    "    result = pd.DataFrame({'ID': ids, 'Label': preds_str})\n",
    "    result.to_csv(output_csv, index=False, sep='\\t')\n",
    "    print(\"Prediction saved successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
