{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 120\n",
    "n = 50000  # Set the number of most frequent words to keep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model run enum\n",
    "class ModelRunMode(Enum):\n",
    "    \"\"\"\n",
    "    Enumeration of Model Run Mode.\n",
    "    \"\"\"\n",
    "    TRAIN           = \"train\"           # Train Mode\n",
    "    CLASSIFY        = \"classify\"        # Classify Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model run mode\n",
    "# Options: \n",
    "#   ModelRunMode.TRAIN.value            (Train the model)\n",
    "#   ModelRunMode.CLASSIFY.value         (Classify data)\n",
    "mode = ModelRunMode.TRAIN.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters cell\n",
    "if mode == ModelRunMode.CLASSIFY.value:\n",
    "    # CLASSIFY mode: Set parameters for classification\n",
    "    input_csv = \"classify_input_datasets/submission3_inputs.csv\"               # CSV file with texts for prediction (ID, Text)\n",
    "    output_csv = \"classify_output_datasets/submission3_outputs_lstm_model.csv\" # CSV file to store prediction result\n",
    "elif mode == ModelRunMode.TRAIN.value:\n",
    "    seed = 42                                                                                   # Global seed for reproducibility across Python, NumPy, and TensorFlow\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)                                                        # Set environment variable for Python's internal hash seed for determinism\n",
    "    os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"                                                      # Attempt to force cuDNN to be deterministic\n",
    "    random.seed(seed)                                                                               # Set Python's built-in random module seed\n",
    "    np.random.seed(seed)                                                                            # Set NumPy random seed\n",
    "    tf.random.set_seed(seed)                                                                        # Set TensorFlow random seed\n",
    "else:\n",
    "    print(\"The selected option is not valid. Options: \\\"train\\\" or \\\"classify\\\"!\")\n",
    "    SystemExit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining global fuctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text, stopwords = True):\n",
    "    def normalize(text):\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove numbers, special characters, e o caractere '\n",
    "        text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "        # Replace multiple spaces with a single space\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    def remove_stopwords(text):\n",
    "        stopwords = [\n",
    "        \"the\", \"of\", \"and\", \"in\", \"to\", \"is\", \"a\", \"that\", \"for\", \"are\", \"on\", \"with\", \n",
    "        \"as\", \"at\", \"by\", \"from\", \"this\", \"it\", \"an\", \"be\", \"or\", \"which\", \"was\", \"were\"\n",
    "        ]\n",
    "        text = ' '.join([word for word in text.split() if word not in stopwords])\n",
    "        return text\n",
    "    \n",
    "    text = normalize(text)\n",
    "    if stopwords:\n",
    "        text = remove_stopwords(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, texts, n_words=None):\n",
    "        self.n_words = n_words\n",
    "        self.texts = texts\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = set()\n",
    "        self.create_index()\n",
    "        \n",
    "\n",
    "    def create_index(self):\n",
    "        word_counter = Counter()\n",
    "        for caption in self.texts:\n",
    "            for word in caption.split():\n",
    "                word_counter[word] += 1\n",
    "\n",
    "        # Sort words by frequency and alphabetically for ties\n",
    "        most_common = word_counter.most_common(self.n_words) if self.n_words else word_counter.items()\n",
    "        self.vocab = [word for word, _ in sorted(most_common, key=lambda x: (-x[1], x[0]))]\n",
    "\n",
    "        # Add special tokens\n",
    "        self.word2idx['<pad>'] = 0\n",
    "        self.word2idx['<unk>'] = 1\n",
    "        for index, word in enumerate(self.vocab):\n",
    "            self.word2idx[word] = index + 2\n",
    "\n",
    "        for word, index in self.word2idx.items():\n",
    "            self.idx2word[index] = word\n",
    "\n",
    "\n",
    "    def encode(self, caption):\n",
    "        tokens = []\n",
    "        for word in caption.split():\n",
    "            if word in self.word2idx:\n",
    "                tokens.append(self.word2idx[word])\n",
    "            else:\n",
    "                tokens.append(self.word2idx['<unk>'])\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return ' '.join([self.idx2word.get(token, '<unk>') for token in tokens])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, tokenizer,max_length=120):\n",
    "    X = df['Text']\n",
    "    y = df['Label']\n",
    "    X = X.apply(lambda x: tokenizer.encode(x))\n",
    "    X = pad_sequences(X, maxlen=max_length, padding='pre')\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN MODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening and cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('../tarefa_1/test_input_dataset/merged_inputs.csv', sep='\\t')\n",
    "y = pd.read_csv('../tarefa_1/test_output_dataset/merged_outputs.csv', sep='\\t')\n",
    "df = pd.merge(X, y, on=\"ID\", how=\"inner\")\n",
    "\n",
    "df['Text'] = df['Text'].str.replace('\\n', ' ')\n",
    "df['Text'] = df['Text'].astype(str)\n",
    "df['Label'] = df['Label'].apply(lambda x: 1 if x == \"AI\" else 0)\n",
    "df['Text'] = df['Text'].apply(clean)\n",
    "\n",
    "# Divisão em treino (70%) e validação (30%)\n",
    "train_df, val_df = train_test_split(df, test_size=0.3, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the vocabulary: 12582\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(df['Text'], n)\n",
    "vocab_size = len(tokenizer)\n",
    "print('Number of words in the vocabulary:', len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = prepare_data(df, tokenizer, max_length)\n",
    "X_val, y_val = prepare_data(val_df, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    4    3  130 1663 2043   17\n",
      "    7 1162  401 3614  911  271    5  851   81   37  155  192  248   13\n",
      "   33   58   70  932  153 2043   45    8  459 2043   34    7 1162  172\n",
      "  750  191  331 3614  309  271  434    5 1337 2043  102    6  931 1610\n",
      " 1279  897   38   11   29    4   40  143   43   20   59  333  248    7\n",
      " 1162   18    6  315   97  110    7  219]\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> research paper focuses investigating charging behavior quantum dot coupled luttinger liquid leads study employs theoretical approach analyze electron transport properties system explores effects coulomb interactions charging dynamics results reveal charging energy quantum dot significantly influenced coupling strength luttinger parameter leads furthermore study indicates charging process can controlled tuning gate voltage magnetic field findings research provide valuable insights into physics charge transport quantum dot systems can useful development future quantum devices\n"
     ]
    }
   ],
   "source": [
    "print(X_val[-1])\n",
    "print(tokenizer.decode(X_val[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tune function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_optimization_lstm(X_train, y_train, X_val, y_val,\n",
    "                                     epochs_list, batch_size_list, \n",
    "                                     learning_rate_list, lstm_units_list,\n",
    "                                     embed_dim_list, \n",
    "                                     dropout_list, n_iter=10):\n",
    "\n",
    "    keras.backend.clear_session()\n",
    "    best_acc = 0.0\n",
    "    best_params = {}\n",
    "\n",
    "    # Split entire dataset into train_val vs test\n",
    "    X_combined = np.concatenate([X_train, X_val])\n",
    "    y_combined = np.concatenate([y_train, y_val])\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X_combined, y_combined, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "    # Prepare random combinations\n",
    "    param_combinations = []\n",
    "    for _ in range(n_iter):\n",
    "        param_combinations.append({\n",
    "            'epochs': random.choice(epochs_list),\n",
    "            'batch_size': random.choice(batch_size_list),\n",
    "            'learning_rate': random.choice(learning_rate_list),\n",
    "            'lstm_units': random.choice(lstm_units_list),\n",
    "            'dropout_rate': random.choice(dropout_list),\n",
    "            'embed_dim': random.choice(embed_dim_list)\n",
    "        })\n",
    "\n",
    "    for i,params in enumerate(param_combinations):\n",
    "        print(f\"\\nIteration {i+1}/{n_iter}\", end=' ')\n",
    "        # Unpack parameters\n",
    "        epochs = params['epochs']\n",
    "        batch_size = params['batch_size']\n",
    "        learning_rate = params['learning_rate']\n",
    "        lstm_units = params['lstm_units']\n",
    "        dropout_rate = params['dropout_rate']\n",
    "        embed_dim = params['embed_dim']\n",
    "\n",
    "        # Batch the datasets\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((X_train_val, y_train_val)).batch(batch_size)\n",
    "        validation_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size)\n",
    "\n",
    "        # Extract input shape and number of classes\n",
    "        input_shape = train_ds.element_spec[0].shape\n",
    "        print(\"Input Shape:\", input_shape)\n",
    "\n",
    "        # Build LSTM model\n",
    "        inputs = keras.Input(shape=(input_shape[1],))  # Assuming (timesteps, features)\n",
    "\n",
    "        x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
    "\n",
    "        for units in lstm_units[:-1]:\n",
    "            x = layers.Bidirectional(layers.LSTM(units, return_sequences=True))(x)\n",
    "            x = layers.Dropout(dropout_rate)(x)\n",
    "        \n",
    "        x = layers.Bidirectional(layers.LSTM(lstm_units[-1]))(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "        outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_accuracy', patience=2, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=2, verbose=0, min_lr=1e-2)\n",
    "        ]\n",
    "\n",
    "        model.compile(optimizer=optimizer,\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        # Train the model\n",
    "        history = model.fit(train_ds, epochs=epochs, batch_size=batch_size,\n",
    "                            verbose=0, validation_data=validation_ds, callbacks=callbacks)\n",
    "\n",
    "        # Evaluate final val accuracy\n",
    "        val_loss, val_acc = model.evaluate(X_val, y_val, verbose=True)\n",
    "\n",
    "        print(\"Params:\", params, \"| Val Acc = {:.4f}\".format(val_acc))\n",
    "\n",
    "        # Keep track of best accuracy (if two combinations yield the same accuracy, we pick the one with the lower validation loss)\n",
    "        if (val_acc > best_acc) or (val_acc == best_acc and val_loss < best_loss):\n",
    "            best_acc = val_acc\n",
    "            best_loss = val_loss\n",
    "            best_params = params\n",
    "            best_model = model\n",
    "    \n",
    "    print(\"Best val acc = {:.4f}\".format(best_acc))\n",
    "    print(\"Best hyperparams:\", best_params)\n",
    "\n",
    "    # Evaluate the best model\n",
    "    loss, accuracy = best_model.evaluate(validation_ds)\n",
    "    \n",
    "    # Get predictions\n",
    "    y_true = np.concatenate([y.numpy() for _, y in validation_ds], axis=0)\n",
    "    y_pred_probs = best_model.predict(validation_ds)\n",
    "    y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "    # Calculate metrics\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nBest Model Evaluation:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Loss: {loss:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "\n",
    "    return best_params, best_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 1/15 Input Shape: (None, 120)\n",
      "18/18 [==============================] - 1s 15ms/step - loss: 0.0188 - accuracy: 0.9947\n",
      "Params: {'epochs': 4, 'batch_size': 16, 'learning_rate': 0.001, 'lstm_units': [32, 32], 'dropout_rate': 0.1, 'embed_dim': 100} | Val Acc = 0.9947\n",
      "\n",
      "Iteration 2/15 Input Shape: (None, 120)\n",
      "18/18 [==============================] - 1s 15ms/step - loss: 0.0144 - accuracy: 0.9965\n",
      "Params: {'epochs': 4, 'batch_size': 16, 'learning_rate': 0.001, 'lstm_units': [32, 32], 'dropout_rate': 0.1, 'embed_dim': 100} | Val Acc = 0.9965\n",
      "\n",
      "Iteration 3/15 Input Shape: (None, 120)\n",
      "18/18 [==============================] - 1s 16ms/step - loss: 0.0739 - accuracy: 0.9894\n",
      "Params: {'epochs': 4, 'batch_size': 16, 'learning_rate': 0.01, 'lstm_units': [32, 32], 'dropout_rate': 0.2, 'embed_dim': 100} | Val Acc = 0.9894\n",
      "\n",
      "Iteration 4/15 Input Shape: (None, 120)\n",
      "18/18 [==============================] - 1s 15ms/step - loss: 0.0997 - accuracy: 0.9876\n",
      "Params: {'epochs': 8, 'batch_size': 16, 'learning_rate': 0.01, 'lstm_units': [32, 32], 'dropout_rate': 0.2, 'embed_dim': 150} | Val Acc = 0.9876\n",
      "\n",
      "Iteration 5/15 Input Shape: (None, 120)\n",
      "18/18 [==============================] - 1s 16ms/step - loss: 0.0297 - accuracy: 0.9947\n",
      "Params: {'epochs': 8, 'batch_size': 16, 'learning_rate': 0.01, 'lstm_units': [64, 32], 'dropout_rate': 0.1, 'embed_dim': 100} | Val Acc = 0.9947\n",
      "\n",
      "Iteration 6/15 Input Shape: (None, 120)\n",
      "18/18 [==============================] - 2s 17ms/step - loss: 0.0224 - accuracy: 0.9947\n",
      "Params: {'epochs': 8, 'batch_size': 16, 'learning_rate': 0.001, 'lstm_units': [64, 32], 'dropout_rate': 0.2, 'embed_dim': 100} | Val Acc = 0.9947\n",
      "\n",
      "Iteration 7/15 Input Shape: (None, 120)\n",
      "18/18 [==============================] - 2s 16ms/step - loss: 0.0293 - accuracy: 0.9929\n",
      "Params: {'epochs': 8, 'batch_size': 16, 'learning_rate': 0.001, 'lstm_units': [32, 32], 'dropout_rate': 0.2, 'embed_dim': 150} | Val Acc = 0.9929\n",
      "\n",
      "Iteration 8/15 Input Shape: (None, 120)\n",
      "18/18 [==============================] - 1s 15ms/step - loss: 0.0438 - accuracy: 0.9947\n",
      "Params: {'epochs': 4, 'batch_size': 16, 'learning_rate': 0.01, 'lstm_units': [32, 32], 'dropout_rate': 0.2, 'embed_dim': 100} | Val Acc = 0.9947\n",
      "\n",
      "Iteration 9/15 Input Shape: (None, 120)\n",
      "18/18 [==============================] - 2s 17ms/step - loss: 0.0183 - accuracy: 0.9947\n",
      "Params: {'epochs': 4, 'batch_size': 16, 'learning_rate': 0.001, 'lstm_units': [64, 32], 'dropout_rate': 0.2, 'embed_dim': 150} | Val Acc = 0.9947\n",
      "\n",
      "Iteration 10/15 Input Shape: (None, 120)\n",
      "18/18 [==============================] - 1s 15ms/step - loss: 0.0193 - accuracy: 0.9947\n",
      "Params: {'epochs': 4, 'batch_size': 16, 'learning_rate': 0.001, 'lstm_units': [32, 32], 'dropout_rate': 0.2, 'embed_dim': 100} | Val Acc = 0.9947\n",
      "\n",
      "Iteration 11/15 Input Shape: (None, 120)\n",
      "18/18 [==============================] - 1s 16ms/step - loss: 0.0593 - accuracy: 0.9876\n",
      "Params: {'epochs': 4, 'batch_size': 16, 'learning_rate': 0.01, 'lstm_units': [64, 32], 'dropout_rate': 0.2, 'embed_dim': 150} | Val Acc = 0.9876\n",
      "\n",
      "Iteration 12/15 Input Shape: (None, 120)\n",
      "18/18 [==============================] - 1s 15ms/step - loss: 0.0611 - accuracy: 0.9841\n",
      "Params: {'epochs': 4, 'batch_size': 16, 'learning_rate': 0.01, 'lstm_units': [32, 32], 'dropout_rate': 0.1, 'embed_dim': 150} | Val Acc = 0.9841\n",
      "\n",
      "Iteration 13/15 Input Shape: (None, 120)\n",
      "18/18 [==============================] - 1s 16ms/step - loss: 0.0543 - accuracy: 0.9929\n",
      "Params: {'epochs': 8, 'batch_size': 16, 'learning_rate': 0.01, 'lstm_units': [32, 32], 'dropout_rate': 0.2, 'embed_dim': 100} | Val Acc = 0.9929\n",
      "\n",
      "Iteration 14/15 Input Shape: (None, 120)\n",
      "18/18 [==============================] - 1s 16ms/step - loss: 0.0428 - accuracy: 0.9929\n",
      "Params: {'epochs': 8, 'batch_size': 16, 'learning_rate': 0.001, 'lstm_units': [32, 32], 'dropout_rate': 0.2, 'embed_dim': 100} | Val Acc = 0.9929\n",
      "\n",
      "Iteration 15/15 Input Shape: (None, 120)\n",
      "18/18 [==============================] - 1s 17ms/step - loss: 0.0242 - accuracy: 0.9929\n",
      "Params: {'epochs': 4, 'batch_size': 16, 'learning_rate': 0.001, 'lstm_units': [64, 32], 'dropout_rate': 0.2, 'embed_dim': 100} | Val Acc = 0.9929\n",
      "Best val acc = 0.9965\n",
      "Best hyperparams: {'epochs': 4, 'batch_size': 16, 'learning_rate': 0.001, 'lstm_units': [32, 32], 'dropout_rate': 0.1, 'embed_dim': 100}\n",
      "31/31 [==============================] - 0s 16ms/step - loss: 0.0781 - accuracy: 0.9837\n",
      "31/31 [==============================] - 1s 13ms/step\n",
      "\n",
      "Best Model Evaluation:\n",
      "Accuracy: 0.9837\n",
      "Loss: 0.0781\n",
      "Precision: 0.9835\n",
      "Recall: 0.9835\n",
      "F1-score: 0.9835\n"
     ]
    }
   ],
   "source": [
    "if mode == ModelRunMode.TRAIN.value:\n",
    "    # Hyperparameter search\n",
    "    epochs_list = [4, 8]\n",
    "    batch_size_list = [16]\n",
    "    learning_rate_list = [1e-2, 1e-3]\n",
    "    lstm_units_list = [[32, 32], [64, 32]]\n",
    "    embed_dim_list = [100, 150]\n",
    "    dropout_list = [0.1, 0.2]\n",
    "    hiperparams, model = hyperparameter_optimization_lstm(X_train, y_train, X_val, y_val,\n",
    "                                                        epochs_list, batch_size_list,\n",
    "                                                        learning_rate_list, lstm_units_list,\n",
    "                                                        embed_dim_list, dropout_list, n_iter=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 120)]             0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 120, 100)          1258200   \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 120, 64)          34048     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 120, 64)           0         \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 64)               24832     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,325,529\n",
      "Trainable params: 1,325,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if mode == ModelRunMode.TRAIN.value:\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == ModelRunMode.TRAIN.value:\n",
    "    model.save('lstm_model_weights/best_lstm_model.h5')\n",
    "    # save params and tokenizer\n",
    "    import pickle\n",
    "    with open('lstm_model_weights/best_lstm_model_params.pkl', 'wb') as f:\n",
    "        pickle.dump(hiperparams, f)\n",
    "\n",
    "    with open('lstm_model_weights/tokenizer_lstm.pkl', 'wb') as f:\n",
    "        pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFY MODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_test(X, tokenizer,max_length=max_length):\n",
    "    X = X.apply(lambda x: tokenizer.encode(x))\n",
    "    X = pad_sequences(X, maxlen=max_length, padding='pre')\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == ModelRunMode.CLASSIFY.value:\n",
    "    X_test = pd.read_csv(input_csv,sep='\\t', index_col=False)['Text']\n",
    "    ids = pd.read_csv(input_csv,sep='\\t', index_col=False)['ID']\n",
    "    X_test = X_test.str.replace('\\n', ' ')\n",
    "    X_test = X_test.astype(str)\n",
    "    X_test = X_test.apply(clean)\n",
    "    print(\"CSV loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == ModelRunMode.CLASSIFY.value:\n",
    "    # open best params and tokenizer\n",
    "    with open('lstm_model_weights/best_lstm_model_params.pkl', 'rb') as f:\n",
    "        hiperparams = pickle.load(f)\n",
    "\n",
    "    with open('lstm_model_weights/tokenizer_lstm.pkl', 'rb') as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "\n",
    "    input_shape = (max_length,)\n",
    "    vocab_size = len(tokenizer)\n",
    "    epochs = hiperparams['epochs']\n",
    "    batch_size = hiperparams['batch_size']\n",
    "    learning_rate = hiperparams['learning_rate']\n",
    "    lstm_units = hiperparams['lstm_units']\n",
    "    dropout_rate = hiperparams['dropout_rate']\n",
    "    embed_dim = hiperparams['embed_dim']\n",
    "\n",
    "    # Build LSTM model\n",
    "    inputs = keras.Input(shape=input_shape)  # Assuming (timesteps, features)\n",
    "\n",
    "    x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
    "\n",
    "    for units in lstm_units[:-1]:\n",
    "        x = layers.Bidirectional(layers.LSTM(units, return_sequences=True))(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = layers.Bidirectional(layers.LSTM(lstm_units[-1]))(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.load_weights('lstm_model_weights/best_lstm_model.h5')\n",
    "    #print(model.summary())\n",
    "    print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == ModelRunMode.CLASSIFY.value:\n",
    "    X_test = prepare_data_test(X_test, tokenizer, max_length)\n",
    "    print(\"X_test shape:\", X_test.shape)\n",
    "    preds = model.predict(X_test)\n",
    "    preds = [1 if pred > 0.5 else 0 for pred in preds]\n",
    "    print(np.unique(preds, return_counts=True))\n",
    "    preds_str = ['AI' if pred == 1 else 'Human' for pred in preds]\n",
    "    result = pd.DataFrame({'ID': ids, 'Label': preds_str})\n",
    "    result.to_csv(output_csv, index=False, sep='\\t')\n",
    "    print(\"Prediction saved successfully\")\n",
    "\n",
    "    # Load the validation dataset\n",
    "    df_true = pd.read_csv(\"../tarefa_1/validation_dataset/dataset3_disclosed_output.csv\", sep=\"\\t\")\n",
    "\n",
    "    # Merge the datasets on the \"ID\" column, adding suffixes to distinguish the identical column names\n",
    "    df_merged = pd.merge(df_true, result, on=\"ID\", suffixes=('_true', '_pred'))\n",
    "\n",
    "    # Calculate the number of correct predictions by comparing the \"Label\" columns\n",
    "    num_correct = (df_merged[\"Label_true\"] == df_merged[\"Label_pred\"]).sum()\n",
    "\n",
    "    # Calculate the percentage of correct predictions\n",
    "    accuracy_percentage = (num_correct / len(df_merged)) * 100\n",
    "\n",
    "    print(f\"Accuracy: {accuracy_percentage:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
