{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e117c95a",
      "metadata": {},
      "source": [
        "# Robustly Optimized BERT Pretraining Approach (RoBERTa) Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f7f3362",
      "metadata": {},
      "source": [
        "```md\n",
        "@authors: Grupo 03\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "00553163",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Notebook Imports\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import re\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import shutil\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tensorflow import keras\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from enum import Enum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7cee9d6c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model run enum\n",
        "class ModelRunMode(Enum):\n",
        "    \"\"\"\n",
        "    Enumeration of Model Run Mode.\n",
        "    \"\"\"\n",
        "    TRAIN           = \"train\"           # Train Mode\n",
        "    CLASSIFY        = \"classify\"        # Classify Mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc6f6268",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model run mode\n",
        "# Options: \n",
        "#   ModelRunMode.TRAIN.value            (Train the model)\n",
        "#   ModelRunMode.CLASSIFY.value         (Classify data)\n",
        "mode = ModelRunMode.TRAIN.value\n",
        "# Prefix for saving the model files\n",
        "model_prefix = \"llm_roberta_model\"\n",
        "file_path = \"llm_roberta_model_weights\"\n",
        "separator_char = \"\\t\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6adb728",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters cell\n",
        "if mode == ModelRunMode.TRAIN.value:\n",
        "    # TRAIN mode: Set parameters for training\n",
        "    input_csv = \"../tarefa_1/clean_input_datasets/gpt_vs_human_data_set_inputs.csv\"     # CSV file with training inputs (ID, Text)\n",
        "    output_csv = \"../tarefa_1/clean_output_datasets/gpt_vs_human_data_set_outputs.csv\"  # CSV file with training outputs (ID, Label)\n",
        "    test_size = 0.3                                                                     # Proportion of the dataset to use as test data\n",
        "    random_state=42                                                                     # Seed for reproducible dataset splitting\n",
        "elif mode == ModelRunMode.CLASSIFY.value:\n",
        "    # CLASSIFY mode: Set parameters for classification\n",
        "    input_csv = \"../tarefa_3/classify_input_datasets/dataset3_inputs.csv\"                       # CSV file with texts for prediction (ID, Text)\n",
        "    output_csv = \"../tarefa_3/classify_output_datasets/dataset3_outputs_llm_roberta_model.csv\"  # CSV file to store prediction result\n",
        "else:\n",
        "    print(\"The selected option is not valid. Options: \\\"train\\\" or \\\"classify\\\"!\")\n",
        "    SystemExit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7713c68b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method to load and merge two datasets by ID column\n",
        "def merge_data_by_id(input_csv, output_csv, sep=\"\\t\"):\n",
        "    df_in = pd.read_csv(input_csv, sep=sep)\n",
        "    df_out = pd.read_csv(output_csv, sep=sep)\n",
        "\n",
        "    # Remove duplicates or NaNs if needed\n",
        "    df_in.dropna(subset=[\"ID\", \"Text\"], inplace=True)\n",
        "    df_out.dropna(subset=[\"ID\", \"Label\"], inplace=True)\n",
        "    df_in.drop_duplicates(subset=[\"ID\"], inplace=True)\n",
        "    df_out.drop_duplicates(subset=[\"ID\"], inplace=True)\n",
        "\n",
        "    df_merged = pd.merge(df_in, df_out, on=\"ID\", how=\"inner\")\n",
        "    return df_merged\n",
        "\n",
        "# Method for text cleaning\n",
        "def text_cleaning(text):\n",
        "        # Download required NLTK resources\n",
        "        nltk.download('stopwords', quiet=True)\n",
        "        nltk.download('punkt_tab', quiet=True)\n",
        "        nltk.download('wordnet', quiet=True)\n",
        "        # Convert text to lowercase\n",
        "        text = text.lower()\n",
        "        # Remove URLs\n",
        "        text = re.sub(r'http[s]?://\\S+', \"\", text)\n",
        "        # Remove HTML tags\n",
        "        text = re.sub(r\"<[^>]*>\", \"\", text)\n",
        "        # Remove common LaTeX commands\n",
        "        text = re.sub(r\"\\\\[a-zA-Z]+(\\{.*?\\})?\", \"\", text)\n",
        "        # Remove email addresses\n",
        "        text = re.sub(r'\\S+@\\S+', \"\", text)\n",
        "        # Remove punctuation\n",
        "        text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "        # Remove digits\n",
        "        text = re.sub(r\"\\d+\", \"\", text)\n",
        "        # Replace newlines and extra whitespace with a single space\n",
        "        text = re.sub(r\"\\s+\", \" \", text).replace('\\n', \" \")\n",
        "        # Trim leading and trailing whitespace\n",
        "        text = text.strip()\n",
        "        # Tokenize text and remove stopwords using NLTK's English stopwords list\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        # Tokenize text\n",
        "        tokens = word_tokenize(text)\n",
        "        # Remove stopwords\n",
        "        filtered_tokens = [tok for tok in tokens if tok not in stop_words]\n",
        "        # Lemmatize tokens\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        lemmatized_tokens = [lemmatizer.lemmatize(tok) for tok in filtered_tokens]\n",
        "        # Return the cleaned text as a string\n",
        "        return \" \".join(lemmatized_tokens)\n",
        "\n",
        "# Method to convert labels to binary\n",
        "def convert_labels_to_binary_and_text(df_merged):\n",
        "    df_merged[\"Label\"] = df_merged[\"Label\"].str.lower().str.strip()\n",
        "    y = np.where(df_merged[\"Label\"] == \"ai\", 1, 0)\n",
        "    texts = df_merged[\"Text\"].tolist()\n",
        "    return y, texts\n",
        "\n",
        "# Method to plot the learning curves\n",
        "def plot_learning_curves(history):\n",
        "    # Loss\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    if 'val_loss' in history.history:\n",
        "        plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "    plt.title(\"Loss\")\n",
        "    plt.legend()\n",
        "    \n",
        "    # Accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='Train Acc')\n",
        "    if 'val_accuracy' in history.history:\n",
        "        plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
        "    plt.title(\"Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Method to check label distribution\n",
        "def check_label_distribution(df_merged):\n",
        "    label_counts = df_merged[\"Label\"].value_counts(dropna=False)\n",
        "    print(\"Label distribution:\\n\", label_counts)\n",
        "\n",
        "# Method to print the first 5 cleaned texts\n",
        "def debug_text_cleaning(df_merged):\n",
        "    for i in range(min(5, len(df_merged))):\n",
        "        print(df_merged[\"Text\"].iloc[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "98f21a18",
      "metadata": {},
      "outputs": [],
      "source": [
        "# BERT Model training function\n",
        "def train_roberta(input_csv, output_csv, model_ckpt=\"roberta-base\", output_dir=file_path, model_prefix= model_prefix, num_train_epochs=3, batch_size=8, test_size=0.2, learning_rate=1e-3, random_state=48):\n",
        "    # Load data\n",
        "    df_merged = merge_data_by_id(input_csv, output_csv, sep=separator_char)\n",
        "\n",
        "    # Check label distribution\n",
        "    check_label_distribution(df_merged=df_merged)\n",
        "\n",
        "    # Text cleaning\n",
        "    df_merged[\"Text\"] = df_merged[\"Text\"].apply(text_cleaning)\n",
        "\n",
        "    # Print the first 5 cleaned texts\n",
        "    debug_text_cleaning(df_merged)\n",
        "\n",
        "    # Convert label/text\n",
        "    labels, texts = convert_labels_to_binary_and_text(df_merged)\n",
        "\n",
        "    # Split entire dataset into train_val vs test\n",
        "    X_train_texts, X_val_texts, y_train, y_val = train_test_split(texts, labels, test_size=test_size, random_state=random_state, stratify=labels)\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "\n",
        "    # Tokenize without truncation/padding to compute lengths\n",
        "    raw_train_encodings = tokenizer(X_train_texts, add_special_tokens=True, truncation=False, padding=False)\n",
        "    token_lengths = [len(seq) for seq in raw_train_encodings[\"input_ids\"]]\n",
        "\n",
        "    # Calculate max_length based on the 90th percentile\n",
        "    max_length = int(np.percentile(token_lengths, 90))\n",
        "\n",
        "    # Tokenize with truncation and padding using the calculated max_length\n",
        "    train_encodings = tokenizer(X_train_texts, truncation=True, padding=\"max_length\", max_length=max_length)\n",
        "    val_encodings = tokenizer(X_val_texts, truncation=True, padding=\"max_length\", max_length=max_length)\n",
        "\n",
        "    # Convert to tf.data.Dataset\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train))\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings), y_val))\n",
        "\n",
        "    # Batch the datasets\n",
        "    train_dataset = train_dataset.batch(batch_size)\n",
        "    val_dataset = val_dataset.batch(batch_size)\n",
        "\n",
        "    # Load model\n",
        "    model = TFAutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=2)\n",
        "\n",
        "    # Compile with Keras optimizer, loss, and metric\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
        "\n",
        "    # Add early stopping\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "\n",
        "    # Fit the model\n",
        "    history = model.fit(train_dataset, validation_data=val_dataset, epochs=num_train_epochs, callbacks=[early_stopping])\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    val_loss, val_acc = model.evaluate(val_dataset)\n",
        "    print(f\"\\nValidation Accuracy: {val_acc:.4f} | Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Display model sumary\n",
        "    model.summary()\n",
        "\n",
        "    # Plot training curves\n",
        "    plot_learning_curves(history)\n",
        "\n",
        "    # If the output_dir already exists, remove it entirely\n",
        "    if os.path.exists(output_dir):\n",
        "        shutil.rmtree(output_dir)\n",
        "\n",
        "    # Now recreate the folder\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Construct the file paths\n",
        "    model_path = os.path.join(output_dir, f\"{model_prefix}_model\")\n",
        "    tokenizer_path = os.path.join(output_dir, f\"{model_prefix}_tokenizer\")\n",
        "    config_path = os.path.join(output_dir, f\"{model_prefix}_config.json\")\n",
        "\n",
        "    # Save the model weights\n",
        "    print(\"Saving model to:\", model_path)\n",
        "    model.save_pretrained(model_path)\n",
        "\n",
        "    # Save the model tokenizer\n",
        "    print(\"Saving tokenizer to:\", tokenizer_path)\n",
        "    tokenizer.save_pretrained(tokenizer_path)\n",
        "\n",
        "    # Save configuration\n",
        "    print(\"Save configuration to:\", config_path)\n",
        "    config_data = {\n",
        "        \"model_ckpt\": model_ckpt,\n",
        "        \"num_train_epochs\": num_train_epochs,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"max_length\": max_length\n",
        "    }\n",
        "    \n",
        "    with open(config_path, \"w\") as f:\n",
        "        json.dump(config_data, f)\n",
        "\n",
        "    # Print end message\n",
        "    print(f\"RoBERTa Model, tokenizer and configuration stored under {output_dir}. Finished Training!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "43fa4fd7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Classification function\n",
        "def classify_roberta(input_csv, output_csv, output_dir=\"llm_roberta_model_weights\", separator_char=separator_char):\n",
        "    # Construct the file paths\n",
        "    model_path = os.path.join(output_dir, f\"{model_prefix}_model\")\n",
        "    tokenizer_path = os.path.join(output_dir, f\"{model_prefix}_tokenizer\")\n",
        "    config_path = os.path.join(output_dir, f\"{model_prefix}_config.json\")\n",
        "\n",
        "    # Load the model\n",
        "    print(f\"Loading model from: {output_dir}\")\n",
        "    model = TFAutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "    \n",
        "    # Load the tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "\n",
        "    # Loading configuration\n",
        "    print(\"Loading configuration from:\", config_path)\n",
        "    with open(config_path, \"r\") as f:\n",
        "         config_data = json.load(f)\n",
        "\n",
        "    # Retrieve the saved configuration max_len\n",
        "    max_length = config_data[\"max_length\"]\n",
        "\n",
        "    # Read the input CSV\n",
        "    df_input = pd.read_csv(input_csv, sep=separator_char)\n",
        "    if \"ID\" not in df_input.columns or \"Text\" not in df_input.columns:\n",
        "        raise ValueError(\"Input CSV must have 'ID' and 'Text' columns for classification.\")\n",
        "    \n",
        "    # Extract texts\n",
        "    texts = df_input[\"Text\"].astype(str).tolist()\n",
        "    \n",
        "    # Tokenize\n",
        "    encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length)\n",
        "    \n",
        "    # Create tf.data.Dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(dict(encodings))\n",
        "    dataset = dataset.batch(16)\n",
        "    \n",
        "    # Predict probabilities (logits => sigmoid)\n",
        "    outputs = model.predict(dataset)\n",
        "\n",
        "    if isinstance(outputs, dict) and \"logits\" in outputs:\n",
        "        logits = outputs[\"logits\"]\n",
        "    else:\n",
        "        logits = outputs\n",
        "    \n",
        "    # Convert logits to probabilities\n",
        "    probs = tf.nn.sigmoid(logits[:, 1]).numpy()\n",
        "    \n",
        "    # Threshold => \"AI\" vs \"Human\"\n",
        "    pred_bin = (probs >= 0.5).astype(int)\n",
        "    pred_labels = np.where(pred_bin == 1, \"AI\", \"Human\")\n",
        "    \n",
        "    # Save result\n",
        "    df_out = pd.DataFrame({\"ID\": df_input[\"ID\"], \"Label\": pred_labels})\n",
        "    df_out.to_csv(output_csv, sep=separator_char, index=False)\n",
        "    print(f\"Predictions saved to {output_csv}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e633c6f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "if mode == ModelRunMode.TRAIN.value:\n",
        "    # Check if Tensorflow is listing available GPUs (if not, continue with CPU)\n",
        "    print(\"Tensorflow List of GPUs:\", tf.config.list_physical_devices('GPU'))\n",
        "    \n",
        "    # Train model\n",
        "    train_roberta(input_csv=input_csv, output_csv=output_csv, model_ckpt=\"roberta-base\", output_dir=file_path, model_prefix=model_prefix, num_train_epochs=5, batch_size=16, test_size=test_size, learning_rate=1e-5, random_state=random_state)\n",
        "\n",
        "if mode == ModelRunMode.CLASSIFY.value:\n",
        "    # Classification\n",
        "    classify_roberta(input_csv=input_csv, output_csv=output_csv, output_dir=file_path, separator_char=separator_char)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
