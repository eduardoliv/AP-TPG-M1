ID	Text
D1-1	The cell cycle, or cell-division cycle, is the sequential series of events that take place in a cell that causes it to divide into two daughter cells. These events include the growth of the cell, duplication of its DNA (DNA replication) and some of its organelles, and subsequently the partitioning of its cytoplasm, chromosomes and other components into two daughter cells in a process called cell division. In eukaryotic cells (having a cell nucleus) including animal, plant, fungal, and protist cells, the cell cycle is divided into two main stages: interphase, and the M phase that includes mitosis and cytokinesis.
D1-2	The cell cycle is the process by which a cell grows, duplicates its DNA, and divides into two daughter cells. It is essential for growth, tissue repair, and reproduction. The cycle consists of four main phases. In G₁ phase, the cell grows, produces proteins, and prepares for DNA replication. During the S phase, DNA is replicated to ensure that each daughter cell receives an identical copy. The G2 phase follows, where the cell continues growing and checks for DNA damage before proceeding to division. Finally, in the M phase, the cell undergoes mitosis, where chromosomes are separated, and cytokinesis, where the cytoplasm splits.
D1-3	Photons, in many atomic models in physics, are particles which transmit light. In other words, light is carried over space by photons. Photon is an elementary particle that is its own antiparticle. In quantum mechanics each photon has a characteristic quantum of energy that depends on frequency: A photon associated with light at a higher frequency will have more energy (and be associated with light at a shorter wavelength).Photons have a rest mass of 0 (zero). However, Einstein's theory of relativity says that they do have a certain amount of momentum. Before the photon got its name, Einstein revived the proposal that light is separate pieces of energy (particles). These particles came to be known as photons.
D1-4	A photon is a fundamental particle of light and other electromagnetic radiation. It has no mass, no electric charge, and always moves at the speed of light (299,792,458 m/s in a vacuum). Photons carry energy and momentum, which depend on their wavelength or frequency. Higher frequency photons (like X-rays) have more energy, while lower frequency photons (like radio waves) have less. Their energy is given by Planck’s equation:E = h f, where E is energy, h is Planck’s constant, and f is frequency.Photons behave as both particles and waves (wave-particle duality), meaning they can interfere, diffract, and be absorbed/emitted like particles. They are responsible for vision, photosynthesis, solar power, and many quantum phenomena.
D1-5	According to the theory of plate tectonics, Earth's lithosphere, its rigid outer shell, is broken into sixteen larger and several smaller plates. These move continuously at a slow pace, due to convection in the underlying ductile mantle, and most volcanic activity on Earth takes place along plate boundaries, where plates are converging (and lithosphere is being destroyed) or are diverging (and new lithosphere is being created). During the development of geological theory, certain concepts that allowed the grouping of volcanoes in time, place, structure and composition have developed that ultimately have had to be explained in the theory of plate tectonics.
D1-6	The theory of plate tectonics explains that Earth’s lithosphere is divided into moving plates that float on the semi-fluid asthenosphere. These movements shape the planet, causing earthquakes, volcanic activity, and mountain formation. The theory builds on continental drift, proposed by Alfred Wegener, and is supported by evidence from seafloor spreading, fossil distribution, and geological formations. Plates move apart at divergent boundaries, collide at convergent boundaries, and slide past each other at transform boundaries. Their movement is driven by mantle convection, gravity, and Earth’s rotation, constantly reshaping the surface. This theory is essential for understanding natural disasters and the geological evolution of continents.
D1-7	Thalidomide is a pharmaceutical drug, first prepared in 1957 in Germany, prescribed for treating morning sickness in pregnant women. The drug was discovered to be teratogenic, causing serious genetic damage to early embryonic growth and development, leading to limb deformation in babies. Several proposed mechanisms of teratogenicity involve different biological functions for the (R)- and (S)-thalidomide enantiomers.In the human body, however, thalidomide undergoes racemization: even if only one of the two enantiomers is administered as a drug, the other enantiomer is produced as a result of metabolism. Thalidomide is currently used for the treatment of other diseases, notably cancer and leprosy. Strict regulations and controls have been implemented to avoid its use by pregnant women and prevent developmental deformities.
D1-8	Thalidomide is a drug that was first developed in the 1950s as a sedative and later prescribed to pregnant women for morning sickness. However, it caused severe birth defects, including missing or malformed limbs, when taken during pregnancy. The tragedy affected thousands of babies and led to stricter drug regulations worldwide. Despite its harmful past, thalidomide was later found to have anti-inflammatory and immunomodulatory properties. Today, it is used under strict controls to treat multiple myeloma, leprosy-related inflammation, and some autoimmune diseases. Due to its risks, it is only prescribed under a controlled program to prevent use during pregnancy. Its history remains a major lesson in pharmaceutical safety.
D1-9	Kepler published his first two laws about planetary motion in 1609, having found them by analyzing the astronomical observations of Tycho Brahe.Kepler's third law was published in 1619. Kepler had believed in the Copernican model of the Solar System, which called for circular orbits, but he could not reconcile Brahe's highly precise observations with a circular fit to Mars' orbit – Mars coincidentally having the highest eccentricity of all planets except Mercury. His first law reflected this discovery. In 1621, Kepler noted that his third law applies to the four brightest moons of Jupiter. Godefroy Wendelin also made this observation in 1643.
D1-10	Kepler’s laws of planetary motion are three fundamental principles that describe the motion of planets around the Sun. These laws were formulated by the German astronomer Johannes Kepler in the early 17th century, building on the detailed observations of the astronomer Tycho Brahe. Kepler’s work came at a time when the heliocentric model (Sun-centered solar system) proposed by Copernicus was still controversial. Kepler initially struggled with the Copernican model, but after inheriting Brahe’s precise astronomical data, he was able to make groundbreaking discoveries. Kepler’s first law, the Law of Ellipses, stated that planets move in elliptical orbits with the Sun at one focus. The second, the Law of Equal Areas, explained that planets sweep out equal areas in equal times.
D1-11	The basic idea of biological evolution is that populations and species of organisms change over time. Today, when we think of evolution, we are likely to link this idea with one specific person: the British naturalist Charles Darwin. In the 1850s, Darwin wrote an influential and controversial book called On the Origin of Species. In it, he proposed that species evolve (or, as he put it, undergo "descent with modification"), and that all living things can trace their descent to a common ancestor. Darwin also suggested a mechanism for evolution: natural selection, in which heritable traits that help organisms survive and reproduce become more common in a population over time.
D1-12	Biological evolution, according to Charles Darwin, refers to the process by which species of organisms change over time through variations in traits that are passed down from one generation to the next. Darwin’s theory of evolution by natural selection suggests that individuals within a species show variation in their characteristics. These variations can affect their ability to survive and reproduce in their environment. Those individuals with traits that give them a survival or reproductive advantage are more likely to pass those traits on to their offspring, while less advantageous traits are gradually eliminated. Over long periods, this process leads to the accumulation of beneficial traits in the population, resulting in the adaptation of species to their environment.
D1-13	Stoichiometry rests upon the very basic laws that help to understand it better, i.e., law of conservation of mass, the law of definite proportions (i.e., the law of constant composition), the law of multiple proportions and the law of reciprocal proportions. In general, chemical reactions combine in definite ratios of chemicals. Since chemical reactions can neither create nor destroy matter, nor transmute one element into another, the amount of each element must be the same throughout the overall reaction. For example, the number of atoms of a given element X on the reactant side must equal the number of atoms of that element on the product side, whether or not all of those atoms are actually involved in a reaction.
D1-14	Stoichiometry is the branch of chemistry that deals with the calculation of reactants and products in chemical reactions. It is based on the concept that matter is conserved during a reaction, meaning the quantity of reactants used equals the quantity of products formed. Stoichiometry involves using balanced chemical equations to determine the molar ratios between substances, allowing the calculation of how much of each reactant is needed and how much product will be formed. This includes conversions between moles, mass, and volume. Stoichiometric calculations are essential for understanding reaction yields, determining the limiting reactant, and ensuring efficient use of materials in chemical processes.
D1-15	General relativity is a theory of gravitation developed by Einstein in the years 1907–1915. The development of general relativity began with the equivalence principle, under which the states of accelerated motion and being at rest in a gravitational field (for example, when standing on the surface of the Earth) are physically identical. The upshot of this is that free fall is inertial motion: an object in free fall is falling because that is how objects move when there is no force being exerted on them, instead of this being due to the force of gravity as is the case in classical mechanics.
D1-16	General relativity, also known as the general theory of relativity, is a geometric theory of gravitation developed by Albert Einstein between 1907 and 1915. It provides a unified description of gravity as a property of spacetime, which is a four-dimensional continuum combining space and time. The theory posits that massive objects cause a curvature in spacetime, and this curvature influences the motion of objects, effectively manifesting as gravity. Einstein developed general relativity to address limitations in Newton's law of universal gravitation, particularly in explaining anomalies like the precession of Mercury's orbit. The theory has been extensively tested and confirmed through observations of gravitational waves, the bending of light around massive objects, and the behavior of objects in strong gravitational fields
D1-17	The recommended number of eggs to consume per week varies depending on several factors, including individual health, dietary preferences, and overall dietary patterns. It's important to note that while eggs are a nutritious food, they are also relatively high in dietary cholesterol. According to general guidelines, including those provided by the American Heart Association and the Dietary Guidelines for Americans, consuming up to seven eggs per week is considered reasonable and can be part of a healthy diet for most people. However, it's important to consider the overall context of your diet, including other sources of cholesterol and saturated fat, as well as your individual health goals and any specific dietary restrictions or considerations you may have.
D1-18	The question of how many eggs one should eat per day is often debated due to concerns about cholesterol and cardiovascular health. Current research suggests that consuming up to three eggs per day can be part of a healthy diet for most people, particularly young, healthy adults. Consuming up to three eggs per day has been shown to increase HDL (good) cholesterol and improve the LDL/HDL ratio, which are favorable changes for cardiovascular health. For most healthy individuals, consuming up to three eggs per day can be beneficial, improving cholesterol profiles and providing essential nutrients without increasing cardiovascular risk. However, individuals with specific health conditions, such as cardiovascular disease or diabetes, should consider their overall dietary patterns.
D1-19	There seems to be some merit to having a single daily glass of red wine, it seems to have something called resveratrol, which acts as an antioxidant and is thought to be good for your heart. Additionally, Italian researchers found that moderate beer drinkers had a 42 percent lower risk of heart disease compared to non-drinkers. For maximum protection, keep your consumption to one pint—at around 5 percent alcohol by volume—a day, the researchers say. Basically, alcohol, in moderation, appears to lower risks of heart and other cardiovascular disease.But this is in moderation, mind you. Just as with everything else, the only difference between medicine and poison is the dosage. A beer a day is good; twelve is not.
D1-20	No level of alcohol consumption is considered entirely safe for health. According to the World Health Organization (WHO), even low levels of alcohol use can increase the risk of certain cancers and other health problems. While some guidelines suggest limits to reduce risks, these guidelines are based on minimizing risk rather than ensuring safety. Recent research indicates that even moderate drinking may increase the risk of chronic diseases and premature death compared to abstaining. Therefore, the safest approach is to avoid alcohol altogether. Alcohol consumption significantly impacts mental health, often exacerbating existing conditions and contributing to new ones. Alcohol initially acts as a depressant, reducing inhibitions and creating a temporary sense of relaxation or confidence.
D1-21	There are three major compounds of life: proteins; lipids and carbohydrates. The perception of sweet taste, mainly associated with advantageous food, has had an important evolutionary influence on different physiological regulation mechanisms. During human development, sugar was always luxury. In 1885 Constantin Fahlberg produced the first artificial sweetener, saccharin, and the scientific establishment was surprised by its extreme sweetness. Significant to this discovery was the fact that sweet taste became affordable to poor people. Following the commercial success of artificial sweeteners, a battle between the sugar and sweetener industries began. Saccharin was claimed to be carcinogenic in rats. However, it was later shown that saccharin is neither toxic nor carcinogenic in normal amounts, yet its reputation remains tarnished.
D1-22	Artificial sweeteners are low- or zero-calorie sugar substitutes used to sweeten foods and beverages without the added calories of sugar. Common examples include aspartame, sucralose, saccharin, and stevia (a natural alternative). They are widely used in diet sodas, sugar-free gum, and diabetic-friendly products. While they provide sweetness without spiking blood sugar levels, they remain controversial. Studies show they are safe in moderation, but concerns exist about potential long-term effects on gut health and metabolism. Some people experience sensitivity to specific sweeteners. Ultimately, artificial sweeteners can be helpful for reducing calorie intake, but their impact varies widely from person to person.
D1-23	Recent findings on the ecology, etiology and pathology of coral pathogens, host resistance mechanisms, previously unknown disease/syndromes and the global nature of coral reef diseases have increased our concern about the health and future of coral reef communities. Much of what has been discovered in the past 4 years is presented in this special issue. Among the significant findings, the role that various Vibrio species play in coral disease and health, the composition of the ‘normal microbiota’ of corals, and the possible role of viruses in the disease process are important additions to our knowledge.  reefs and a major cause of reef deterioration.
D1-24	Coral reefs are among the most biodiverse and vital ecosystems on the planet, supporting millions of marine species and providing essential services like coastal protection and livelihoods for humans. However, their health is rapidly declining due to climate change, ocean acidification, overfishing, and pollution. Rising sea temperatures cause coral bleaching, where corals expel the algae they depend on for energy, often leading to mass die-offs. Ocean acidification, driven by increased CO₂ absorption, weakens coral skeletons, further threatening reef stability. The future of coral reefs depends on urgent conservation efforts. Strategies like reducing carbon emissions, establishing marine protected areas, and coral restoration projects (e.g., coral farming) offer hope. Research into resilient coral species and reef rehabilitation techniques is also promising.
D1-25	Although the earliest life seems to have arisen quite soon in Earth’s history, less than 800 million years after its formation, simple forms continued for 1.6 billion years until the multicellular Francevillian biota developed and quickly died out about 2.1 billion years ago. After that, although multicellular forms were in existence from about 1.7 billion years ago, it was not until the Cambrian ‘explosion’ from 540–500 million years ago that multicellular life expanded and prospered. This is essentially what we know. But fossilisation is rare and early life has resulted in tiny traces  difficult to interpret. At present there are believed to be about one trillion species on Earth, less than one per cent of those which have ever lived.
D1-26	The question of how life appeared on Earth is one of science’s greatest mysteries. While there is no definitive answer, scientists propose various theories based on evidence and hypotheses.VPrimordial Soup Theory: This idea suggests that life began in a "soup" of organic molecules in early Earth's oceans. Energy from lightning, volcanic activity, or UV radiation may have triggered chemical reactions, forming basic building blocks of life like amino acids and nucleotides. Experiments like the 1953 Miller-Urey experiment support this theory by showing that organic molecules can form under simulated early Earth conditions. Hydrothermal Vent Hypothesis: Some researchers believe life began near deep-sea hydrothermal vents, where hot, mineral-rich water provided the energy and raw materials for life.
D1-27	We investigate the problem about the reason for the significant sub-centurial (50-55 and 60-65 yr) and quasi-centurial (120-130 yr) climatic oscillations. The 50-55 and 60-65 yr cycles are clearly detectable in numerous global and regional climatic parameters, i.e. in the temperatures of the Northern Hemisphere and World Ocean surface, in the tree rings widths, in the atmosphere concentration of CO2, etc. Searching for analogues of these cycles in the solar activity we study the connections between various types of solar, geophysical and climatic cycles. In this Paper I we analyze time series of residual variations of the Northern Hemisphere (AD 1610-1979) and World Ocean surface (AD 18561995) temperatures in respect to the corresponding regression models "sunspot activity – temperature".
D1-28	Significant climatic oscillations on sub-centurial (50–55 years, 60–65 years) and quasi-centurial (120–130 years) timescales are thought to arise from natural variability in Earth's climate system, driven by a combination of internal processes and external forcing mechanisms. The causes of these oscillations include: Ocean-Atmosphere Interactions Atlantic Multidecadal Oscillation (AMO) and Pacific Decadal Oscillation (PDO) are large-scale oceanic patterns of variability that operate on multidecadal timescales (50–70 years). These affect global heat distribution, weather patterns, and long-term climate trends. Solar Variability Variations in solar activity, such as the Gleissberg Cycle (~80–120 years), impact Earth's climate. Changes in solar irradiance affect atmospheric dynamics, ocean circulation, and surface temperatures.
D1-29	The COVID-19 was detected in Wuhan, central China’s Hubei Province, but that doesn’t mean the novel coronavirus had originated from China. It may have originated from somewhere else. To determine the origin of an unknown virus requires scientific, meticulous research, instead of random speculation. The source of the virus is a scientific issue and requires a professional investigation. Without evidence supported by data or research, no one can claim where it had originated from. Connecting a virus with a region may cause xenophobic behavior. The pandemic influenza of 2009 originated in North America, but it was not called the “North America flu.” That is why the World Health Organization named the novel coronavirus “COVID-19,” which avoids stigmatizing a particularly region.
D1-30	The origin of the COVID-19 pandemic, caused by SARS-CoV-2, remains uncertain, with two main theories under discussion. The dominant hypothesis suggests a natural origin through zoonotic spillover, where the virus jumped from bats, its likely reservoir, to humans, possibly via an intermediate species such as pangolins. This theory links the early outbreak to a seafood market in Wuhan, China, where live wild animals were sold. Another theory suggests an accidental leak from a laboratory, like the Wuhan Institute of Virology, which studies bat coronaviruses, though no direct evidence supports this claim. While most scientists favor the natural origin hypothesis, limited data and restricted early access make the exact source of the virus difficult to confirm.
D1-31	An experimental demonstration of a novel all-optical technique for loading ion traps, that has particular application to microtrap architectures, is presented. The technique is based on photo-ionisation of an atomic beam created by pulsed laser ablation of a calcium target, and provides improved temporal control compared to traditional trap loading methods. Ion loading rates as high as 125 ions per second have so far been observed. Also described are observations of trap loading where Rydberg state atoms are photo-ionised by the ion Doppler cooling laser.
D1-32	This research paper presents an all-optical ion-loading technique for scalable microtrap architectures. The technique utilizes a combination of optical tweezers and laser cooling to trap and cool individual ions, which can then be loaded into a microtrap. This approach eliminates the need for complex and expensive ion-loading mechanisms, allowing for the creation of more compact and scalable microtraps. The paper also explores the performance of the microtrap using this technique, demonstrating its potential for use in a variety of applications in quantum computing and precision measurement.
D1-33	We demonstrate that bistability of the nuclear spin polarization in optically pumped semiconductor quantum dots is a general phenomenon possible in dots with a wide range of parameters. In experiment, this bistability manifests itself via the hysteresis behavior of the electron Zeeman splitting as a function of either pump power or external magnetic field. In addition, our theory predicts that the nuclear polarization can strongly influence the charge dynamics in the dot leading to bistability in the average dot charge.
D1-34	The research paper titled "Bistability of optically-induced nuclear spin orientation in quantum dots" investigates the behavior of nuclear spins in quantum dots under various optical and magnetic conditions. The primary subject of the study is the bistability of nuclear spin orientation, which refers to the ability of the system to exist in two stable states with different spin orientations. The researchers found that by controlling the optical and magnetic fields, they could induce and manipulate the bistability of nuclear spin orientation in the quantum dots. They also discovered that the duration of the spin polarization was dependent on the strength of the magnetic field. The study's main conclusion is that the bistability of nuclear spin orientation in quantum dots can be controlled and utilized for various practical applications, such as quantum information processing and spin-based electronics.
D1-35	We investigate some fundamental features of a class of non-linear relativistic lagrangian field theories with kinetic self-coupling. We focus our attention upon theories admitting static, spherically symmetric solutions in three space dimensions which are finite-energy and stable. We determine general conditions for the existence and stability of these non-topological soliton solutions. In particular, we perform a linear stability analysis that goes beyond the usual Derrick-like criteria. On the basis of these considerations we obtain a complete characterization of the soliton-supporting members of the aforementioned class of non-linear field theories. We then classify the family of soliton-supporting theories according to the central and asymptotic behaviors of the soliton field, and provide illustrative explicit examples of models belonging to each of the corresponding sub-families. In the present work we restrict most of our considerations to one and many-components scalar models.
D1-36	This research paper investigates the existence and properties of non-topological solitons in field theories with kinetic self-coupling. Such solitons are localized, stable configurations of the field that do not rely on topological considerations for their stability. The study is conducted by solving the equations of motion for the field and analyzing the resulting solutions. The paper explores the conditions under which non-topological solitons can arise and examines their properties, such as their energy, size, and stability. The results of this research can have significant implications for various fields of physics, including particle physics and condensed matter physics.
D1-37	The Galactic center (GC) is a dense and chaotic region filled with unusual sources, such as intense star forming regions, dense star clusters, nonthermal radio filaments, and a massive black hole. The proximity of the GC region makes it an ideal place to study the unusual processes that tend to manifest themselves in Galactic nuclei. This thesis uses single-dish and interferometric radio continuum, radio recombination line, polarized radio continuum, and mid-IR observations to study the wide variety of physical processes seen in the GC region on physical scales from 0.1 to 100 parsecs. These observations provide one of the most sensitive studies of the radio continuum emission in the central 500 parsecs. I also study the properties of nonthermal radio filaments, which can constrain their origin and the structure of the magnetic field in the GC region. The presence of massive star clusters and a massive black hole suggest that starburst and AGN phenomena can manifest themselves in our Galaxy.
D1-38	This research paper provides an overview of recent surveys of the Galactic Center and their implications for understanding the nature of the Galactic Center lobe. The paper highlights key discoveries related to the morphology, kinematics, and energetics of the lobe, and discusses their relevance for models of the Milky Way's central region. The authors conclude that the lobe is likely a result of past accretion events and feedback from the central black hole, and suggest that future observations will help to refine our understanding of its properties and evolution.
D1-39	This document reports on a series of experimental and theoretical studies conducted to assess the astro-particle physics potential of three future large-scale particle detectors proposed in Europe as next generation underground observatories. The proposed apparatus employ three different and, to some extent, complementary detection techniques: GLACIER (liquid Argon TPC), LENA (liquid scintillator) and MEMPHYS (\WC), based on the use of large mass of liquids as active detection media. The results of these studies are presented along with a critical discussion of the performance attainable by the three proposed approaches coupled to existing or planned underground laboratories, in relation to open and outstanding physics issues such as the search for matter instability, the detection of astrophysical- and geo-neutrinos and to the possible use of these detectors in future high-intensity neutrino beams.
D1-40	This research paper presents the scientific case and prospects for large underground, liquid-based detectors for astro-particle physics in Europe. The paper discusses the potential for these detectors to make groundbreaking discoveries in the fields of neutrino and dark matter research, as well as their ability to contribute to the study of supernovae, solar neutrinos, and geoneutrinos. The authors conclude that these detectors are crucial for advancing our understanding of the universe and recommend continued investment in their development and operation.
D1-41	In this paper, the equations of motion for geodesics in the neutral rotating Black Ring metric are derived and the separability of these equations is considered. The bulk of the paper is concerned with sets of solutions where the geodesic equations can be examined analytically - specifically geodesics confined to the axis of rotation, geodesics restricted to the equatorial plane, and geodesics that circle through the centre of the ring. The geodesics on the rotational axis behave like a particle in a potential well, while the geodesics confined to the equatorial plane mimic those of the Schwarzschild metric. It is shown that it is impossible to have circular orbits that pass through the ring, but some numerical results are presented which suggest that it is possible to have bound orbits that circle through the ring.
D1-42	This paper investigates particle motion in the rotating black ring metric, a solution to the equations of General Relativity describing a five-dimensional black hole with a ring topology around its event horizon. The authors derive the equations of motion for test particles moving in this metric and analyze their behavior in various regimes, including geodesic motion, circular orbits, and chaotic trajectories. They find that the black ring metric exhibits several interesting features, such as the existence of stable circular orbits in the plane of rotation and the appearance of chaotic behavior for particles with nonzero angular momentum. These findings provide new insights into the dynamics of black hole spacetimes and may have implications for astrophysical phenomena such as accretion disks and binary black hole mergers.
D1-43	A recent radio survey of globular clusters has increased the number of millisecond pulsars drastically. M28 is now the globular cluster with the third largest population of known pulsars, after Terzan 5 and 47 Tuc. This prompted us to revisit the archival Chandra data on M28 to evaluate whether the newly discovered millisecond pulsars find a counterpart among the various X-ray sources detected in M28 previously. The radio position of PSR J1824-2452H is found to be in agreement with the position of CXC 182431-245217 while some faint unresolved X-ray emission near to the center of M28 is found to be coincident with the millisecond pulsars PSR J1824-2452G, J1824-2452J, J1824-2452I and J1824-2452E.
D1-44	We reconstruct a rational Lax matrix of size R+1 from its spectral curve (the desingularization of the characteristic polynomial) and some additional data. Using a twisted Cauchy--like kernel (a bi-differential of bi-weight (1-nu,nu)) we provide a residue-formula for the entries of the Lax matrix in terms of bases of dual differentials of weights nu and 1-nu respectively. All objects are described in the most explicit terms using Theta functions. Via a sequence of ``elementary twists'', we construct sequences of Lax matrices sharing the same spectral curve and polar structure and related by conjugations by rational matrices. Particular choices of elementary twists lead to construction of sequences of Lax matrices related to finite--band recurrence relations (i.e. difference operators) sharing the same shape. Recurrences of this kind are satisfied by several types of orthogonal and biorthogonal polynomials.
D1-45	The paper "Effective inverse spectral problem for rational Lax matrices and applications" presents a new method for solving the inverse spectral problem for rational Lax matrices. The method is based on the theory of algebraic curves and uses a combination of geometric and analytical techniques. The authors apply this method to two important problems in mathematical physics: the nonlinear Schr dinger equation and the Korteweg-de Vries equation. They show that the new method provides an effective and efficient way to solve these problems, and that it can be used to obtain new insights into the behavior of these equations. The main findings of the paper include a new formula for the spectral curve of the rational Lax matrix, a new approach to computing the Baker-Akhiezer function for the nonlinear Schr dinger equation, and a new method for finding the soliton solutions of the Korteweg-de Vries equation.
D1-46	In this paper, the bit energy requirements of training-based transmission over block Rayleigh fading channels are studied. Pilot signals are employed to obtain the minimum mean-square-error (MMSE) estimate of the channel fading coefficients. Energy efficiency is analyzed in the worst case scenario where the channel estimate is assumed to be perfect and the error in the estimate is considered as another source of additive Gaussian noise. It is shown that bit energy requirement grows without bound as the snr goes to zero, and the minimum bit energy is achieved at a nonzero snr value below which one should not operate. The effect of the block length on both the minimum bit energy and the snr value at which the minimum is achieved is investigated. Flash training schemes are analyzed and shown to improve the energy efficiency in the low-snr regime. Energy efficiency analysis is also carried out when peak power constraints are imposed on pilot signals.
D1-47	The research paper titled "An Energy Efficiency Perspective on Training for Fading Channels" focuses on the tradeoff between energy efficiency and channel estimation accuracy in wireless communication systems. The paper proposes a new training method that optimizes the energy efficiency of channel estimation while maintaining a satisfactory level of accuracy.The key findings of the research are that traditional training methods consume a significant amount of energy and do not necessarily result in the highest accuracy. The proposed method reduces energy consumption by adjusting the number of pilot symbols transmitted, while maintaining an acceptable level of accuracy.The paper concludes that the proposed method can significantly improve the energy efficiency of wireless communication systems, especially in scenarios where energy is a limiting factor. It also highlights the importance of considering energy efficiency in the design of wireless communication systems.
D1-48	The low-snr capacity of M-ary PSK transmission over both the additive white Gaussian noise (AWGN) and fading channels is analyzed when hard-decision detection is employed at the receiver. Closed-form expressions for the first and second derivatives of the capacity at zero snr are obtained. The spectral-efficiency/bit-energy tradeoff in the low-snr regime is analyzed by finding the wideband slope and the bit energy required at zero spectral efficiency. Practical design guidelines are drawn from the information-theoretic analysis. The fading channel analysis is conducted for both coherent and noncoherent cases, and the performance penalty in the low-power regime for not knowing the channel is identified.
D1-49	In this paper, pilot-assisted transmission over Gauss-Markov Rayleigh fading channels is considered. A simple scenario, where a single pilot signal is transmitted every T symbols and T-1 data symbols are transmitted in between the pilots, is studied. First, it is assumed that binary phase-shift keying (BPSK) modulation is employed at the transmitter. With this assumption, the training period, and data and training power allocation are jointly optimized by maximizing an achievable rate expression. Achievable rates and energy-per-bit requirements are computed using the optimal training parameters. Secondly, a capacity lower bound is obtained by considering the error in the estimate as another source of additive Gaussian noise, and the training parameters are optimized by maximizing this lower bound.
D1-50	This research paper focuses on optimizing training sequences for Gauss-Markov Rayleigh fading channels. The study proposes a new algorithm for designing training sequences based on the maximum likelihood estimation of channel parameters. The algorithm is compared with existing methods and is shown to provide better performance in terms of channel estimation accuracy. The study concludes that the proposed algorithm can significantly enhance the performance of wireless communication systems operating in fading channels.
D1-51	In this paper, the error performance of on-off frequency shift keying (OOFSK) modulation over fading channels is analyzed when the receiver is equipped with multiple antennas. The analysis is conducted in two cases: the coherent scenario where the fading is perfectly known at the receiver, and the noncoherent scenario where neither the receiver nor the transmitter knows the fading coefficients. For both cases, the maximum a posteriori probability (MAP) detection rule is derived and analytical probability of error expressions are obtained. The effect of fading correlation among the receiver antennas is also studied. Simulation results indicate that for sufficiently low duty cycle values, lower probability of error values with respect to FSK signaling are achieved. Equivalently, when compared to FSK modulation, OOFSK with low duty cycle requires less energy to achieve the same probability of error, which renders this modulation a more energy efficient transmission technique.
D1-52	This research paper presents a performance analysis for multichannel reception of Orthogonal Offset Frequency Shift Keying (OOFSK) signaling. The main theme of the study is to investigate the impact of the number of channels on the system's performance in terms of bit error rate (BER) and throughput. The researchers conducted simulations using different numbers of channels and varying signal-to-noise ratios (SNRs). The results show that increasing the number of channels improves the system's performance, particularly at low SNRs. However, the improvement in BER saturates beyond a certain number of channels. Furthermore, the study also reveals that increasing the number of channels may not necessarily lead to an increase in throughput. The research provides valuable insights into the design and optimization of multichannel communication systems using OOFSK signaling.
D1-53	In this paper, the performance of signaling strategies with high peak-to-average power ratio is analyzed in both coherent and noncoherent fading channels. Two recently proposed modulation schemes, namely on-off binary phase-shift keying and on-off quaternary phase-shift keying, are considered. For these modulation formats, the optimal decision rules used at the detector are identified and analytical expressions for the error probabilities are obtained. Numerical techniques are employed to compute the error probabilities. It is concluded that increasing the peakedness of the signals results in reduced error rates for a given power level and hence improve the energy efficiency.
D1-54	This research paper analyses the error probability of peaky signaling over fading channels. The study considers the effects of fading on the peaky signaling, and derives the closed-form expressions for the error probability of the peaky signaling over Rayleigh and generalized fading channels. The results show that the error probability of the peaky signaling is significantly affected by the channel fading. It is also found that the performance of the peaky signaling is improved by increasing the peakiness factor. The findings of this study provide insights for the design of reliable communication systems over fading channels using peaky signaling.
D1-55	The effective Lagrangian of electromagnetic fields at the cubic order in field strength has been considered. This generalized Lagrangian is motivated by electrodynamics on non-commutative spaces. We find the canonical and symmetrical energy-momentum tensors and show that they possess non-zero traces. The propagation of a linearly polarized laser beam in the external transverse magnetic field is investigated. We evaluate the induced ellipticity which allows us to obtain a constraint on parameters introduced from the PVLAS experimental data.
D1-56	The effective Lagrangian at cubic order in electromagnetic fields and vacuum birefringence is a topic of significant interest in the field of quantum electrodynamics. This research paper aims to explore the theoretical framework for deriving the effective Lagrangian at cubic order and its implications for the phenomenon of vacuum birefringence. The paper will provide a detailed analysis of the relevant mathematical and physical concepts, including the use of Feynman diagrams and perturbative expansions. The paper will also discuss the experimental evidence for vacuum birefringence and its potential applications in testing fundamental physics theories. Overall, this research paper contributes to the ongoing efforts to understand the fundamental properties of the universe through the study of quantum electrodynamics.
D1-57	A Brownian Motor is a nanoscale or molecular device that combines the effects of thermal noise, spatial or temporal asymmetry, and directionless input energy to drive directed motion. Because of the input energy, Brownian motors function away from thermodynamic equilibrium and concepts such as linear response theory, fluctuation dissipation relations, and detailed balance do not apply. The {\em generalized} fluctuation-dissipation relation, however, states that even under strongly thermodynamically non-equilibrium conditions the ratio of the probability of a transition to the probability of the time-reverse of that transition is the exponent of the change in the internal energy of the system due to the transition. Here, we derive an extension of the generalized fluctuation dissipation theorem for a Brownian motor for the ratio between the probability for the motor to take a forward step and the probability to take a backward step.
D1-58	The research paper titled "Symmetry Relations for Trajectories of a Brownian Motor" explores the properties of a Brownian motor, which is a device that can convert thermal energy into mechanical work. The authors investigate the symmetry properties of the motor's trajectory using mathematical models and simulations. The main finding of the study is that the motor's trajectory exhibits a remarkable symmetry, which is related to the underlying symmetries of the motor's structure. The authors also show that this symmetry can be broken by external perturbations, which can lead to changes in the motor's efficiency. Overall, the study provides new insights into the fundamental properties of Brownian motors and their potential applications in nanotechnology and biophysics.
D1-59	We study Casimir forces on the partition in a closed box (piston) with perfect metallic boundary conditions. Related closed geometries have generated interest as candidates for a repulsive force. By using an optical path expansion we solve exactly the case of a piston with a rectangular cross section, and find that the force always attracts the partition to the nearest base. For arbitrary cross sections, we can use an expansion for the density of states to compute the force in the limit of small height to width ratios. The corrections to the force between parallel plates are found to have interesting dependence on the shape of the cross section. Finally, for temperatures in the range of experimental interest we compute finite temperature corrections to the force (again assuming perfect boundaries).
D1-60	This research paper investigates the Casimir forces in a piston geometry at zero and finite temperatures. We study the effect of temperature on the Casimir force, which is the force that arises between two closely spaced parallel plates due to the fluctuations of the electromagnetic field. We use the scattering approach to calculate the Casimir force for different piston geometries, including a rectangular piston and a cylindrical piston. Our results show that the Casimir force decreases with increasing temperature and that the geometry of the piston has a significant effect on the magnitude of the force. We also investigate the role of surface roughness on the Casimir force and find that it can have a significant impact on the force at low temperatures. These findings have important implications for the design and optimization of nanoscale devices that rely on the Casimir force for their operation.
D1-61	We extend recent work that included the effect of pressure forces to derive the precession rate of eccentric accretion discs in cataclysmic variables to the case of double degenerate systems. We find that the logical scaling of the pressure force in such systems results in predictions of unrealistically high primary masses. Using the prototype AM CVn as a calibrator for the magnitude of the effect, we find that there is no scaling that applies consistently to all the systems in the class. We discuss the reasons for the lack of a superhump period to mass ratio relationship analogous to that known for SU UMa systems and suggest that this is because these secondaries do not have a single valued mass-radius relationship. We highlight the unreliability of mass-ratios derived by applying the SU UMa expression to the AM CVn binaries.
D1-62	Inspired by a recent astro-ph posting, I propose a creation of an Alternative History astro-ph archive (althistastro-ph). Such an archive would serve as a final resting place for the various telescope (and possibly other) proposals that were not successful. As we all know, from both submitting proposals and also from serving on various time allocation committees, many excellent proposals ``do not make it''. Creating such an AltHist archive would serve many goals, including venting the frustration of the authors and also providing possible amusement for the readers. These are worthy goals, but they alone would not warrant creating such an archive. The truly useful role of AltHistAstro-ph archive would be to match astronomers with unappreciated ideas with other astronomers with underutilized resources, hopefully leading in some cases to resurrection of old proposals and resulting publications in the regular astro-ph archive.
D1-63	The field of astronomy is constantly evolving with new discoveries and advancements in technology. However, the limited funding and resources allocated to the astronomical community have hindered its progress. In this research paper, a modest proposal is presented to address this issue. The proposal suggests a collaborative effort between government agencies, private organizations, and individual donors to establish a sustainable funding model for the astronomical community. The paper also discusses the potential benefits of this proposal, including increased research opportunities and a more comprehensive understanding of the universe. Furthermore, the proposal emphasizes the need for transparency and accountability in the distribution of funds. The research paper concludes by acknowledging the challenges associated with implementing such a proposal but highlights the importance of investing in the advancement of astronomy for the betterment of society as a whole.
D1-64	Aims: In the context of space interferometry missions devoted to the search of exo-Earths, this paper investigates the capabilities of new single mode conductive waveguides at providing modal filtering in an infrared and monochromatic nulling experiment; Methods: A Michelson laser interferometer with a co-axial beam combination scheme at 10.6 microns is used. After introducing a Pi phase shift using a translating mirror, dynamic and static measurements of the nulling ratio are performed in the two cases where modal filtering is implemented and suppressed. No additional active control of the wavefront errors is involved. Results: We achieve on average a statistical nulling ratio of 2.5e-4 with a 1-sigma upper limit of 6e-4, while a best null of 5.6e-5 is obtained in static mode.
D1-65	This research paper presents a mid-infrared laser light nulling experiment using single-mode conductive waveguides. The objective of this study is to investigate the feasibility of nulling laser light at mid-infrared wavelengths using waveguides. The experiment was conducted using a CO2 laser and a single-mode conductive waveguide made of silicon. The results of the experiment show that the waveguide successfully nulls the laser light at mid-infrared wavelengths. The study concludes that single-mode conductive waveguides can be an effective approach for nulling laser light at mid-infrared wavelengths, which has important implications for applications in astronomy and remote sensing.
D1-66	Semiclassical reasoning suggests that the process by which an object collapses into a black hole and then evaporates by emitting Hawking radiation may destroy information, a problem often referred to as the black hole information paradox. Further, there seems to be no unique prediction of where the information about the collapsing body is localized. We propose that the latter aspect of the paradox may be a manifestation of an inconsistent self-reference in the semiclassical theory of black hole evolution. This suggests the inadequacy of the semiclassical approach or, at worst, that standard quantum mechanics and general relavity are fundamentally incompatible. One option for the resolution for the paradox in the localization is to identify the G\"odel-like incompleteness that corresponds to an imposition of consistency, and introduce possibly new physics that supplies this incompleteness.
D1-67	In this paper we outline several points of view on the interplay between discrete and continuous wavelet transforms; stressing both pure and applied aspects of both. We outline some new links between the two transform technologies based on the theory of representations of generators and relations. By this we mean a finite system of generators which are represented by operators in Hilbert space. We further outline how these representations yield sub-band filter banks for signal and image processing algorithms.
D1-68	This research paper compares the discrete wavelet transform (DWT) and continuous wavelet transform (CWT) for signal analysis. The study analyzes the performance of both transforms in terms of time and frequency resolution, computational complexity, and noise robustness. The results show that CWT provides better time-frequency resolution compared to DWT, but at a higher computational cost. However, DWT is found to be more robust to noise. The study concludes that the choice of transform depends on the specific application and the trade-off between resolution and computational cost.
D1-69	We propose a generic test for models in which gravity is modified to do away with dark matter. These models tend to have gravitons couple to a different metric than ordinary matter. A strong test of such models comes from comparing the arrival time of the gravitational wave pulse from a cosmological event such as a supernova with the arrival times of the associated pulses of neutrinos and photons. For SN 1987a we show that the gravity wave would have arrived 5.3 days after the neutrino pulse.
D1-70	This research paper presents a generic test for modified gravity models that emulate dark matter. The study focuses on the galactic dynamics of dwarf spheroidal galaxies and utilizes the Jeans equation to derive the velocity dispersion profiles. The authors compare the predictions of modified gravity models to those of dark matter models and find that some modified gravity models can accurately reproduce the observed velocity dispersion profiles without the need for dark matter. However, the study also highlights the limitations of the generic test and emphasizes the need for further testing and refinement of modified gravity models. The results suggest that modified gravity models have the potential to provide an alternative explanation for the observed galactic dynamics, but more research is needed to fully understand their implications.
D1-71	We study the IR-through-UV interstellar extinction curves towards 328 Galactic B and late-O stars. We use a new technique which employs stellar atmosphere models in lieu of unreddened "standard" stars. This technique is capable of virtually eliminating spectral mismatch errors in the curves. It also allows a quantitative assessment of the errors and enables a rigorous testing of the significance of relationships between various curve parameters, regardless of whether their uncertainties are correlated. Analysis of the curves gives the following results: (1) In accord with our previous findings, the central position of the 2175 A extinction bump is mildly variable, its width is highly variable, and the two variations are unrelated. (2) Strong correlations are found among some extinction properties within the UV region, and within the IR region. (3) With the exception of a few curves with extreme (i.e., large) values of R(V), the UV and IR portions of Galactic extinction curves are not correlated with each other.
D1-72	The research paper titled "An Analysis of the Shapes of Interstellar Extinction Curves. V. The IR-Through-UV Curve Morphology" focuses on analyzing the shapes of interstellar extinction curves. The primary theme of this research is to study the morphology of IR-through-UV extinction curves and investigate their variations in different environments. The research findings suggest that the shape of the extinction curve depends on the physical properties of the interstellar environment, such as the size distribution of dust grains and the abundance of heavy elements. The conclusions drawn from this research provide valuable insights into the nature and evolution of interstellar dust and the interstellar medium.
D1-73	Using the Rossi X-ray Timing Explorer (RossiXTE), astronomers have discovered that disk-accreting neutron stars with weak magnetic fields produce three distinct types of high-frequency X-ray oscillations. These oscillations are powered by release of the binding energy of matter falling into the strong gravitational field of the star or by the sudden nuclear burning of matter that has accumulated in the outermost layers of the star. The frequencies of the oscillations reflect the orbital frequencies of gas deep in the gravitational field of the star and/or the spin frequency of the star. These oscillations can therefore be used to explore fundamental physics, such as strong-field gravity and the properties of matter under extreme conditions, and important astrophysical questions, such as the formation and evolution of millisecond pulsars.
D1-74	Accreting Neutron Stars in Low-Mass X-Ray Binary Systems are of great interest in astrophysics due to the extreme conditions they present. In this research paper, we investigate the properties of these systems using observational data and theoretical models. We focus on the accretion process and its effects on the neutron star, as well as the emission properties of the X-ray radiation. Our results show that the accretion rate plays a crucial role in determining the behavior of these systems, with higher rates leading to more intense X-ray emission and potentially triggering thermonuclear explosions. We also find evidence for the existence of a boundary layer between the accretion disk and the neutron star, which can affect the observed X-ray spectra. Our study provides insight into the physics of accreting neutron stars and highlights the importance of continued observational and theoretical investigations in this field.
D1-75	We compute the high-frequency emission and absorption noise in a fractional quantum Hall effect (FQHE) sample at arbitrary temperature. We model the edges of the FQHE as chiral Luttinger liquids (LL) and we use the non-equilibrium perturbative Keldysh formalism. We find that the non-symmetrized high frequency noise contains important signatures of the electron-electron interactions that can be used to test the Luttinger liquid physics, not only in FQHE edge states, but possibly also in other one-dimensional systems such as carbon nanotubes. In particular we find that the emission and absorption components of the excess noise (defined as the difference between the noise at finite voltage and at zero voltage) are different in an interacting system, as opposed to the non-interacting case when they are identical.
D1-76	The fractional quantum Hall effect (FQHE) has been a topic of intense research in condensed matter physics due to its exotic properties and potential applications in quantum computing. In this paper, we investigate the emission and absorption noise in the FQHE system using numerical simulations and theoretical analysis. Specifically, we focus on the role of edge states and disorder in generating noise in the FQHE regime. Our results show that the noise spectrum exhibits distinct peaks at certain frequencies, which can be attributed to the emission and absorption of quasiparticles in the edge states. We also find that disorder can significantly enhance the noise level at low frequencies, while the edge states dominate the high-frequency noise. Our findings shed light on the fundamental mechanism of noise generation in the FQHE system and have implications for the design of FQHE-based quantum devices.
D1-77	The existence of magnetosonic solitons in dusty plasmas is investigated. The nonlinear magnetohydrodynamic equations for a warm dusty magnetoplasma are thus derived. A solution of the nonlinear equations is presented. It is shown that, due to the presence of dust, static structures are allowed. This is in sharp contrast to the formation of the so called shocklets in usual magnetoplasmas. A comparatively small number of dust particles can thus drastically alter the behavior of the nonlinear structures in magnetized plasmas.
D1-78	This research paper investigates the formation and propagation of magnetosonic solitons in a dusty plasma slab. Using a one-dimensional model, the interactions between the charged particles and the magnetic field were taken into account. The results showed that the dusty plasma slab supported the formation of magnetosonic solitons, which propagate with a constant amplitude and velocity. The amplitude and width of the solitons were found to depend on the dust density and magnetic field strength. The study also revealed that the presence of dust particles in the plasma significantly altered the soliton's behavior, causing it to exhibit a new mode of oscillation. These findings have important implications for understanding the dynamics of dusty plasmas and could be useful in developing new technologies for plasma-based devices.
D1-79	Very recently, it has been shown that thermal noise and its artificial versions (Johnson-like noises) can be utilized as an information carrier with peculiar properties therefore it may be proper to call this topic Thermal Noise Informatics. Zero Power (Stealth) Communication, Thermal Noise Driven Computing, and Totally Secure Classical Communication are relevant examples. In this paper, while we will briefly describe the first and the second subjects, we shall focus on the third subject, the secure classical communication via wire. This way of secure telecommunication utilizes the properties of Johnson(-like) noise and those of a simple Kirchhoff's loop. The communicator is unconditionally secure at the conceptual (circuit theoretical) level and this property is (so far) unique in communication systems based on classical physics.
D1-80	The paper "Thermal noise informatics" explores the potential of using thermal noise to achieve secure communication through a wire, zero-power communication, and thermal noise driven computing. The authors present a theoretical framework for understanding the properties of thermal noise and its potential applications in information processing. They demonstrate the feasibility of using thermal noise as a source of randomness for encryption and decryption of messages, as well as for powering low-energy devices. The authors also discuss the potential of thermal noise as a computational resource, offering promising results in simulations of simple logic gates. Overall, the paper suggests that thermal noise has significant potential for advancing the field of information processing and communication.
D1-81	An iterative method for recovering the bulk information in asymptotically AdS spacetimes is presented. We consider zero energy spacelike geodesics and their relation to the entanglement entropy in three dimensions to determine the metric in certain symmetric cases. A number of comparisons are made with an alternative extraction method presented in arXiv:hep-th/0609202, and the two methods are then combined to allow metric recovery in the most general type of static, spherically symmetric setups. We conclude by extracting the mass and density profiles for a toy model example of a gas of radiation in (2+1)-dimensional AdS.
D1-82	This research paper focuses on the numerical extraction of metrics in the Anti-de Sitter/Conformal Field Theory (AdS/CFT) correspondence. The AdS/CFT correspondence is a theoretical framework that relates two seemingly different physical theories - gravity in a spacetime called Anti-de Sitter space and a quantum field theory on the boundary of that spacetime. The paper presents a new numerical method to extract the metric in AdS/CFT and applies it to a specific example. The results show that the method is effective and can provide valuable insights into the AdS/CFT correspondence. The research concludes that this numerical approach can be applied to other scenarios in AdS/CFT and can be useful in understanding the interplay between gravity and quantum field theory.
D1-83	We report quantitative relations between corruption level and economic factors, such as country wealth and foreign investment per capita, which are characterized by a power law spanning multiple scales of wealth and investments per capita. These relations hold for diverse countries, and also remain stable over different time periods. We also observe a negative correlation between level of corruption and long-term economic growth. We find similar results for two independent indices of corruption, suggesting that the relation between corruption and wealth does not depend on the specific measure of corruption. The functional relations we report have implications when assessing the relative level of corruption for two countries with comparable wealth, and for quantifying the impact of corruption on economic growth and foreign investments.
D1-84	This research paper explores the quantitative relations between corruption and economic factors. By analyzing data from various countries, the study finds that corruption has a significant negative impact on economic growth, foreign direct investment, and government spending. Moreover, the research shows that corruption is correlated with income inequality and poverty. The study concludes that reducing corruption is crucial for promoting economic development and improving the standard of living for people in developing countries.
D1-85	In this article we calculate the electrical conductivity in QED using the 2PI effective action. We use a modified version of the usual 2PI effective action which is defined with respect to self-consistent solutions of the 2-point functions. We show that the green functions obtained from this modified effective action satisfy ward identities and that the conductivity obtained from the kubo relation is gauge invariant. We work to 3-loop order in the modified 2PI effective action and show explicitly that the resulting expression for the conductivity contains the square of the amplitude that corresponds to all binary collision and production processes.
D1-86	This research paper presents a study of the electrical conductivity of quantum electrodynamics (QED) using the 2PI effective action. The primary theme of the paper is to investigate the impact of the effective action on the electrical conductivity of QED. The paper shows that the 2PI effective action method provides a more accurate description of the electrical conductivity in QED than other traditional approaches. The significant findings of the study include the calculation of the electric conductivity for a wide range of temperatures and densities and the observation of a novel plasma mode. The research concludes that the 2PI effective action is a powerful tool for exploring the electrical conductivity of QED and can provide valuable insights into the physics of strongly interacting systems.
D1-87	Cosmic shear constrains cosmology by exploiting the apparent alignments of pairs of galaxies due to gravitational lensing by intervening mass clumps. However galaxies may become (intrinsically) aligned with each other, and with nearby mass clumps, during their formation. This effect needs to be disentangled from the cosmic shear signal to place constraints on cosmology. We use the linear intrinsic alignment model as a base and compare it to an alternative model and data. If intrinsic alignments are ignored then the dark energy equation of state is biased by ~50 per cent. We examine how the number of tomographic redshift bins affects uncertainties on cosmological parameters and find that when intrinsic alignments are included two or more times as many bins are required to obtain 80 per cent of the available information. We investigate how the degradation in the dark energy figure of merit depends on the photometric redshift scatter.
D1-88	This research paper presents a study on the constraints of dark energy from cosmic shear power spectra. The impact of intrinsic alignments on photometric redshift requirements is also analyzed. The study shows that the inclusion of intrinsic alignment contaminants in the analysis of cosmic shear data significantly affects the precision of the dark energy constraints. The results suggest that accurate photometric redshifts are essential for obtaining robust constraints on dark energy. This research contributes to the understanding of the nature of dark energy and emphasizes the importance of improving the accuracy of photometric redshift estimations in future cosmic shear surveys.
D1-89	A Brownian time process is a Markov process subordinated to the absolute value of an independent one-dimensional Brownian motion. Its transition densities solve an initial value problem involving the square of the generator of the original Markov process. An apparently unrelated class of processes, emerging as the scaling limits of continuous time random walks, involve subordination to the inverse or hitting time process of a classical stable subordinator. The resulting densities solve fractional Cauchy problems, an extension that involves fractional derivatives in time. In this paper, we will show a close and unexpected connection between these two classes of processes, and consequently, an equivalence between these two families of partial differential equations.
D1-90	The research paper titled "Brownian subordinators and fractional Cauchy problems" studies the behavior of Brownian subordinators, which are stochastic processes that describe the distribution of waiting times between two events. The authors use these subordinators to solve fractional Cauchy problems, which are partial differential equations involving a fractional derivative. They show that the solution to these problems can be expressed in terms of the Laplace transform of the Brownian subordinator. The authors also provide numerical simulations to illustrate the behavior of the solution in various scenarios. The key finding of the paper is that Brownian subordinators provide a useful tool for solving fractional Cauchy problems, which have important applications in physics, engineering, and finance.
D1-91	Binary microlensing light curves have a variety of morphologies. Many are indistinguishable from point lens light curves. Of those that deviate from the point lens form, caustic crossing light curves have tended to dominate identified binary lens events. Other distinctive signatures of binary lenses include significant asymmetry, multiple peaks, and repeating events. We have quantified, using high resolution simulations, the theoretically expected relative numbers of each type of binary lens event, based on its measurable characteristics. We find that a microlensing survey with current levels of photometric uncertainty and sampling should find at least as many non-caustic crossing binary lens events as caustic crossing events; in future surveys with more sensitive photometry, the contribution of distinctive non-caustic crossing events will be even greater.
D1-92	This research paper aims to explore the properties of binary microlensing light curves beyond the conventional caustic crossings. The study will investigate the various features of light curves that result from binary microlensing, including the effects of the source trajectory, the source size, and the binary geometry. The research will utilize a combination of numerical simulations and statistical analyses to identify patterns in the light curves and explore the underlying physical processes. By examining the properties of binary microlensing light curves beyond the caustic crossings, this study will contribute to a deeper understanding of the dynamics of binary systems and their potential applications in astrophysics.
D1-93	The radiation hardness of silicon charged particle sensors is compared with single crystal and polycrystalline diamond sensors, both experimentally and theoretically. It is shown that for Si- and C-sensors, the NIEL hypothesis, which states that the signal loss is proportional to the Non-Ionizing Energy Loss, is a good approximation to the present data. At incident proton and neutron energies well above 0.1 GeV the radiation damage is dominated by the inelastic cross section, while at non-relativistic energies the elastic cross section prevails. The smaller inelastic nucleon-Carbon cross section and the light nuclear fragments imply that at high energies diamond is an order of magnitude more radiation hard than silicon, while at energies below 0.1 GeV the difference becomes significantly smaller.
D1-94	This research paper compares the radiation hardness of diamond and silicon sensors. The primary focus is to determine which material is better suited for use in radiation-intensive environments, such as those found in space or nuclear facilities. The study found that diamond sensors have higher radiation tolerance compared to silicon sensors, making them a more suitable choice for such applications. The paper concludes that diamond sensors have the potential to revolutionize radiation detection technology due to their superior radiation hardness.
D1-95	The topological insulator is an electronic phase stabilized by spin-orbit coupling that supports propagating edge states and is not adiabatically connected to the ordinary insulator. In several ways it is a spin-orbit-induced analogue in time-reversal-invariant systems of the integer quantum Hall effect (IQHE). This paper studies the topological insulator phase in disordered two-dimensional systems, using a model graphene Hamiltonian introduced by Kane and Mele as an example. The nonperturbative definition of a topological insulator given here is distinct from previous efforts in that it involves boundary phase twists that couple only to charge, does not refer to edge states, and can be measured by pumping cycles of ordinary charge.
D1-96	The research paper explores the concept of topological insulators (TI) beyond the Brillouin zone through the use of Chern parity. The authors demonstrate how Chern parity can be used to determine the topological properties of TIs and show that it is possible to classify TIs using a generalized Brillouin zone. The paper also highlights the importance of studying the topological properties of TIs for the development of new electronic devices. Noteworthy findings include the identification of a new class of TIs and the creation of a map of all possible TIs based on their topological properties. Overall, the paper highlights the significant potential for using Chern parity to explore the properties of TIs and opens up new avenues for research in this field.
D1-97	We report an experimental realization of one-way quantum computing on a two-photon four-qubit cluster state. This is accomplished by developing a two-photon cluster state source entangled both in polarization and spatial modes. With this special source, we implemented a highly efficient Grover's search algorithm and high-fidelity two qubits quantum gates. Our experiment demonstrates that such cluster states could serve as an ideal source and a building block for rapid and precise optical quantum computation.
D1-98	This research paper presents experimental realization of one-way quantum computing with two-photon four-qubit cluster states. The study demonstrates the creation and manipulation of cluster states, as well as the implementation of various quantum gates. The results suggest that the four-qubit cluster state is a promising resource for one-way quantum computing. This study lays the foundation for future research in the field of quantum computing and provides a step towards practical applications of quantum information processing.
D1-99	At the air/water interface, 4,-8-alkyl[1,1,-biphenyl]-4-carbonitrile (8CB) domains with different thicknesses coexist in the same Langmuir film, as multiple bilayers on a monolayer. The edge dislocation at the domain boundary leads to line tension, which determines the domain shape and dynamics. By observing the domain relaxation process starting from small distortions, we find that the line tension is linearly dependent on the thickness difference between the coexisting phases in the film. Comparisons with theoretical treatments in the literature suggest that the edge dislocation at the boundary locates near the center of the film, which means that the 8CB multilayers are almost symmetric with respect to the air/water interface.
D1-100	The research paper investigates the line tension and structure of smectic liquid crystal multilayers at the air-water interface. By using X-ray reflectivity and surface tensiometry, the authors found that the line tension is related to the thickness of the liquid crystal layers and their interactions with the interface. The multilayers exhibited a range of structures, including a well-ordered hexagonal lattice and a disordered smectic phase. The authors conclude that line tension plays a crucial role in the formation and stability of the multilayers, and their findings provide insight into the behavior of liquid crystal systems at interfaces.
D2-100	It is an approximation useful in chemistry, but not strictly correct. It is not, in general, obeyed by nuclear reactions, or even in one common medical procedure, PET scanning (positron emission tomography). In this procedure, radioactive variants of common drug or body chemicals are injected into a patient under observation. Some of a particular radioactive chemical element or elements are radioactive in a particular way—they are neutron-light. Their nuclear reactions in the body are similar to beta-decay of neutron-heavy elements, except that the “electrons” released are not electrons, but the corresponding anti-matter: positrons. As anti-matter, positrons are quite unwelcome in ordinary matter—the patient—and very quickly encounter electrons, annihilating each electron-positron pair.
D2-11	These nutrients are needed to keep bones, teeth and muscles healthy. A lack of vitamin D can lead to bone deformities such as rickets in children, and bone pain caused by a condition called osteomalacia in adults. Government advice is that everyone should consider taking a daily vitamin D supplement during the autumn and winter. People at high risk of not getting enough vitamin D, all children aged 1 to 4, and all babies (unless they're having more than 500ml of infant formula a day) should take a daily supplement throughout the year. There have been some reports about vitamin D reducing the risk of coronavirus (COVID-19).
D2-12	Vitamin D is essential for maintaining healthy bones, teeth, and immune function, primarily by regulating calcium and phosphorus levels in the body. Humans can synthesize vitamin D through skin exposure to sunlight (UVB rays), but factors like limited sun exposure, darker skin, age, and geographic location can reduce production. The recommended daily intake varies by age and health status. For most adults, 600-800 IU (15-20 µg) per day is sufficient, though some individuals may require higher doses, especially if deficient. Dietary sources include fatty fish, fortified foods, and supplements. Deficiency can lead to rickets in children, osteomalacia in adults, and weakened immunity, making adequate vitamin D intake crucial for overall health and well-being. Regular checks can ensure optimal levels.
D2-13	Within 50 million years, the pressure and density of hydrogen in the center of the protostar became great enough for it to begin thermonuclear fusion.As helium accumulates at its core, the Sun is growing brighter; early in its main-sequence life its brightness was 70% that of what it is today.The temperature, reaction rate, pressure, and density increased until hydrostatic equilibrium was achieved: the thermal pressure counterbalancing the force of gravity. At this point, the Sun became a main-sequence star. Solar wind from the Sun created the heliosphere and swept away the remaining gas and dust from the protoplanetary disc into interstellar space.
D2-15	There are an estimated 2 trillion galaxies in the known Universe, some will hold millions and others will hold billions of stars. Most of the stars will have various amounts of planets in orbit around them and trillions of those planets will be in the Goldilocks Zone, which is an area in space that has the right conditions for liquid water to exist. So the logical thing you might say is “There must be life on some of them”. Maybe some other intelligent life form on a faraway world is asking that same question. If we are alone that would make us more special and unique than we could ever imagine.
D2-16	Type 2 diabetes rates have risen dramatically worldwide since the 1980s, constituting a global health crisis. The prevalence has more than doubled over the past three decades, increasing from approximately 108 million adults in 1980 to over 537 million (10.5% of global adult population) by 2021. This surge has been particularly pronounced in middle-income countries experiencing rapid urbanization, with Pacific Islands and Middle Eastern nations showing the highest prevalence rates, while China and India have the largest absolute numbers. Key drivers include rising obesity rates, increasingly sedentary lifestyles, Westernization of diets, aging populations, and improved detection methods. While some high-income countries have shown signs of stabilization recently, global projections remain concerning, with cases expected to reach 783 million by 2045.
D2-17	Quarks and gluons are both fundamental particles in the Standard Model of particle physics, but they have distinct roles. Quarks are the building blocks of matter, combining to form protons, neutrons, and other hadrons. They come in six flavors: up, down, charm, strange, top, and bottom. Quarks have fractional electric charges and interact via the strong nuclear force, which holds them together inside protons and neutrons. Gluons, on the other hand, are force carrier particles responsible for transmitting the strong nuclear force between quarks. Unlike quarks, gluons have no mass and no electric charge, but they do carry a property called color charge, which allows them to bind quarks together through a phenomenon known as color confinement.
D2-18	Insects as food or edible insects are insect species used for human consumption. Over 2 billion people are estimated to eat insects on a daily basis. Globally, more than 2,000 insect species are considered edible, though far fewer are discussed for industrialized mass production and regionally authorized for use in food. Many insects are highly nutritious, though nutritional content depends on species and other factors such as diet and age. Insects offer a wide variety of flavors and are commonly consumed whole or pulverized for use in dishes and processed food products such as burger patties, pasta, or snacks. Like other foods, there can be risks associated with consuming insects, such as allergic reactions.
D2-2	Spermidine is an aliphatic polyamine. Spermidine synthase (SPDS) catalyzes its formation from putrescine. It is a precursor to other polyamines, such as spermine and its structural isomer thermospermine. Spermidine synchronizes an array of biological processes, (such as Ca2+, Na+, K+ -ATPase) thus maintaining membrane potential and controlling intracellular pH and volume. Spermidine regulates biological processes, such as Ca2+ influx by glutamatergic N-methyl-D-aspartate receptor (NMDA receptor), which has been associated with nitric oxide synthase (NOS) and cGMP/PKG pathway activation and a decrease of Na+,K+-ATPase activity in cerebral cortex synaptosomes. Spermidine is a longevity agent in mammals due to various mechanisms of action, which are just beginning to be understood.
D2-21	The probability of extraterrestrial life existing remains one of science's great unknowns, with estimates varying widely based on different assumptions and models. While we lack direct evidence of life beyond Earth, several scientific factors inform our understanding: The Drake Equation, formulated in 1961, attempts to estimate the number of active, communicative extraterrestrial civilizations in our galaxy by multiplying several probability factors. However, many of these factors remain highly uncertain. Many astrobiologists consider microbial life likely to exist elsewhere, with estimates ranging from cautious to highly optimistic. The probability of intelligent civilizations remains more speculative, with the Fermi Paradox (questioning why we haven't detected them) suggesting either rarity or communication difficulties.
D2-22	Insects can be used for food production and are already consumed in many cultures worldwide. They are a nutritious, sustainable, and environmentally friendly protein source. Edible insects such as crickets, mealworms, grasshoppers, and black soldier fly larvae are rich in protein, healthy fats, vitamins, and minerals like iron and zinc. Compared to traditional livestock, insect farming requires less land, water, and feed, while producing lower greenhouse gas emissions. Insects can be used in whole form, ground into flours for protein bars, pasta, or baked goods, or processed into animal feed. While cultural barriers exist in some regions, the growing interest in sustainable food sources is driving research and commercial production of edible insects globally.
D2-23	Diabetes type 1 and type 2 are both chronic conditions affecting blood sugar regulation but have different causes and mechanisms. Type 1 diabetes is an autoimmune disease where the immune system attacks insulin-producing cells in the pancreas, leading to little or no insulin production. It usually develops in childhood or adolescence and requires lifelong insulin therapy. Type 2 diabetes is primarily caused by insulin resistance, where the body still produces insulin but cannot use it effectively. It is more common in adults and is strongly linked to obesity, diet, and lifestyle factors. Type 2 can often be managed with diet, exercise, and medication, while type 1 requires insulin replacement. Both types require careful blood sugar management to prevent complications.
D2-24	Entomophagy—the practice of consuming insects as food—has deep historical roots in many cultures worldwide, with approximately 2 billion people regularly incorporating insects into their diets. While Western societies have traditionally viewed insects with disgust, growing environmental and food security concerns have sparked renewed interest in this protein source. Nutritionally, edible insects offer impressive benefits. Most species provide complete proteins containing all essential amino acids, often at higher concentrations than conventional meats. Many insects are rich in micronutrients like iron, zinc, and B vitamins, while containing healthy fats including omega-3 fatty acids. Cricket flour, for example, contains approximately 60-70% protein by weight, exceeding most plant and animal sources.
D2-25	"Quarks and gluons are fundamental particles in the Standard Model of particle physics, both essential to our understanding of strong nuclear forces, yet they differ in several key ways. Fundamental Nature Quarks are elementary fermions that serve as the building blocks of hadrons (protons, neutrons, and other composite particles). Six ""flavors"" exist: up, down, charm, strange, top, and bottom. In contrast, gluons are gauge bosons that mediate the strong force between quarks, similar to how photons mediate the electromagnetic force. Properties Quarks possess fractional electric charges (+2/3 or -1/3), mass, spin-1/2, and color charge. Gluons are massless, have spin-1, carry no electric charge, but uniquely possess both color and anticolor charges."
D2-26	The global increase in demand for meat and the limited land area available prompt the search for alternative protein sources. Also the sustainability of meat production has been questioned. Edible insects as an alternative protein source for human food and animal feed are interesting in terms of low greenhouse gas emissions, high feed conversion efficiency, low land use, and their ability to transform low value organic side streams into high value protein products. More than 2000 insect species are eaten mainly in tropical regions. The role of edible insects in the livelihoods and nutrition of people in tropical countries is discussed, but this food source is threatened
D2-27	In 1802, Lamarck published Hydrogéologie, and became one of the first to use the term biology in its modern sense. In Hydrogéologie, Lamarck advocated a steady-state geology based on a strict uniformitarianism. He argued that global currents tended to flow from east to west, and continents eroded on their eastern borders, with the material carried across to be deposited on the western borders. Thus, the Earth's continents marched steadily westward around the globe. That year, he also published Recherches sur l'Organisation des Corps Vivants, in which he drew out his theory on evolution. He believed that all life was organized in a vertical chain, with gradation between the lowest forms and the highest forms of life.
D2-3	The feasibility of extraterrestrial life is a subject of intense scientific interest, supported by discoveries in astrobiology, planetary science, and extremophile research. While no direct evidence has been found, several factors suggest its plausibility. Thousands of exoplanets have been discovered, some located in the habitable zone where liquid water could exist, a key ingredient for life as we know it. The discovery of extremophiles on Earth, thriving in extreme conditions such as deep-sea vents and frozen tundras, suggests that life could also adapt to harsh extraterrestrial environments like those on Mars, Europa, or Enceladus. Additionally, complex organic molecules, the building blocks of life, have been detected in meteorites, comets, and interstellar clouds, indicating that life’s fundamental chemistry is widespread.
D2-30	More than 1,900 species of edible insects are consumed worldwide. However, this number continues to increase as more studies are carried out on this issue. Most of these known species are collected directly from the natural environment, but available data on the amounts of insects that are ingested worldwide are scarce. In any case, there is agreement that of the hundreds of insect species used as human food worldwide, the most common can be encompassed into four groups: beetles; ants, bees and wasps; grasshoppers and crickets and, finally, moths, caterpillars, and butterflies. According to FAO, the most consumed insects are beetles (Coleoptera, 31%), caterpillars (Lepidoptera, 18%) and bees, wasps and ants (Hymenoptera, 14%).
D2-31	Vitamin D plays a crucial role in human health, primarily regulating calcium and phosphorus absorption for bone formation and maintenance. The body produces vitamin D when skin is exposed to sunlight, but it can also be obtained through diet (fatty fish, egg yolks) and supplements. The recommended daily intake is 600-800 IU for most adults, though needs increase with age and limited sun exposure. Deficiency can lead to bone disorders like rickets in children and osteomalacia in adults, muscle weakness, and potentially increased risk of certain chronic diseases. Factors affecting vitamin D status include geographical location, season, skin pigmentation, age, and dietary habits. Regular moderate sun exposure combined with dietary sources provides sufficient vitamin D for most people.
D2-37	Obesity and type 2 diabetes have a close relationship. The likelihood and severity of type 2 diabetes are closely related to BMI. Obese people have a higher risk of diabetes than healthy weight people, with a threefold increase in risk for overweight people. While it is well known that body fat distribution is an important primary indicator of diabetes risk, the precise mechanism of association is unknown. It is also unknown why not all obese people develop type 2 diabetes and why not all type 2 diabetics are obese. Also obesity significantly increases your chances of developing type 2 diabetes at some point in your life. When there is an excess of fat, the tissues must process more nutrients.
D2-39	Rates of type 2 diabetes have increased markedly since 1960 in parallel with obesity. As of 2015, there were approximately 392 million people diagnosed with the disease compared to around 30 million in 1985. Typically, it begins in middle or older age, although rates of type 2 diabetes are increasing in young people. Type 2 diabetes is associated with a ten-year-shorter life expectancy. Diabetes was one of the first diseases ever described, dating back to an Egyptian manuscript from c. 1500 BCE. Type 1 and type 2 diabetes were identified as separate conditions in 400–500 CE with type 1 associated with youth and type 2 with being overweight. The importance of insulin in the disease was determined in the 1920s
D2-4	Many cross sectional and prospective studies have confirmed the association between obesity and type 2 diabetes. Most people with type 2 diabetes are overweight or obese: more than 85% of people with type 2 diabetes in southeast Scotland in 2005 had a body mass index (weight in kilograms divided by height in metres squared) of over 25. Recent evidence indicates that high waist circumference may be an even better indicator than body mass index (BMI) of increased risk of type 2 diabetes. The risk of developing diabetes over a 14 year follow-up period in the nurses' health study was 49 times higher among women whose baseline BMI was >35 than among women whose baseline BMI was <22.
D2-41	Quarks are fundamental particles that make up protons and neutrons, the building blocks of atomic nuclei. They are part of the Standard Model of particle physics and are classified as elementary particles, meaning they have no known substructure. Quarks come in six types, or flavors: up, down, charm, strange, top, and bottom. They always exist in groups, held together by the strong nuclear force, which is mediated by particles called gluons. The most common quarks in nature are up and down, which combine to form protons and neutrons. Quarks have fractional electric charges and cannot exist independently outside of bound states due to a phenomenon called color confinement.
D2-42	In order to determine how much Vitamin D supplement to take, you first need to find out what your current Vitamin D level is. You determine the level of vitamin D by doing a blood test for 25 (OH) vitamin D. That’s 25 hydroxy vitamin D. Most experts agree that a level of 32 nanograms per milliliter (ng/ml) in the blood is optimal for bone health but no one agrees on what is optimal for immune health, heart health, cancer health and all the other important functions that vitamin D appears to have. Your skin, where vitamin D is made, has a wonderful system for making the right amount, then shutting off further production.
D2-46	"Obesity is recognized as a major risk factor for type 2 diabetes, though it does not directly cause the disease. The connection between these conditions is so significant that healthcare professionals have coined the term ""diabesity"" to highlight that the majority of individuals with diabetes are obese or overweight demonstrates that the risk of developing type 2 diabetes increases linearly with rising body mass index. This explains why the worldwide increase in obesity prevalence has led to a corresponding surge in type 2 diabetes cases globally. The relationship works through several mechanisms: excess body fat, particularly visceral fat, affects insulin sensitivity and function. Some individuals with obesity can produce more insulin without overtaxing the pancreasy."
D2-49	"According to Poe, the initial state of matter was a single ""Primorial Particle"". ""Divine Volition"", manifesting itself as a repulsive force, fragmented the Primordial Particle into atoms. Atoms spread evenly throughout space, until the repulsive force stops, and attraction appears as a reaction: then matter begins to clump together forming stars and star systems, while the material universe is drawn back together by gravity, finally collapsing and ending eventually returning to the Primordial Particle stage in order to begin the process of repulsion and attraction once again. This part of Eureka describes a Newtonian evolving universe which shares a number of properties with relativistic models, and for this reason Poe anticipates some themes of modern cosmology."
D2-5	There were observations of spectral lines. That means there are distinct levels where electrons stay. Classical mechanics does not allow such distinctive levels for electrons to stay. For this new approach is needed. In classical mechanics, energy remains constant. This must be building block. It is my interpretation. Then. he thought that the electrons obey some function distribution which give distinctive levels. This is possible for sin and cos functions. So the electrons should obey sin and cos functions and also obey total energy conservation i.e to start with kinetic energy + potential energy = constant. Potential energy and sin function is not problem, because we know pendulum oscillations. How to modify kinetic energy to give sine function.
D2-52	Mainstream physicists treat “quarks” and “gluons” as real particles. However, the observational evidence disagrees with that conclusion. Whereas electron and protons exist for billions of years, “quarks” are only observed for less than billionths of a second. Gluons are not observed at all, but are only theorized. When “quarks” are “observed,” it is during a collision among protons and/or neutrons in an accelerator. After the collision, the “quarks” completely disappear and are never observed again. There has never been an observation of a “sea of quarks” among from which could be used to construct new protons or neutrons. “Quarks” are nothing more than debris from a collision; much like glass shards are debris from a glass striking a brick wall.
D2-53	"Lavoisier's Law, formally known as the Law of Conservation of Mass, is one of the fundamental principles in chemistry established by French chemist Antoine Lavoisier in the late 18th century . The law states that In a closed system, matter is neither created nor destroyed during chemical reactions or physical transformations . In simpler terms, the total mass of substances present before a chemical reaction equals the total mass after the reaction, even though the substances may change form. Lavoisier demonstrated this principle through meticulous experiments where he carefully measured the mass of reactants and products. Antoine Lavoisier (1743-1794), often called the ""father of modern chemistry,"" formulated this principle around 1774."
D2-54	Using the Schwinger-Dyson equations the behavior of quark and gluon propagators is studied in the Landau gauge for momenta from the deep Euclidean to the confinement regime. We find that while at short distances quarks and gluons propagate like free particles, over longer distances, of the order of a fermi, the gluon propagator is greatly enhanced as are the triple-gluon and quark-gluon couplings. These in turn suppress the propagation of massless quarks over long distances to such an extent that they have no physical particle pole, exactly as expected of a confining theory. We study the way the world changes as the number of massless flavors of quark is increased from zero.
D2-55	In physics and chemistry, the law of conservation of mass or principle of mass conservation states that for any system closed to all transfers of matter the mass of the system must remain constant over time. The law implies that mass can neither be created nor destroyed, although it may be rearranged in space, or the entities associated with it may be changed in form. For example, in chemical reactions, the mass of the chemical components before the reaction is equal to the mass of the components after the reaction. Thus, during any chemical reaction and low-energy thermodynamic processes in an isolated system, the total mass of the reactants, or starting materials, must be equal to the mass of the products.
D2-6	The active quest for extra-solar planets has opened a new chapter in the book of the search for extra-terrestrial life. This was already an active field of science with the exploration of the Solar System by means of space probes, which gave rise to a ‘space age’ from 1957 when Sputnik-1, the first Earth-orbiting artificial satellite, was launched. Current technology allows us to land a robotic chemistry laboratory on other Solar System bodies, or return samples to Earth, the latter coming with the advantage of being able to adapt analysis strategies to unexpected findings. Based on our current understanding, Mars, Europa, Enceladus and, if we consider life based on a liquid other than water, Titan are the most promising places.
D2-61	Aminopropyltransferases transfer aminopropyl groups from decarboxylated S-adenosylmethionine to amine acceptors, forming polyamines. Structural and biochemical studies have been carried out with the human spermidine synthase, which is highly specific for putrescine as the amine acceptor, and the Thermotoga maritima spermidine synthase, which prefers putrescine but is more tolerant of other substrates. Comparison of the structures of the human spermidine synthase with both substrates and products with the known structure of T. maritima spermidine synthase complexed to a multisubstrate analogue inhibitor and analysis of the properties of site-directed mutants provide a general mechanistic hypothesis for the aminopropyl transfer reaction. The studies also provide a structural basis for the specificity of the spermidine synthase subclass of the aminopropyltransferase family.
D2-63	The use of insects for food represents a promising yet complex solution to address the growing need for sustainable food sources. Insects offer several advantages over traditional livestock, including lower resource requirements, such as land, water, and feed, and reduced environmental impact, emitting fewer greenhouse gases. They are rich in protein and essential nutrients, making them a healthy and potentially cost-effective alternative to conventional meat products. However, cultural perceptions and regulatory frameworks present significant barriers. Many Western societies are not accustomed to consuming insects, which may hinder widespread acceptance. Ensuring the safety and regulatory compliance of insect-based foods is also crucial. Additionally, developing efficient production and processing methods to scale up insect farming is essential for feasibility.
D2-64	Spermidine synthase (SpdS) is an enzyme responsible for producing spermidine, a polyamine essential for cell growth and proliferation. It typically exists as a homodimer or monomer, depending on the species, and has a Rossmann-like fold common in nucleotide-binding proteins. The enzyme’s active site binds putrescine and decarboxylated S-adenosylmethionine (dcSAM), with key aspartate and glutamate residues facilitating catalysis. During the reaction, SpdS transfers an aminopropyl group from dcSAM to putrescine, forming spermidine and 5′-methylthioadenosine (MTA). This process plays a vital role in cellular homeostasis, stress response, and DNA stabilization. The enzyme ensures efficient polyamine biosynthesis, which is crucial for cellular function and growth.
D2-67	The Lavoisier Law, also known as the Law of Conservation of Mass, was formulated by French chemist Antoine Lavoisier in 1789. This law states that in a closed system, the total mass of the reactants is equal to the total mass of the products during a chemical reaction. In simpler terms, matter cannot be created or destroyed in a chemical reaction; it only transforms from one form to another. This fundamental principle revolutionized chemistry, as it emphasized the importance of balancing chemical equations and understanding the fate of matter in reactions. The law laid the groundwork for modern chemical theory and remains a cornerstone of scientific understanding.
D2-69	Lavoisier was born in 1743 into a wealthy family of lawyers, and initially prepared for a legal career, being awarded a baccalaureate in law in 1763. Antoine Lavoisier played the central role in what has come to be known as the chemical revolution and he was active also in agricultural and fiscal reform as well as technological development. He is credited with establishing that oxygen is an element and water its compound with hydrogen, refining experimental methods in chemistry, reforming chemical nomenclature along systematic lines, defining element operationally, and denying phlogiston a place in chemical explanation. This chapter critically examines the analytical conception of the elements that Lavoisier proposes.
D2-7	Spermidine synthases are essential enzymes that function as aminopropyltransferases, transferring aminopropyl groups from decarboxylated S-adenosylmethionine to amine acceptors, forming polyamines. Structurally, these enzymes contain a SAM-MT fold and typically form homodimers. The mechanism follows an SN2 reaction pathway where spermidine synthase (SPDS) catalyzes the formation of spermidine from putrescine The human spermidine synthase is highly specific for putrescine as the amine acceptor. Crystal structures have been solved at high resolution, including the putrescine aminopropyltransferase from Thermotoga maritima at 1.5 Å . These structural studies, combined with biochemical analyses of enzymes from various organisms, have provided detailed insights into the catalytic mechanism and substrate specificity of this important class of enzymes.
D2-70	Lavoisier’s law, also known as the Law of Conservation of Mass, states that mass is neither created nor destroyed in a chemical reaction. This fundamental principle, proposed by Antoine Lavoisier in the late 18th century, is crucial to the field of chemistry and science in general. It laid the foundation for modern chemistry by establishing that the total mass of reactants in a chemical reaction is equal to the total mass of products. This law enabled scientists to develop more accurate chemical equations and understand that chemical reactions involve the rearrangement of atoms, not their creation or annihilation. Lavoisier’s law was vital in refuting earlier theories like the phlogiston theory and is key to stoichiometry.
D2-73	"Lamarck's ""Hydrogéologie"" (1802) represents a fascinating but often overlooked contribution to early geological thought. While Lamarck is primarily remembered for his evolutionary biology theories, this work showcases his broader scientific vision. In it, he proposed that Earth's features formed through gradual processes over immense time periods—challenging the catastrophist views popular in his era. The text emphasized water's crucial role in shaping landscapes and theorized about atmospheric influences on geological formations. Notably, Lamarck's uniformitarian approach in geology paralleled his biological thinking about gradual change. Though overshadowed by his biological works and contemporaries like Cuvier, ""Hydrogéologie"" demonstrates Lamarck's cross-disciplinary thinking and his willingness to challenge scientific orthodoxy across multiple fields."
D2-77	The possibility of extraterrestrial life is rooted in the abundance of potentially habitable environments and the presence of basic components necessary for life. In our solar system, planets like Mars and moons such as Europa and Enceladus show promise due to evidence of water, a key ingredient for life. Beyond our solar system, exoplanets in habitable zones, identified through missions like Kepler, suggest countless opportunities for life. However, while conditions may be favorable, the origin of life remains elusive, requiring the right environment, building blocks, and catalytic events. Detection of life relies heavily on advanced telescopes and space probes, with confirmation posing significant challenges. Regardless of current limitations, the search for extraterrestrial life continues.
D2-78	Since the mid-20th century, active research has taken place to look for signs of extraterrestrial life, encompassing searches for current and historic extraterrestrial life, and a narrower search for extraterrestrial intelligent life. Depending on the category of search, methods range from analysis of telescope and specimen data to radios used to detect and transmit communications. The concept of extraterrestrial life, particularly extraterrestrial intelligence, has had a major cultural impact, especially extraterrestrials in fiction. Science fiction has communicated scientific ideas, imagined a range of possibilities, and influenced public interest in and perspectives on extraterrestrial life. One shared space is the debate over the wisdom of attempting communication with extraterrestrial intelligence. Some encourage aggressive methods to try to contact intelligent extraterrestrial life.
D2-80	A PET (Positron Emission Tomography) scan is a sophisticated medical imaging technique that reveals how tissues and organs function at the molecular level. Unlike CT or MRI scans that show anatomical structure, PET scans detect metabolic activity by using a radioactive tracer—typically a form of glucose—injected into the bloodstream. This tracer emits positrons that create signals detected by the scanner. Areas with higher metabolic activity, such as rapidly growing cancer cells, absorb more tracer and appear brighter on the resulting 3D images. PET scans are particularly valuable for diagnosing and monitoring cancer, neurological disorders like Alzheimer's, and cardiovascular conditions. The procedure is painless, typically takes 30 minutes to 2 hours, and exposes patients to minimal radiation.
D2-82	A PET scan (Positron Emission Tomography) is a medical imaging technique that allows doctors to observe the metabolic activity of tissues and organs in the body. It involves injecting a small amount of a radioactive tracer (a substance containing a radioactive atom) into the bloodstream. This tracer is typically a form of glucose that is metabolized by cells. The PET scanner detects the radiation emitted by the tracer, creating detailed images that show how tissues are functioning, rather than just their structure. PET scans are commonly used in the diagnosis and monitoring of conditions like cancer, heart disease, and neurological disorders (such as Alzheimer’s disease). They are especially useful in identifying abnormal cell activity, which can help detect cancerous tumors.
D2-85	A PET scan, or Positron Emission Tomography, is a sophisticated imaging technique used in healthcare to visualize the metabolic processes within the body. It involves the injection of a radioactive tracer, often a glucose analog, into the patient. As the tracer interacts with body tissues, it emits positrons, which collide with electrons, resulting in the release of gamma rays. These emissions are detected by the PET scanner, which constructs a detailed image showing the distribution and activity of the tracer within the body. Unlike structural imaging methods such as CT scans or MRIs, a PET scan provides functional information, highlighting areas of high metabolic activity. This makes it particularly valuable in diagnosing and assessing conditions like cancer, heart disease,  neurological disorders.
D2-88	Jean-Baptiste Lamarck’s Hydrogéologie (1802) proposed a holistic, steady-state geological model emphasizing uniformitarianism—the idea that gradual processes shape Earth’s surface over immense time. He theorized that continents erode eastward, with debris deposited westward via global currents, causing a slow continental migration. Lamarck integrated living organisms into Earth’s dynamics, arguing their metabolic activity drives the decomposition and recycling of materials, sustaining planetary equilibrium. This vision positioned biology as central to geology, pioneering ecological thinking by linking abiotic and biotic processes. However, his speculative approach—rooted in outdated four-element chemistry—drew criticism from contemporaries like Georges Cuvier and Antoine Lavoisier. Despite its flaws, Hydrogéologie laid groundwork for synthesizing Earth sciences, envisioning a unified “Physique terrestre” (terrestrial physics) combining meteorology, hydrogeology, and biology
D2-9	There are two types of diabetes: type 1, where your body is unable to make insulin; and type 2, where your body is unable to properly use the insulin it makes. When your body doesn't have enough insulin, you can't move glucose from your blood to the cells, and then you don't get the nutrients your body needs for energy. Type 2 diabetes is also often genetic, as in my case. The fact that there's no break from the disease. I have diabetes 24/7, 365 days a year. Managing what I eat is a constant challenge, especially when there are potato chips nearby. And honestly, I hate cooking and I'm always looking for easy recipes.
D2-90	The principle of positron emission tomography (PET) is that radiation emitted from a radiopharmaceutical injected intravenously into a patient is registered by external detectors positioned at different orientations. This is a technology that is extremely useful in the detection and treatment of diseases like cancer. However, there is some confusion about how PET works. Before PET was introduced a different type of radiation imaging called SPECT (single-photon emission computerised tomography) was used. This technology was based on the pinhole camera technique whereby a reverse image of an object is obtained by passing the light through a small aperture or pinhole.
D2-92	With the emergence of positron emission tomography (PET) from research laboratories into routine clinical use, it is important to redefine the most appropriate use of each imaging technique. The aim of this review article is to show the potential of PET in oncology. We discuss the most promising indications and the perspectives for the future. We will also point out the shortcomings and the important questions to be answered before fully considering PET as a necessary tool in the day-to-day practice of oncology. Although many studies have documented the high accuracy of 18F-FDG PET for the detection and staging of malignant tumours and for the monitoring of therapy results in these patients.
D2-95	Positron emission tomography (PET) is a functional imaging technique that uses radioactive substances known as radiotracers to visualize and measure changes in metabolic processes, and in other physiological activities including blood flow, regional chemical composition, and absorption. Different tracers are used for various imaging purposes, depending on the target process within the body. PET is a common imaging technique, a medical scintillography technique used in nuclear medicine. A radiopharmaceutical – a radioisotope attached to a drug – is injected into the body as a tracer. When the radiopharmaceutical undergoes beta plus decay, a positron is emitted, and when the positron interacts with an ordinary electron, the two particles annihilate and two gamma rays are emitted in opposite directions.
D2-1	The Solar System faces a dramatic future over billions of years. In approximately 5-7 billion years, our Sun will exhaust its hydrogen fuel and expand into a red giant, likely engulfing Mercury and Venus. Earth may either be consumed or rendered uninhabitable as oceans and atmosphere evaporate. The outer planets will survive but experience significant changes in temperature and orbital dynamics. Eventually, the Sun will shed its outer layers, creating a planetary nebula while its core collapses into a white dwarf star. This white dwarf will slowly cool over trillions of years, leaving our former solar neighborhood incredibly dim. Any surviving planets will orbit in perpetual darkness, frozen and lifeless.
D2-10	"The origin of angiosperms (flowering plants) represents what Charles Darwin famously called an ""abominable mystery"" due to their apparent sudden appearance and rapid diversification in the fossil record. Modern research combines multiple lines of evidence to address this evolutionary puzzle. The fossil record shows unequivocal angiosperm evidence beginning in the Early Cretaceous (approximately 125-130 million years ago), with remarkably diverse forms appearing within 20-30 million years. Key early fossils include Archaefructus and Montsechia, with water lily relatives among the earliest documented flowering plants. Molecular evidence, however, suggests an earlier origin than fossils indicate. DNA-based phylogenies identify Amborella, water lilies, and Austrobaileyales as the earliest-diverging lineages. Molecular clock analyses often place angiosperm origins in the Jurassic period (145-201 million years ago)."
D2-14	Molecular data on relationships within angiosperms confirm the view that their increasing morphological diversity through the Cretaceous reflected their evolutionary radiation. Despite the early appearance of aquatics and groups with simple flowers, the record is consistent with inferences from molecular trees that the first angiosperms were woody plants with pinnately veined leaves, multiparted flowers, uniovulate ascidiate carpels, and columellar monosulcate pollen. Molecular data appear to refute the hypothesis based on morphology that angiosperms and Gnetales are closest living relatives. Morphological analyses of living and fossil seed plants that assume molecular relationships identify glossopterids, Bennettitales, and Caytonia as angiosperm relatives; these results are consistent with proposed homologies between the cupule of glossopterids and Caytonia and the angiosperm bitegmic ovule.
D2-19	Schrödinger's cat is a famous thought experiment proposed by physicist Erwin Schrödinger in 1935 to illustrate the apparent paradoxes of quantum mechanics. It involves a hypothetical scenario where a cat is placed in a sealed box with a radioactive atom, a Geiger counter, and a poison mechanism. If the radioactive atom decays (a random quantum event), the poison is released, killing the cat. According to quantum mechanics, until observed, the radioactive atom exists in a superposition of both decayed and undecayed states. This implies the cat would be simultaneously both alive and dead until someone opens the box to check. The experiment highlights the disconnect between quantum mechanical principles and our everyday experience of reality, challenging our understanding of observation.
D2-20	For a fossil to exist it needs to survive a long time. Many animals are active kills by predictors. In such cases one might find a few teeth but most bone and tissue is gone. In rain forests the amount of insects make finding anything unlikely. Any animal is quickly dissected. Bones often dissolve in the slightly acid rain. Good places to find fossils are hence riverbeds or banks where cold freezing water quickly kills the animal and sediments are readily available to bury the remains long enough to begin the next step in the fossilization process. Similarly lakes that break through a wall and empty might evaporate and have large amount of fish and other animals.
D2-28	Fossils have played a significant role in shaping Greek mythology, as ancient Greeks often interpreted large, mysterious bones as evidence of mythical creatures. The discovery of massive fossils, such as those of mammoths, mastodons, and prehistoric reptiles, likely influenced myths about giants, cyclopes, and dragons. For instance, the large skulls of prehistoric elephants, which have a central nasal cavity, may have been mistaken for the skulls of one-eyed Cyclopes, reinforcing the legend of these formidable beings. Similarly, fossilized bones of large extinct animals could have inspired stories of titans and heroes slaying monstrous beasts. Greek naturalists like Herodotus and Pliny the Elder documented fossil findings, showing early attempts to explain these remains within a mythological and historical framework.
D2-29	Synthetic biology emerged as a specialized branch of bioengineering over the last 15-20 years, propelled by the technologies of DNA sequencing and DNA synthesis. The emphasis is now on characterizing molecular components and standardizing them so that they work together. Today, computer programming languages have become more abstract by building upon previous languages, making it easier and faster to create complex programs. Similarly, in the future, standardized molecular components will make biology understandable through abstraction. This will in turn make it easier to engineer new functions. Of course any biological abstraction would rely on molecular details (just like all computer code is finally executed as assembly), so studying molecular biology is very worthwhile.
D2-32	Synthetic biology is bringing together engineers and biologists to design and build novel biomolecular components, networks and pathways, and to use these constructs to rewire and reprogram organisms. These re-engineered organisms will change our lives over the coming years, leading to cheaper drugs, 'green' means to fuel our cars and targeted therapies for attacking 'superbugs' and diseases, such as cancer. The de novo engineering of genetic circuits, biological modules and synthetic pathways is beginning to address these crucial problems and is being used in related practical applications. The circuit-like connectivity of biological parts and their ability to collectively process logical operations was first appreciated nearly 50 years ago.
D2-33	Computational systems biology is an emerging field in biological simulation that attempts to model or simulate intra- and intercellular events using data gathered from genomic, proteomic or metabolomic experiments. The need to model complex temporal and spatiotemporal processes at many different scales has led to the emergence of numerous techniques, including systems of differential equations, Petri nets, cellular automata simulators, agent-based models and pi calculus. This review provides a brief summary and an assessment of most of these approaches. It also provides examples of how these methods are being used to facilitate drug discovery and development. Systems biology is a newly emerging, multi-disciplinary field that studies the mechanisms underlying complex biological processes by treating these processes as integrated systems.
D2-34	The main causes of lung cancer are primarily linked to smoking and environmental exposures. Tobacco smoking is the leading cause, responsible for about 85% of cases, as the harmful chemicals in smoke damage DNA in lung cells. Secondhand smoke is also a significant risk factor. Another major cause is exposure to radon gas, a naturally occurring radioactive substance that can accumulate in buildings. Occupational exposures to substances like asbestos, arsenic, and diesel exhaust also increase the risk, particularly in certain industries. Additionally, genetic factors and family history can make some individuals more susceptible, while air pollution and poor air quality further contribute to the development of lung cancer.
D2-35	"Einstein and Schrödinger exchanged significant ideas throughout their careers. Both physicists shared deep skepticism about the Copenhagen interpretation of quantum mechanics, maintaining a correspondence that revealed their mutual discomfort with quantum indeterminism. Einstein's famous quote, ""God does not play dice with the universe,"" echoed Schrödinger's own reservations. Their intellectual relationship flourished through letters and occasional meetings, where they discussed unified field theory and the philosophical implications of quantum mechanics. Schrödinger's cat thought experiment partially emerged from these discussions, attempting to illustrate what both considered absurdities in quantum theory. Though they never resolved their concerns about quantum mechanics, their dialogue represented one of the most profound scientific exchanges of the 20th century, challenging mainstream quantum interpretations while searching for deeper truths."
D2-36	"Per- and polyfluoroalkyl substances (PFAS) are a large, complex group of synthetic chemicals that have been used in consumer products worldwide since the 1950s. These compounds serve as ingredients in various everyday products, with applications ranging from industrial uses to consumer goods due to their unique water and oil-repellent properties. PFAS are often referred to as ""forever chemicals"" due to their extraordinary environmental persistence They represent persistent contaminants that are particularly challenging to address through conventional wastewater treatment processes Their persistence has made them contaminants of emerging concern in industrialized countries, where they accumulate in the environment over time. Research indicates that PFAS exposure poses significant health risks: PFAS disrupt biological membranes resulting in cellular inhibition."
D2-38	The variation in PFAS regulatory values across the globe can be easily addressed due to the influence of multiple scientific, technical, and social factors. The varied toxicology and the insufficient definition of PFAS exposure rate are among the main factors contributing to this discrepancy. The lack of proven standard approaches for examining PFAS in surface water, groundwater, wastewater, or solids adds more technical complexity. Although it is agreed that PFASs pose potential health risks in various media, the link between the extent of PFAS exposure and the significance of PFAS risk remain among the evolving research areas. There is a growing need to address the correlation between the frequency and the likelihood of human exposure to PFAS .
D2-40	An unexpected emergent property of a complex system may be a result of the interplay of the cause-and-effect among simpler, integrated parts (see biological organisation). Biological systems manifest many important examples of emergent properties in the complex interplay of components. Traditional study of biological systems requires reductive methods in which quantities of data are gathered by category, such as concentration over time in response to a certain stimulus. Computers are critical to analysis and modelling of these data. The goal is to create accurate real-time models of a system's response to environmental and internal stimuli, such as a model of a cancer cell in order to find weaknesses in its signalling pathways.
D2-43	Early lung cancer often has no symptoms and can only be detected by medical imaging. As the cancer progresses, most people experience nonspecific respiratory problems: coughing, shortness of breath, or chest pain. Other symptoms depend on the location and size of the tumor. Those suspected of having lung cancer typically undergo a series of imaging tests to determine the location and extent of any tumors. Definitive diagnosis of lung cancer requires a biopsy of the suspected tumor be examined by a pathologist under a microscope. . In addition to recognizing cancerous cells, a pathologist can classify the tumor according to the type of cells it originates from.
D2-44	To further illustrate, Schrödinger described how one could, in principle, create a superposition in a large-scale system by making it dependent on a quantum particle that was in a superposition. He proposed a scenario with a cat in a closed steel chamber, wherein the cat's life or death depended on the state of a radioactive atom, whether it had decayed and emitted radiation or not. According to Schrödinger, the Copenhagen interpretation implies that the cat remains both alive and dead until the state has been observed. Schrödinger did not wish to promote the idea of dead-and-live cats as a serious possibility; on the contrary, he intended the example to illustrate the absurdity of the existing view of quantum mechanics
D2-45	Educational robotics teaches the design, analysis, application and operation of robots. Robots include articulated robots, mobile robots or autonomous vehicles. Educational robotics can be taught from elementary school to graduate programs. Robotics may also be used to motivate and facilitate the instruction other, often foundational, topics such as computer programming, artificial intelligence or engineering design. Robotics engineers design robots, maintain them, develop new applications for them, and conduct research to expand the potential of robotics. Robots have become a popular educational tool in some middle and high schools, as well as in numerous youth summer camps, raising interest in programming, artificial intelligence and robotics among students.
D2-47	Systems Biology is not one thing, and is quite different to different people. In my view Systems Biology is really a modern perspective on doing basic Biology research. It is not really an applied field, intending to directly be useful for industry and make products. Instead it aims to unlock the key principles of life, which can be used to do that thereafter. However, a lot of the processes of Systems Biology involves using cutting edge tools, especially computational ones, which can give you a good handle on the process of being a “data scientist”, handling large amount of experimentally obtained data, processing and analyzing it to make decisions about future course of actions.
D2-48	Educational robotics represents an innovative approach to learning that combines technology with hands-on experience, offering numerous benefits for students across different age groups. Educational robotics provides a strong foundation in science, technology, engineering, and mathematics (STEM). As students engage with robotics, they naturally develop capabilities in these critical disciplines, preparing them for future academic and career challenges. This approach is increasingly viewed as a strategic way to introduce and incentivize students to pursue careers in STEM fields. Beyond technical knowledge, robotics education cultivates essential life skills that benefit students regardless of their eventual career path. Students working with robotics develop problem-solving abilities, critical thinking, and resilience when facing challenges.
D2-50	The expanding Sun is expected to vaporize Mercury as well as Venus, and render Earth and Mars uninhabitable (possibly destroying Earth as well). Eventually, the core will be hot enough for helium fusion; the Sun will burn helium for a fraction of the time it burned hydrogen in the core. The Sun is not massive enough to commence the fusion of heavier elements, and nuclear reactions in the core will dwindle. Its outer layers will be ejected into space, leaving behind a dense white dwarf, half the original mass of the Sun but only the size of Earth. The ejected outer layers may form a planetary nebula, returning some of the material that formed the Sun.
D2-51	"The Big Bang theory is the predominant scientific explanation for our universe's origin, describing not an explosion but rather how the universe expanded from an initial state of extreme density and temperature approximately 13.8 billion years ago. This initial state, called a singularity, contained all matter in an incredibly hot, dense point before expansion began. Proposed by physicist Georges Lemaître in 1927, the theory is supported by multiple lines of evidence, including the cosmic microwave background radiation (essentially the Big Bang's ""afterglow"") and the abundance of light elements in the universe. Later refinements include cosmic inflation theory, suggesting that the early universe briefly expanded faster than light speed."
D2-56	Metabolic engineering is a specialized field that focuses on optimizing metabolic pathways in organisms to enhance the production of desired products, such as biofuels, pharmaceuticals, and industrial chemicals. Applications include the development of microbes or cells capable of efficiently producing high-value compounds, improving the yield and sustainability of chemical manufacturing, and enhancing the production of bio-based materials. It also plays a key role in environmental biotechnology, such as detoxifying pollutants or remediating soil and water. Additionally, metabolic engineering is used to improve agricultural crops by enhancing their nutrient content, stress tolerance, or resistance to pests. By tailoring metabolic pathways, it enables the creation of novel therapeutic agents, advanced biofuels, and sustainable industrial processes.
D2-57	"In modern terms Schrodinger's hypothetical cat experiment describes the measurement problem: quantum theory describes the cat system as a combination of two possible outcomes but only one outcome is ever observed.  The experiment poses the question, ""when does a quantum system stop existing as a superposition of states and become one or the other?"" (More technically, when does the actual quantum state stop being a non-trivial linear combination of states, each of which resembles different classical states, and instead begin to have a unique classical description?) Standard microscopic quantum mechanics describes multiple possible outcomes of experiments but only one outcome is observed. The thought experiment illustrates this apparent paradox."
D2-58	The Big Bang theory is the prevailing cosmological model explaining the origin and evolution of the universe. It proposes that the universe began approximately 13.8 billion years ago as an extremely hot and dense point, which rapidly expanded in a process called inflation. As the universe expanded and cooled, fundamental particles and then atoms formed, eventually leading to the creation of stars, galaxies, and other cosmic structures. Observational evidence, such as the cosmic microwave background radiation (leftover heat from the early universe) and the abundance of light elements, strongly supports this theory. The Big Bang also explains the universe's large-scale structure and the observed redshift of distant galaxies, which indicates the ongoing expansion of space.
D2-59	"Synthetic biology, which aims to design or assemble biological parts for useful applications, has emerged as a transformative field with wide-ranging implications across multiple sectors. Synthetic biology has significant medical applications, including improving drug discovery and development through rapid screening of DNA sequences, often leveraging AI and machine learning to identify promising drug candidates. The field enables production of therapeutic molecules with precise targeting and efficacy, and advances antibody and vaccine production. Researchers are engineering biological systems that could potentially treat complex diseases such as cancer. In industrial settings, synthetic biology enables the creation of ""cell factories"" that can efficiently produce valuable compounds. Applications include: Biofuel production through engineered microorganisms; Production of pharmaceuticals and biomaterials using biological systems"
D2-60	The dangers of PFAS, or per- and polyfluoroalkyl substances, stem from their persistence in the environment and their ability to bioaccumulate in living organisms. Known as 'forever chemicals,' PFAS do not break down easily, leading to long-term environmental contamination. They are found in products like non-stick cookware, firefighting foam, and water-repellent textiles. PFAS can enter the human body through contaminated drinking water and food, particularly in areas near industrial sites. Prolonged exposure has been linked to serious health issues, including certain cancers, hormonal disruptions, and weakened immune systems. Additionally, PFAS contribute to ecosystem harm by affecting wildlife and biodiversity. Their long-lasting nature makes them a significant environmental and public health concern, as they are challenging to eliminate.
D2-62	Educational robotics offers a wealth of advantages, particularly in promoting interactive and hands-on learning, fostering creativity, and encouraging teamwork. By providing practical applications for theoretical concepts, it strengthens students' understanding of STEM subjects such as motion and forces. Adding to this, it equips students with technical skills relevant to modern industries and cater to various learning styles, including visual and kinesthetic learning, especially benefiting students with disabilities. Furthermore, robotics offers immediate feedback, enabling students to quickly identify and correct mistakes, which reinforces their learning process. Its fun and interactive nature boosts motivation and engagement, instilling a positive attitude toward education. In essence, educational robotics not only enhances academic learning but also prepares students for the demands of the 21st-century workforce.
D2-65	Understanding Greenland ice sheet (GrIS) hydrology is essential for evaluating response of ice dynamics to a warming climate and future contributions to global sea level rise. Recently observed increases in temperature and melt extent over the GrIS have prompted numerous remote sensing, modeling, and field studies gauging the response of the ice sheet and outlet glaciers to increasing meltwater input, providing a quickly growing body of literature describing seasonal and annual development of the GrIS hydrologic system. This system is characterized by supraglacial streams and lakes that drain through moulins, providing an influx of meltwater into englacial and subglacial environments that increases basal sliding speeds of outlet glaciers in the short term.
D2-66	Computational systems biology is crucial for understanding the complexity of biological systems through the integration of mathematical modeling, computer simulations, and data analysis. It enables researchers to study how genes, proteins, and metabolic pathways interact in a holistic manner, rather than in isolation. This approach is essential for deciphering the dynamics of biological networks, such as gene regulatory networks, signaling pathways, and metabolic systems. The importance of computational systems biology lies in its ability to predict biological behaviors, identify potential drug targets, and develop personalized medicine strategies. It also helps in understanding diseases at a systems level, enabling the identification of biomarkers for diagnosis and treatment.
D2-68	The ultimate goal of metabolic engineering is to be able to use these organisms to produce valuable substances on an industrial scale in a cost-effective manner. Current examples include producing beer, wine, cheese, pharmaceuticals, and other biotechnology products.[2] Another possible area of use is the development of oil crops whose composition has been modified to improve their nutritional value.[3] Some of the common strategies used for metabolic engineering are (1) overexpressing the gene encoding the rate-limiting enzyme of the biosynthetic pathway, (2) blocking the competing metabolic pathways, (3) heterologous gene expression, and (4) enzyme engineering. Since cells use these metabolic networks for their survival, changes can have drastic effects on the cells' viability.
D2-71	Metabolic engineering is the field of biotechnology that involves the modification of an organism’s metabolic pathways to enhance the production of desired products, such as chemicals, fuels, pharmaceuticals, or food ingredients. By altering or optimizing the flow of metabolites within cells, metabolic engineers aim to improve the efficiency and yield of biochemical processes. This is done through techniques like gene editing, overexpression of enzymes, and the introduction of new pathways. Metabolic engineering has applications in industrial biotechnology, where it enables the production of biofuels, antibiotics, and biodegradable plastics, and is also crucial for improving processes in medicine, agriculture, and environmental sustainability.
D2-72	Synthetic biology is an interdisciplinary field that combines biology with engineering, technology, and computational science to design and construct new biological parts, devices, and systems, or to redesign existing natural biological systems for specific purposes. It involves advanced genetic engineering techniques, such as CRISPR for gene editing and synthetic DNA construction, alongside computational modeling and bioprinting. Applications span medicine, environmental science, and industry, including the development of organisms capable of producing medications, biofuels, or addressing environmental challenges. While it holds great potential, synthetic biology also raises ethical considerations regarding the unintended consequences of engineered organisms and the responsible use of this technology to ensure societal benefits.
D2-74	Greenland is the world’s largest island, located in the North Atlantic Ocean, and is an autonomous territory of Denmark. It is geographically part of North America but politically and historically connected to Europe. Greenland is known for its vast ice sheet, which covers about 80% of the island, making it a critical component of global climate systems. The island’s population is relatively small, primarily consisting of Inuit people. Its economy relies heavily on fishing, while tourism and mining are growing sectors. Greenland has significant natural resources, including minerals and oil, though environmental concerns and the impact of climate change, such as melting ice, are major issues. Greenland’s strategic location has also gained geopolitical importance in recent years.
D2-75	"The Big Bang theory is an effort to explain what happened at the very beginning of our universe. Discoveries in astronomy and physics have shown beyond a reasonable doubt that our universe did in fact have a beginning. Prior to that moment there was nothing; during and after that moment there was something: our universe. The big bang theory is an effort to explain what happened during and after that moment. According to the standard theory, our universe sprang into existence as ""singularity"" around 13.7 billion years ago. What is a ""singularity"" and where does it come from? Well, Singularities are zones which defy our current understanding of physics. They are thought to exist at the core of ""black holes."""
D2-76	Phthalates or phthalate esters, are esters of phthalic acid. They are mainly used as plasticizers, i.e., substances added to plastics to increase their flexibility, transparency, durability, and longevity. They are used primarily to soften polyvinyl chloride (PVC). While phthalates are commonly used as plasticizers, not all plasticizers are phthalates. The two terms are specific, unique, and not used interchangeably. Lower-molecular-weight phthalates are typically replaced in many products in the United States, Canada, and European Union over health concerns. They are being replaced by higher molecular-weight phthalates as well as non-phthalic plasticizers. Phthalates are commonly ingested in small quantities via the diet. There are numerous forms of phthalates not regulated by governments. One of the most  known phtalates is bis(2-ethylhexyl) phthalate.
D2-79	Greenland is as old as the rest of the earth. And it has no other name than Grønland or Grœnland both of which translate and even sound close to Greenland. The very first humans there were Vikings or Norse, which as government evolved became Denmark. Norway and Denmark, Iceland and Faroe Islands, even areas of England and Scotland ruled by Vikings came under a united Denmark. The story is Ingólgur Arnason wanted to discourage more people coming so named his discovery Iceland, but Eric the Red wanting to encourage settlers named his discovery Greenland although Greenland is more appropriate for Iceland and Iceland more appropriate for Greenland. Just consider them early examples of 5th Avenue Advertizing.
D2-8	Fossils appear to have directly contributed to the mythology of many civilizations, including the ancient Greeks. Classical Greek historian Herodotos wrote of an area near Hyperborea where gryphons protected golden treasure. There was indeed gold mining in that approximate region, where beaked Protoceratops skulls were common as fossils. A later Greek scholar, Aristotle, eventually realized that fossil seashells from rocks were similar to those found on the beach, indicating the fossils were once living animals.  He had previously explained them in terms of vaporous exhalations,  which Persian polymath Avicenna modified into the theory of petrifying fluids (succus lapidificatus). Recognition of fossil seashells as originating in the sea was built upon in the 14th century by Albert of Saxon.
D2-81	Educational robotics has a wide range of applications aimed at enhancing learning experiences and developing essential skills in students. One of its primary uses is to teach STEM (Science, Technology, Engineering, and Mathematics) concepts, providing hands-on learning through building, programming, and interacting with robots. It encourages critical thinking, problem-solving, and creativity. Robotics also supports the development of coding and computational thinking, as students learn to program robots to perform tasks and solve challenges. Furthermore, educational robotics promotes collaboration and teamwork, as many projects involve group work. In addition to its academic benefits, it can also be used to teach engineering principles, mechanical design, and mathematics in a practical, engaging way. Additionally, robotics fosters 21st-century skills like adaptability, perseverance, and communication.
D2-83	"Metabolic engineering is a multidisciplinary field that applies principles of genetic engineering to modify cellular metabolism. Its goals encompass several key objectives: Production Optimization -  The fundamental aim of metabolic engineering is to mathematically model metabolic networks, calculate yields of useful products, and identify constraints in these networks. By applying genetic engineering techniques to modify these networks, researchers seek to increase product formation and overall process efficiency. Creating Industrial Cell Factories A primary purpose is to develop ""cell factories"" capable of producing cost-effective molecules at industrial scale. This involves optimizing existing biochemical pathways or introducing new pathway components, most commonly in bacteria, yeast, or plantsProcess Improvement."
D2-84	Fossils are the preserved remains or traces of ancient life, offering critical insights into Earth's history and evolution. They provide evidence of extinct species, helping scientists reconstruct past ecosystems and understand how life has changed over millions of years. Fossils reveal how species adapted, evolved, or became extinct, shedding light on the processes of natural selection and environmental changes. Additionally, they are used to date rock layers and determine the age of geological formations, aiding in the study of Earth's geologic timeline. Fossils also provide clues about ancient climates and environmental conditions, helping researchers understand past climate changes and their impacts. By studying fossils, we gain a deeper understanding of life's diversity, the history of our planet.
D2-86	Robotic education is nothing but the Hybrid Education that is provided to students, education when integrated with technology and coding is called as Robotic education. Robotic education is far better than the traditional educational system because of it instructional based teaching. Coding is not preparing an application, it is just understanding the instructions in a sequence and then execution them in a way we need the result. Coding allows the users to create something, traditional education is all about learning that is there in book, where creativity, critical thinking, programming is neglected. As robotic education is based on algorithm (set of programmed instructions to execute in sequence) it triggers students to go one step ahead.
D2-87	Join any group of distance educators today and the chances are you will hear talk of exponential expansion of distance education when the information superhighways come into being, within a decade. This might be called the Big Bang theory of distance education. You will hear enthusiastic talk about two-way communication (at last) between teacher and student, replacing the old one-way systems of print, radio and television. At last, students everywhere will be able to explore massive knowledge stores. You may also hear gloomy comments about limited access, costs and the dangers of technological determinism. You may even hear critics who seriously decry the commodification of knowledge represented by distance education.
D2-89	Greenland, the world's largest island that isn't a continent, covers 836,330 square miles in the North Atlantic, with approximately 80% of its surface covered by ice. An autonomous territory within the Kingdom of Denmark, Greenland has a population of about 56,000, predominantly Inuit. The capital, Nuuk, houses roughly one-third of its inhabitants. Despite its name, most of Greenland experiences Arctic conditions with limited vegetation. Its economy relies on fishing, tourism, and substantial mineral resources, including rare earth elements becoming increasingly valuable due to climate change. Greenlandic culture blends traditional Inuit practices with Scandinavian influences. Climate change poses significant challenges as the melting ice sheet contributes to global sea level rise while simultaneously opening new economic opportunities through increased resource accessibility.
D2-91	The Big Bang theory is the leading explanation for the origin of the universe. It posits that the universe began approximately 13.8 billion years ago from an extremely hot, dense state known as a singularity. This singularity then rapidly expanded in a process called cosmic inflation, causing space itself to stretch exponentially in a fraction of a second. As the universe expanded, it began to cool, allowing matter to form, leading to the creation of atoms, stars, galaxies, and eventually the structure of the universe we observe today. Key evidence supporting the Big Bang theory includes the cosmic microwave background radiation (a faint glow left over from the early universe), the abundance of light elements like hydrogen and helium.
D2-93	Metabolic engineering is not another form of classic manipulation of intermediary metabolism; rather, it is the purposeful design of metabolic networks. The metabolic engineering approach examines biochemical reactions in their entirety, rather than individually, and is concerned with the construction of novel pathways, the thermodynamic feasibility of pathways, and the location of limiting branch-point(s) and enzymatic reaction(s) in a reaction network. The ultimate goal is the construction of strains with superior yield and productivity obtained through a convergent evolutionary process. Progress has been made toward this objective within the framework of Metabolic Control Analysis through the development of methodologies for the experimental determination of elasticities and flux control coefficients.
D2-94	Synthetic biology is an interdisciplinary field that combines principles from biology, engineering, and computer science to design and construct new biological parts, systems, and organisms or to re-engineer existing biological systems for specific purposes. It involves creating synthetic genetic circuits, modifying genomes, or designing artificial enzymes to carry out tasks that natural organisms cannot. The goal of synthetic biology is to apply engineering principles to biology, enabling the synthesis of novel compounds, the production of biofuels, the development of new therapeutics, and innovations in environmental sustainability. It has vast potential for applications in medicine, agriculture, energy, and biotechnology, but also raises important ethical and safety considerations.
D2-96	Though a part of the continent of North America, Greenland has been politically and culturally associated with the European kingdoms of Norway and Denmark for more than a millennium, beginning in 986.[19] Greenland has been inhabited at intervals over at least the last 4,500 years by circumpolar peoples whose forebears migrated there from what is now Canada.[20][21] Norsemen from Norway settled the uninhabited southern part of Greenland beginning in the 10th century (having previously settled Iceland), and their descendants lived in Greenland for 400 years until disappearing in the late 15th century. The 13th century saw the arrival of Inuit.
D2-97	There has been a steady increase in the number of studies investigating educational robotics and its impact on academic and social skills of young learners. Educational robots are used both in and out of school environments to enhance K–12 students’ interest, engagement, and academic achievement in various fields of STEM education. Some prior studies show evidence for the general benefits of educational robotics as being effective in providing impactful learning experiences. However, there appears to be a need to determine the specific benefits which have been achieved through robotics implementation in K–12 formal and informal learning settings. In this study, we present a systematic review of the literature on K–12 educational robotics.
D2-98	Plasticizers like phthalates were thought to be mainly hormone disruptors. They caused fertility problems and adverse developmental effects. Surprisingly, since the 2000s researchers noted that they were instrumental in causing obesity, type 2 diabetes and cardiovascular disease. Another key point, phthalates can be absorbed through the skin, ingested by mouth or inhaled. In order to understand how phthalates cause cardiovascular disease, we need to investigate the effects of phthalates on fatty tissue. Phthalates have a negative effect on fatty tissue that leads to adipose tissue dysfunction. Fat cells develop out of adipose stem cells committing themselves and maturing into fatty cells that can store lipids. This process has the name adipogenesis.
D2-99	The main causes of lung cancer are multifaceted and involve a combination of lifestyle, environmental, and genetic factors. The most significant risk factor is smoking, including both active smoking and secondhand exposure. Cigarettes release numerous harmful chemicals that damage lung cells over time, increasing the likelihood of cancer development. While not everyone who smokes develops lung cancer, the risk is considerably higher for smokers compared to non-smokers. Radon, a radioactive gas found in certain soil types, is another major contributor. It can accumulate in homes and buildings, posing a long-term health risk if exposure levels are high. Outdoor and indoor air pollution, such as emissions from vehicles and industrial activities, as well as cooking fumes from coal or wood.
