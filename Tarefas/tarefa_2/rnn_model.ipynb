{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RNN Model Notebook**\n",
    "\n",
    "@authors: miguelrocha and Grupo 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix\n",
    "from helpers.dataset import Dataset\n",
    "from helpers.activation import TanhActivation\n",
    "from helpers.losses import BinaryCrossEntropy\n",
    "from helpers.metrics import accuracy\n",
    "from helpers.activation import ReLUActivation\n",
    "from models.rnn_model import RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modificação na classe Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
    "        self.velocity = {}  # Dicionário para armazenar velocidades dos gradientes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "\n",
    "    def update(self, param, grad):\n",
    "        \"\"\"Atualiza os pesos usando Gradient Descent com Momentum\"\"\"\n",
    "\n",
    "        param_id = id(param)  # Usar ID único do numpy array\n",
    "\n",
    "        if param_id not in self.velocity:\n",
    "            self.velocity[param_id] = np.zeros_like(grad)\n",
    "\n",
    "        # Atualização com momentum\n",
    "        self.velocity[param_id] = self.momentum * self.velocity[param_id] + (1 - self.momentum) * grad\n",
    "        return param - self.learning_rate * self.velocity[param_id]  # Retorna os novos pesos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tratamento de Dados**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Análise Inicial dos Datasets e Junção dos mesmos para tratamento simultâneo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Completo - Primeiras 5 linhas:\n",
      "     ID                                               Text  Label\n",
      "0  3035  We present a general numerical scheme for the ...  Human\n",
      "1   925  The present paper aims at introducing the inno...  Human\n",
      "2   470  This research paper investigates the phenomeno...     AI\n",
      "3  2060  This research paper explores the concept of th...     AI\n",
      "4  3167  The paper explores the concept of reference fr...     AI\n",
      "\n",
      "Dataset Completo - Ultimas 5 linhas:\n",
      "          ID                                               Text Label\n",
      "1979   D3-96  The relationship between Darwin's theory of ev...   Nap\n",
      "1980   D3-97  Charles Darwin's historic visit to the Galápag...   Nap\n",
      "1981   D3-98  The Galápagos Islands (Spanish: Islas Galápago...   Nap\n",
      "1982   D3-99  The Galapagos Islands played a pivotal role in...   Nap\n",
      "1983  D3-100  The Galapagos’ natural environment was substan...   Nap\n"
     ]
    }
   ],
   "source": [
    "# Definir os caminhos dos arquivos de TREINO\n",
    "input_csv1 = \"../tarefa_1/test_input_dataset/merged_inputs.csv\"\n",
    "output_csv1 = \"../tarefa_1/test_output_dataset/merged_outputs.csv\"\n",
    "\n",
    "# Definir os caminhos dos arquivos de TESTE FINAL\n",
    "input_csv2 = \"../tarefa_2/classify_input_datasets/submission3_inputs.csv\"\n",
    "output_csv2 = \"../tarefa_2/layout_datasets/submission3_layout_outputs.csv\" # dataset apenas utilizado para adicionar o layout ID Label\n",
    " \n",
    "# Carregar os datasets de treino\n",
    "df_input1 = pd.read_csv(input_csv1, sep=\"\\t\")  \n",
    "df_output1 = pd.read_csv(output_csv1, sep=\"\\t\")\n",
    "\n",
    "# Carregar os datasets de teste\n",
    "df_input2 = pd.read_csv(input_csv2, sep=\"\\t\")\n",
    "df_output2 = pd.read_csv(output_csv2, sep=\"\\t\")\n",
    "\n",
    "# Junção com coluna ID\n",
    "df_train = pd.merge(df_input1, df_output1, on=\"ID\")\n",
    "df_test = pd.merge(df_input2, df_output2, on=\"ID\")\n",
    "\n",
    "# Concatenar treino e teste para aplicar as alterações simultaneamente\n",
    "df_dataset1_merged = pd.concat([df_train, df_test], ignore_index=True)\n",
    "\n",
    "# Mostrar as primeiras 5 linhas do dataset completo\n",
    "print(\"\\nDataset Completo - Primeiras 5 linhas:\")\n",
    "print(df_dataset1_merged.head())\n",
    "\n",
    "print(\"\\nDataset Completo - Ultimas 5 linhas:\")\n",
    "print(df_dataset1_merged.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remover caracteres especiais e pontuação e Converter em minúsculas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto limpo - primeiras 5 linhas:\n",
      "     ID                                               Text  Label\n",
      "0  3035  we present a general numerical scheme for the ...  Human\n",
      "1   925  the present paper aims at introducing the inno...  Human\n",
      "2   470  this research paper investigates the phenomeno...     AI\n",
      "3  2060  this research paper explores the concept of th...     AI\n",
      "4  3167  the paper explores the concept of reference fr...     AI\n"
     ]
    }
   ],
   "source": [
    "# Função para limpar texto\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Converter para minúsculas\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Remover pontuação\n",
    "    return text\n",
    "\n",
    "df_dataset1_merged[\"clean_text\"] = df_dataset1_merged[\"Text\"].apply(clean_text)\n",
    "\n",
    "# Manter apenas as colunas desejadas e renomear clean_text para Text\n",
    "df_dataset1_merged = df_dataset1_merged[[\"ID\", \"clean_text\", \"Label\"]].rename(columns={\"clean_text\": \"Text\"})\n",
    "\n",
    "print(\"Texto limpo - primeiras 5 linhas:\")\n",
    "print(df_dataset1_merged.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remover stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto sem stopwords - primeiras 5 linhas:\n",
      "     ID                                               Text  Label\n",
      "0  3035  we present general numerical scheme practical ...  Human\n",
      "1   925  present paper aims introducing innovative tech...  Human\n",
      "2   470  research paper investigates phenomenon softeni...     AI\n",
      "3  2060  research paper explores concept accelerated ex...     AI\n",
      "4  3167  paper explores concept reference frames specia...     AI\n"
     ]
    }
   ],
   "source": [
    "# Lista de stopwords comuns\n",
    "stopwords = {\n",
    "    \"the\", \"of\", \"and\", \"in\", \"to\", \"is\", \"a\", \"that\", \"for\", \"are\", \"on\", \"with\", \n",
    "    \"as\", \"at\", \"by\", \"from\", \"this\", \"it\", \"an\", \"be\", \"or\", \"which\", \"was\", \"were\"\n",
    "}\n",
    "\n",
    "# Função para remover stopwords\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()  # Dividir em palavras\n",
    "    filtered_words = [word for word in words if word not in stopwords]  # Remover stopwords\n",
    "    return \" \".join(filtered_words)  # Juntar as palavras de novo\n",
    "\n",
    "# Aplicar ao dataset\n",
    "df_dataset1_merged[\"Text\"] = df_dataset1_merged[\"Text\"].apply(remove_stopwords)\n",
    "\n",
    "# Exibir as primeiras 5 linhas após remoção de stopwords\n",
    "print(\"Texto sem stopwords - primeiras 5 linhas:\")\n",
    "print(df_dataset1_merged.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Criar Embeddings e Label Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de palavras carregadas do GloVe: 400000\n"
     ]
    }
   ],
   "source": [
    "# Mapear labels para valores numéricos\n",
    "label_map = {\"Human\": 0, \"AI\": 1}\n",
    "df_dataset1_merged[\"Label\"] = df_dataset1_merged[\"Label\"].map(label_map)\n",
    "\n",
    "# Carregar o GloVe\n",
    "EMBEDDING_DIM = 50  # Dimensão do embedding\n",
    "\n",
    "# Diretório e nome do ficheiro GloVe\n",
    "glove_dir = \"helpers\"\n",
    "glove_filename = \"glove.6B.50d.txt\"\n",
    "glove_zip_url = \"http://nlp.stanford.edu/data/glove.6B.zip\"  # URL do GloVe oficial\n",
    "\n",
    "# Criar diretório se não existir\n",
    "os.makedirs(glove_dir, exist_ok=True)\n",
    "\n",
    "# Caminho completo do ficheiro\n",
    "glove_path = os.path.join(glove_dir, glove_filename)\n",
    "glove_zip_path = os.path.join(glove_dir, \"glove.6B.zip\")\n",
    "\n",
    "# Verificar se o ficheiro já existe\n",
    "if not os.path.exists(glove_path):\n",
    "    print(\"Ficheiro GloVe não encontrado. A fazer download...\")\n",
    "\n",
    "    # Download do ficheiro ZIP do GloVe\n",
    "    response = requests.get(glove_zip_url, stream=True)\n",
    "    with open(glove_zip_path, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    \n",
    "    print(\"Download concluído. A extrair ficheiros...\")\n",
    "\n",
    "    # Extrair apenas o ficheiro necessário\n",
    "    with zipfile.ZipFile(glove_zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extract(glove_filename, path=glove_dir)\n",
    "\n",
    "    print(\"Extração concluída!\")\n",
    "\n",
    "# Agora podemos carregar o GloVe\n",
    "embedding_dict = {}\n",
    "with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.array(values[1:], dtype=\"float32\")\n",
    "        embedding_dict[word] = vector\n",
    "\n",
    "print(f\"Total de palavras carregadas do GloVe: {len(embedding_dict)}\")\n",
    "\n",
    "# Converter palavras para embeddings\n",
    "def text_to_embedding(text, embedding_dict, embedding_dim=50):\n",
    "    words = text.split()\n",
    "    embeddings = [embedding_dict.get(word, np.zeros(embedding_dim)) for word in words]  # Usa vetor do GloVe ou vetor zerado\n",
    "    \n",
    "    # Se a lista estiver vazia, retorna um vetor de zeros\n",
    "    if len(embeddings) == 0:\n",
    "        embeddings = [np.zeros(embedding_dim)]\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "df_dataset1_merged[\"Embedding\"] = df_dataset1_merged[\"Text\"].apply(lambda x: text_to_embedding(x, embedding_dict, EMBEDDING_DIM))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padronizar o comprimento das sequências**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formato final dos dados para o modelo: (1984, 130, 50)\n",
      "Dataset após embedding - primeiras 5 linhas:\n",
      "     ID                                               Text  Label\n",
      "0  3035  [[0.5738700032234192, -0.32728999853134155, 0....    0.0\n",
      "1   925  [[0.7249799966812134, 0.5722399950027466, -0.2...    0.0\n",
      "2   470  [[0.7125800251960754, 0.6449199914932251, 0.05...    1.0\n",
      "3  2060  [[0.7125800251960754, 0.6449199914932251, 0.05...    1.0\n",
      "4  3167  [[-0.7121599912643433, 0.028648000210523605, 0...    1.0\n"
     ]
    }
   ],
   "source": [
    "# Padronizar comprimento das sequências\n",
    "MAX_SEQUENCE_LENGTH = 130  # foram testados vários valores sendo o melhor 130\n",
    "\n",
    "def pad_embedding_sequence(seq, max_length, embedding_dim):\n",
    "    seq = np.array(seq)  # Garante que a sequência é um array NumPy\n",
    "    \n",
    "    if seq.shape[0] == 0:  # Se for uma sequência vazia, criar um array de zeros\n",
    "        seq = np.zeros((1, embedding_dim))\n",
    "\n",
    "    if seq.shape[0] > max_length:  # Truncar se for maior\n",
    "        return seq[:max_length]\n",
    "    \n",
    "    padding = np.zeros((max_length - seq.shape[0], embedding_dim))  # Criar padding\n",
    "    return np.vstack([seq, padding])  # Adicionar padding no final\n",
    "\n",
    "# Aplicar padding às sequências de embeddings\n",
    "df_dataset1_merged[\"Embedding\"] = df_dataset1_merged[\"Embedding\"].apply(\n",
    "    lambda x: pad_embedding_sequence(x, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)\n",
    ")\n",
    "\n",
    "# Converter para array NumPy para alimentar o modelo\n",
    "X = np.array(df_dataset1_merged[\"Embedding\"].tolist())\n",
    "y = np.array(df_dataset1_merged[\"Label\"])  # Labels numéricos\n",
    "\n",
    "print(\"Formato final dos dados para o modelo:\", X.shape)  # Deve ser (n_amostras, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)\n",
    "\n",
    "# Manter apenas as colunas desejadas e renomear \"Embedding\" para \"Text\"\n",
    "df_dataset1_merged = df_dataset1_merged[[\"ID\", \"Embedding\", \"Label\"]].rename(columns={\"Embedding\": \"Text\"})\n",
    "\n",
    "print(\"Dataset após embedding - primeiras 5 linhas:\")\n",
    "print(df_dataset1_merged.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalização dos Embeddings**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formato final dos dados para o modelo: (1984, 130, 50)\n",
      "\n",
      "Dataset após normalização dos embeddings:\n",
      "     ID                                               Text  Label\n",
      "0  3035  [[0.6229451367824401, -0.6616349872681468, 0.2...    0.0\n",
      "1   925  [[1.3730016267611846, 1.4058269465314142, -0.6...    0.0\n",
      "2   470  [[0.9887738921291833, 1.5442610704979023, 0.16...    1.0\n",
      "3  2060  [[1.5017445407917538, 1.7339764749495419, 0.17...    1.0\n",
      "4  3167  [[-2.187671688541481, -0.14573981075688355, 0....    1.0\n"
     ]
    }
   ],
   "source": [
    "# Função para normalizar cada embedding (zero mean, unit variance)\n",
    "def normalize_embedding(emb):\n",
    "    mean = np.mean(emb, axis=0)  # Média por dimensão do embedding\n",
    "    std = np.std(emb, axis=0) + 1e-8  # Desvio padrão (evita divisão por zero)\n",
    "    return (emb - mean) / std\n",
    "\n",
    "# Aplicar normalização alternativa aos embeddings\n",
    "df_dataset1_merged[\"Text\"] = df_dataset1_merged[\"Text\"].apply(normalize_embedding)\n",
    "\n",
    "# Converter para array NumPy para treinar o modelo\n",
    "X = np.array(df_dataset1_merged[\"Text\"].tolist())\n",
    "y = np.array(df_dataset1_merged[\"Label\"])  # Labels numéricos\n",
    "\n",
    "print(\"Formato final dos dados para o modelo:\", X.shape)  # Deve ser (n_amostras, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)\n",
    "\n",
    "# Print do dataset atualizado\n",
    "print(\"\\nDataset após normalização dos embeddings:\")\n",
    "print(df_dataset1_merged.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop da coluna ID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formato final dos dados para o modelo: (1984, 130, 50)\n",
      "\n",
      "Dataset após drop:\n",
      "                                                Text  Label\n",
      "0  [[0.6229451367824401, -0.6616349872681468, 0.2...    0.0\n",
      "1  [[1.3730016267611846, 1.4058269465314142, -0.6...    0.0\n",
      "2  [[0.9887738921291833, 1.5442610704979023, 0.16...    1.0\n",
      "3  [[1.5017445407917538, 1.7339764749495419, 0.17...    1.0\n",
      "4  [[-2.187671688541481, -0.14573981075688355, 0....    1.0\n"
     ]
    }
   ],
   "source": [
    "if \"ID\" in df_dataset1_merged.columns:\n",
    "    df_dataset1_merged = df_dataset1_merged.drop(columns=[\"ID\"])\n",
    "\n",
    "print(\"Formato final dos dados para o modelo:\", X.shape)  # Deve ser (n_amostras, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)\n",
    "\n",
    "# Print do dataset atualizado\n",
    "print(\"\\nDataset após drop:\")\n",
    "print(df_dataset1_merged.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Divisão do Dataset**\n",
    "\n",
    "Dataset de Treino:\n",
    "\n",
    "- 70% : Treino\n",
    "- 15% : Validação\n",
    "- 15% : Teste\n",
    "\n",
    "Dataset de Avaliação:\n",
    "\n",
    "- 100% : Teste Final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do conjunto de treino: (1318, 2)\n",
      "Tamanho do conjunto de validação: (282, 2)\n",
      "Tamanho do conjunto de teste: (284, 2)\n",
      "Tamanho do conjunto de avaliação final: (100, 2)\n",
      "Formato dos dados:\n",
      "   Treino: (1318, 130, 50)\n",
      "   Validação: (282, 130, 50)\n",
      "   Teste: (284, 130, 50)\n",
      "   Avaliação final: (100, 130, 50)\n"
     ]
    }
   ],
   "source": [
    "# Definir seed global para garantir reprodutibilidade\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "######################################################### dataset de teste\n",
    "# Separar as últimas linhas para avaliação final\n",
    "df_eval_final = df_dataset1_merged.tail(100)\n",
    "\n",
    "# Remover essas linhas do dataset antes de embaralhar\n",
    "df_remaining = df_dataset1_merged.iloc[:-100]\n",
    "#########################################################\n",
    "\n",
    "# Embaralhar o dataset restante\n",
    "df_remaining = df_remaining.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "# Definir proporções de treino (70%), validação (15%) e teste (15%)\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15  # 15% validação\n",
    "test_ratio = 0.15  # 15% teste\n",
    "\n",
    "# Definir índices para divisão\n",
    "train_index = int(len(df_remaining) * train_ratio)\n",
    "val_index = train_index + int(len(df_remaining) * val_ratio)\n",
    "\n",
    "# Separar os conjuntos de treino, validação e teste\n",
    "df_train = df_remaining.iloc[:train_index]\n",
    "df_val = df_remaining.iloc[train_index:val_index]\n",
    "df_test = df_remaining.iloc[val_index:]\n",
    "\n",
    "# Print dos tamanhos dos datasets\n",
    "print(f\"Tamanho do conjunto de treino: {df_train.shape}\")\n",
    "print(f\"Tamanho do conjunto de validação: {df_val.shape}\")\n",
    "print(f\"Tamanho do conjunto de teste: {df_test.shape}\")\n",
    "print(f\"Tamanho do conjunto de avaliação final: {df_eval_final.shape}\")\n",
    "\n",
    "# Converter para arrays NumPy\n",
    "X_train, y_train = np.array(df_train[\"Text\"].tolist()), np.array(df_train[\"Label\"])\n",
    "X_val, y_val = np.array(df_val[\"Text\"].tolist()), np.array(df_val[\"Label\"])\n",
    "X_test, y_test = np.array(df_test[\"Text\"].tolist()), np.array(df_test[\"Label\"])\n",
    "X_eval_final, y_eval_final = np.array(df_eval_final[\"Text\"].tolist()), np.array(df_eval_final[\"Label\"])\n",
    "\n",
    "# Print dos formatos dos dados\n",
    "print(f\"Formato dos dados:\")\n",
    "print(f\"   Treino: {X_train.shape}\")\n",
    "print(f\"   Validação: {X_val.shape}\")\n",
    "print(f\"   Teste: {X_test.shape}\")\n",
    "print(f\"   Avaliação final: {X_eval_final.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verificação Final do Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Primeiras 5 entradas do conjunto de TREINO:\n",
      "                                                Text  Label\n",
      "0  [[1.2648330130038024, -1.3120997287976162, 0.0...    0.0\n",
      "1  [[1.923546133922557, -2.3442699269752563, 0.43...    0.0\n",
      "2  [[1.3654664319093903, 2.0011569695179863, 0.10...    1.0\n",
      "3  [[1.3464225567418249, 1.2965997871220445, 0.15...    1.0\n",
      "4  [[1.3212941873089101, 1.9241303852529341, -1.3...    1.0\n",
      "\n",
      " Primeiras 5 entradas do conjunto de VALIDAÇÃO:\n",
      "                                                   Text  Label\n",
      "1318  [[1.632567589969771, 1.3259334388866952, 0.095...    1.0\n",
      "1319  [[-1.9020010963581078, 0.13419086271709868, 0....    1.0\n",
      "1320  [[-0.5495222804483586, -0.04835099471844644, -...    0.0\n",
      "1321  [[1.0487307372902497, 1.4485013158732332, -0.1...    1.0\n",
      "1322  [[0.38219893342825073, -0.3728841095297984, -1...    0.0\n",
      "\n",
      " Primeiras 5 entradas do conjunto de TESTE:\n",
      "                                                   Text  Label\n",
      "1600  [[0.5849216259806982, 1.0337079910141158, 0.02...    1.0\n",
      "1601  [[1.5977119560468027, 1.7411947929493024, -0.0...    1.0\n",
      "1602  [[0.9950938780213777, 1.292549912981319, 0.322...    1.0\n",
      "1603  [[0.7798383898149496, -1.0176742917553248, 0.0...    0.0\n",
      "1604  [[1.2587668669195653, 1.301188002885766, 0.430...    0.0\n",
      "\n",
      " Primeiras 5 entradas do conjunto de AVALIAÇÃO FINAL:\n",
      "                                                   Text  Label\n",
      "1884  [[-0.14352316822646857, -0.5211221295450191, -...    NaN\n",
      "1885  [[-0.523949529963233, -0.38993163718082324, -1...    NaN\n",
      "1886  [[-0.5536266516613957, -0.2810512879998921, -1...    NaN\n",
      "1887  [[-0.1477796822581197, 0.03604279675452571, -0...    NaN\n",
      "1888  [[-0.016057940068671412, 0.0005216555352636244...    NaN\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Primeiras 5 entradas do conjunto de TREINO:\")\n",
    "print(df_train.head())\n",
    "\n",
    "print(\"\\n Primeiras 5 entradas do conjunto de VALIDAÇÃO:\")\n",
    "print(df_val.head())\n",
    "\n",
    "print(\"\\n Primeiras 5 entradas do conjunto de TESTE:\")\n",
    "print(df_test.head())\n",
    "\n",
    "print(\"\\n Primeiras 5 entradas do conjunto de AVALIAÇÃO FINAL:\")\n",
    "print(df_eval_final.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Construção do modelo RNN com código raiz (Sem TensorFlow/SKLearn)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inicialização de Pesos**\n",
    "\n",
    "Antes de tudo, vamos definir os pesos da rede:\n",
    "\n",
    "- W_xh: Pesa a entrada para os neurônios recorrentes.\n",
    "- W_hh: Pesa as conexões recorrentes.\n",
    "- W_hy: Pesa a saída do neurônio recorrente para a previção final.\n",
    "- b_h e b_y: Bias da camada oculta e da saída."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos e Biases inicializados!\n"
     ]
    }
   ],
   "source": [
    "# Definir hiperparâmetros\n",
    "input_size = 50    # Dimensão dos embeddings\n",
    "hidden_size = 64   # Número de neurônios na camada oculta\n",
    "output_size = 1    # Saída binária (0 ou 1)\n",
    "learning_rate = 0.01  \n",
    "\n",
    "# Inicializar pesos\n",
    "np.random.seed(42)  # Para reprodutibilidade\n",
    "W_xh = np.random.randn(input_size, hidden_size) * 0.01  # Pesos da entrada para a camada oculta\n",
    "W_hh = np.random.randn(hidden_size, hidden_size) * 0.01 # Pesos da camada oculta para ela mesma\n",
    "W_hy = np.random.randn(hidden_size, output_size) * 0.01 # Pesos da camada oculta para saída\n",
    "\n",
    "# Bias\n",
    "b_h = np.zeros((1, hidden_size))\n",
    "b_y = np.zeros((1, output_size))\n",
    "\n",
    "print(\"Pesos e Biases inicializados!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Função de Custo (Binary Cross-Entropy)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    y_pred = np.clip(y_pred, 1e-8, 1 - 1e-8)  # 🔹 Evita log(0) ou log(1)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)) / y_pred.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mini-Batches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mini_batches(X, y, batch_size=16, shuffle=True):\n",
    "    \"\"\"Divide os dados em mini-batches.\"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    indices = np.arange(n_samples)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        yield X[indices[start:end]], y[indices[start:end]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Otimização de Hiperparâmetros (Inicial)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testando hiperparâmetros: {'epochs': 10, 'batch_size': 16, 'learning_rate': 0.001, 'momentum': 0.9, 'bptt_trunc': 2}\n",
      "Época 1/10 - Loss: 3.6829\n",
      "Época 2/10 - Loss: 3.6783\n",
      "Época 3/10 - Loss: 3.6765\n",
      "Época 4/10 - Loss: 3.6730\n",
      "Época 5/10 - Loss: 3.6698\n",
      "Época 6/10 - Loss: 3.6665\n",
      "Época 7/10 - Loss: 3.6636\n",
      "Época 8/10 - Loss: 3.6614\n",
      "Época 9/10 - Loss: 3.6579\n",
      "Época 10/10 - Loss: 3.6556\n",
      "Formato de preds: (282,)\n",
      "Accuracy com esses hiperparâmetros: 0.5355\n",
      "\n",
      "Testando hiperparâmetros: {'epochs': 10, 'batch_size': 16, 'learning_rate': 0.005, 'momentum': 0.95, 'bptt_trunc': 3}\n",
      "Época 1/10 - Loss: 3.6713\n",
      "Época 2/10 - Loss: 3.6669\n",
      "Época 3/10 - Loss: 3.6627\n",
      "Época 4/10 - Loss: 3.6570\n",
      "Época 5/10 - Loss: 3.6518\n",
      "Época 6/10 - Loss: 3.6441\n",
      "Época 7/10 - Loss: 3.6336\n",
      "Época 8/10 - Loss: 3.6187\n",
      "Época 9/10 - Loss: 3.5934\n",
      "Época 10/10 - Loss: 3.5415\n",
      "Formato de preds: (282,)\n",
      "Accuracy com esses hiperparâmetros: 0.5355\n",
      "\n",
      "Testando hiperparâmetros: {'epochs': 10, 'batch_size': 16, 'learning_rate': 0.007, 'momentum': 0.8, 'bptt_trunc': 2}\n",
      "Época 1/10 - Loss: 3.6802\n",
      "Época 2/10 - Loss: 3.6725\n",
      "Época 3/10 - Loss: 3.6654\n",
      "Época 4/10 - Loss: 3.6593\n",
      "Época 5/10 - Loss: 3.6520\n",
      "Época 6/10 - Loss: 3.6436\n",
      "Época 7/10 - Loss: 3.6312\n",
      "Época 8/10 - Loss: 3.6122\n",
      "Época 9/10 - Loss: 3.5720\n",
      "Época 10/10 - Loss: 3.4750\n",
      "Formato de preds: (282,)\n",
      "Accuracy com esses hiperparâmetros: 0.5355\n",
      "\n",
      "Melhor combinação encontrada: {'epochs': 10, 'batch_size': 16, 'learning_rate': 0.001, 'momentum': 0.9, 'bptt_trunc': 2} com accuracy 0.5355\n"
     ]
    }
   ],
   "source": [
    "# Função de ativação Sigmoid\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Definir pesos corretamente (Xavier Initialization)\n",
    "W_xh = np.random.randn(input_size, hidden_size) * np.sqrt(1. / input_size)\n",
    "W_hh = np.random.randn(hidden_size, hidden_size) * np.sqrt(1. / hidden_size)\n",
    "W_hy = np.random.randn(hidden_size, output_size) * np.sqrt(1. / hidden_size)\n",
    "\n",
    "HYPERPARAMS = [\n",
    "    {\"epochs\": 10, \"batch_size\": 16, \"learning_rate\": 0.001, \"momentum\": 0.9, \"bptt_trunc\": 2},\n",
    "    {\"epochs\": 10, \"batch_size\": 16, \"learning_rate\": 0.005, \"momentum\": 0.95, \"bptt_trunc\": 3},\n",
    "    {\"epochs\": 10, \"batch_size\": 16, \"learning_rate\": 0.007, \"momentum\": 0.8, \"bptt_trunc\": 2},\n",
    "]\n",
    "\n",
    "best_accuracy = 0\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "# Testando hiperparâmetros\n",
    "for params in HYPERPARAMS:\n",
    "    print(f\"\\nTestando hiperparâmetros: {params}\")\n",
    "\n",
    "    rnn = RNN(\n",
    "        n_units=20,\n",
    "        # activation=ReLUActivation(),\n",
    "        activation=TanhActivation(),\n",
    "        bptt_trunc=params[\"bptt_trunc\"],\n",
    "        input_shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM),\n",
    "        epochs=params[\"epochs\"],\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        momentum=params[\"momentum\"],\n",
    "        loss=BinaryCrossEntropy,\n",
    "        metric=accuracy\n",
    "    )\n",
    "\n",
    "    optimizer = Optimizer(learning_rate=params[\"learning_rate\"])\n",
    "    rnn.initialize(optimizer)\n",
    "\n",
    "    for epoch in range(params[\"epochs\"]):\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in get_mini_batches(X_train, y_train, params[\"batch_size\"]):\n",
    "            y_pred = rnn.forward_propagation(X_batch)\n",
    "            y_pred_final = sigmoid(y_pred[:, -1, :])  # Aplica Sigmoid na última saída\n",
    "\n",
    "            loss = binary_cross_entropy(y_batch.reshape(-1, 1), y_pred_final)\n",
    "            grad_loss = (y_pred_final - y_batch.reshape(-1, 1)) / y_batch.shape[0]\n",
    "\n",
    "            grad_loss_expanded = np.zeros_like(y_pred)\n",
    "            grad_loss_expanded[:, -1, :] = grad_loss\n",
    "\n",
    "            rnn.backward_propagation(grad_loss_expanded)\n",
    "\n",
    "            total_loss += loss\n",
    "\n",
    "        print(f\"Época {epoch+1}/{params['epochs']} - Loss: {total_loss:.4f}\")\n",
    "\n",
    "    # Avaliação\n",
    "    preds = rnn.predict(X_val)\n",
    "    \n",
    "    # Debug do formato de `preds`\n",
    "    print(f\"Formato de preds: {preds.shape}\")\n",
    "\n",
    "    # Corrigir caso `preds` seja 1D\n",
    "    if preds.ndim == 1:\n",
    "        preds = preds[:, np.newaxis]\n",
    "\n",
    "    acc = accuracy(y_val, preds)\n",
    "\n",
    "    print(f\"Accuracy com esses hiperparâmetros: {acc:.4f}\")\n",
    "\n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        best_params = params\n",
    "        best_model = rnn\n",
    "\n",
    "print(f\"\\nMelhor combinação encontrada: {best_params} com accuracy {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Treinar o Modelo Final com melhor accuracy (obtido no passo anterior)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino final - Época 1/10 - Loss: 3.6844\n",
      "Treino final - Época 2/10 - Loss: 3.6825\n",
      "Treino final - Época 3/10 - Loss: 3.6817\n",
      "Treino final - Época 4/10 - Loss: 3.6793\n",
      "Treino final - Época 5/10 - Loss: 3.6770\n",
      "Treino final - Época 6/10 - Loss: 3.6760\n",
      "Treino final - Época 7/10 - Loss: 3.6742\n",
      "Treino final - Época 8/10 - Loss: 3.6732\n",
      "Treino final - Época 9/10 - Loss: 3.6709\n",
      "Treino final - Época 10/10 - Loss: 3.6688\n",
      "Formato de y_test_pred: (284,)\n",
      "\n",
      "Accuracy final no conjunto de teste: 0.4754\n",
      "\n",
      "Accuracy final no conjunto de teste: 0.4754\n",
      "F1 Score: 0.0000\n",
      "Recall: 0.0000\n",
      "Precision: 0.0000\n",
      "Confusion Matrix:\n",
      " [[135   0]\n",
      " [149   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Eduardo\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "final_rnn = RNN(\n",
    "    n_units=20,\n",
    "    # activation=ReLUActivation(),\n",
    "    activation=TanhActivation(),\n",
    "    bptt_trunc=best_params[\"bptt_trunc\"],\n",
    "    input_shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM),\n",
    "    epochs=best_params[\"epochs\"],\n",
    "    batch_size=best_params[\"batch_size\"],\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    momentum=best_params[\"momentum\"],\n",
    "    loss=BinaryCrossEntropy,\n",
    "    metric=accuracy\n",
    ")\n",
    "\n",
    "final_optimizer = Optimizer(learning_rate=best_params[\"learning_rate\"])\n",
    "final_rnn.initialize(final_optimizer)\n",
    "\n",
    "for epoch in range(best_params[\"epochs\"]):\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in get_mini_batches(X_train, y_train, best_params[\"batch_size\"]):\n",
    "        y_pred = final_rnn.forward_propagation(X_batch)\n",
    "        y_pred_final = sigmoid(y_pred[:, -1, :])  # Aplica Sigmoid na última saída\n",
    "\n",
    "        loss = binary_cross_entropy(y_batch.reshape(-1, 1), y_pred_final)\n",
    "\n",
    "        # Calcular o gradiente correto\n",
    "        grad_loss = (y_pred_final - y_batch.reshape(-1, 1)) / y_batch.shape[0]\n",
    "\n",
    "        # Expandir para 3 dimensões para ser compatível com a RNN\n",
    "        grad_loss_expanded = np.zeros_like(y_pred)  # (batch_size, timesteps, output_size)\n",
    "        grad_loss_expanded[:, -1, :] = grad_loss  # Apenas o último timestep recebe gradiente\n",
    "\n",
    "        # Passar o gradiente expandido\n",
    "        final_rnn.backward_propagation(grad_loss_expanded)\n",
    "\n",
    "        total_loss += loss\n",
    "\n",
    "    print(f\"Treino final - Época {epoch+1}/{best_params['epochs']} - Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Testar Modelo Final\n",
    "y_test_pred = final_rnn.predict(X_test)\n",
    "\n",
    "print(f\"Formato de y_test_pred: {y_test_pred.shape}\")  # Debug\n",
    "\n",
    "# Se for 1D, expandimos para 2D\n",
    "if y_test_pred.ndim == 1:\n",
    "    y_test_pred = y_test_pred[:, np.newaxis]\n",
    "\n",
    "# Se for 2D (batch_size, timesteps), pegamos o último timestep\n",
    "if y_test_pred.ndim == 2:\n",
    "    y_test_pred_final = y_test_pred[:, -1]  #  Sem `:` no final, pois já é 1D\n",
    "else:\n",
    "    y_test_pred_final = y_test_pred[:, -1, :]  #  Apenas se for 3D\n",
    "\n",
    "y_test_pred_labels = (y_test_pred_final > 0.5).astype(int)\n",
    "\n",
    "y_test_true = y_test.flatten()\n",
    "accuracy = np.mean(y_test_pred_labels == y_test_true)\n",
    "print(f\"\\nAccuracy final no conjunto de teste: {accuracy:.4f}\")\n",
    "\n",
    "# Garantir que y_test_true e y_test_pred_labels tenham o mesmo tamanho\n",
    "if len(y_test_true) != len(y_test_pred_labels):\n",
    "    print(f\"Warning: Tamanhos diferentes! y_test_true={len(y_test_true)}, y_test_pred_labels={len(y_test_pred_labels)}\")\n",
    "    min_size = min(len(y_test_true), len(y_test_pred_labels))\n",
    "    y_test_true = y_test_true[:min_size]\n",
    "    y_test_pred_labels = y_test_pred_labels[:min_size]\n",
    "\n",
    "# Calcular as metricas\n",
    "accuracy = np.mean(y_test_pred_labels == y_test_true)\n",
    "f1 = f1_score(y_test_true, y_test_pred_labels)\n",
    "recall = recall_score(y_test_true, y_test_pred_labels)\n",
    "precision = precision_score(y_test_true, y_test_pred_labels)\n",
    "cm = confusion_matrix(y_test_true, y_test_pred_labels)\n",
    "\n",
    "# Exibir os resultados\n",
    "print(f\"\\nAccuracy final no conjunto de teste: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Previsão para o Dataset2 (disponibilizado pelo professor)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formato de y_test_pred2: (100,)\n"
     ]
    }
   ],
   "source": [
    "# Testar Modelo Final\n",
    "y_test_pred2 = final_rnn.predict(X_eval_final)\n",
    "\n",
    "print(f\"Formato de y_test_pred2: {y_test_pred2.shape}\")  # 🛠️ Debug\n",
    "\n",
    "# Se for 1D, expandimos para 2D\n",
    "if y_test_pred2.ndim == 1:\n",
    "    y_test_pred2 = y_test_pred2[:, np.newaxis]\n",
    "\n",
    "# Se for 2D (batch_size, timesteps), pegamos o último timestep\n",
    "if y_test_pred2.ndim == 2:\n",
    "    y_test_pred_final2 = y_test_pred2[:, -1]  #  Sem `:` no final, pois já é 1D\n",
    "else:\n",
    "    y_test_pred_final2 = y_test_pred2[:, -1, :]  #  Apenas se for 3D\n",
    "\n",
    "y_test_pred_labels2 = (y_test_pred_final2 > 0.5).astype(int)\n",
    "\n",
    "######################################################################### criação do ficheiro csv com a previsão\n",
    "\n",
    "# Criar IDs para cada amostra com o formato \"D2-1\", \"D2-2\", etc.\n",
    "id_column = [f\"D2-{i}\" for i in range(1, len(y_test_pred_labels2) + 1)]\n",
    "\n",
    "# Converter labels para \"Human\" e \"AI\"\n",
    "labels = np.where(y_test_pred_labels2.flatten() == 1, \"AI\", \"Human\")\n",
    "\n",
    "# Criar DataFrame com ID e LABEL\n",
    "df_output = pd.DataFrame({\n",
    "    \"ID\": id_column,\n",
    "    \"Label\": labels\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Análise de resultados**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Treino com dataset: gpt_vs_human**\n",
    "\n",
    "- Durante o treino: 0.87 - 0.9\n",
    "\n",
    "- Para dataset1: 0.66\n",
    "\n",
    "- Para dataset2: 0.8 - 1.0\n",
    "\n",
    "- Para ai_human: 0.51\n",
    "\n",
    "**Treino com dataset: ai_human**\n",
    "\n",
    "- Durante o treino: 0.81 - 0.84\n",
    "\n",
    "- Para gpt_vs_human: 0.49"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Hypertuning com base no modelo anterior - teste com 3600 combinações diferentes**\n",
    "\n",
    "Foi feito o loop apresentado abaixo, com 3600 combinações, porém por uma questão de brevidade, estamos neste momento a rodar o código apenas com o melhor resultado obtido:\n",
    "\n",
    "**Melhor combinação encontrada: {'epochs': 5, 'batch_size': 8, 'learning_rate': 0.01, 'momentum': 0.8, 'bptt_trunc': 6} com accuracy 0.8929**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipo de accuracy antes da chamada: <class 'numpy.float64'>\n",
      "\n",
      "A testar hiperparâmetros: {'epochs': 5, 'batch_size': 8, 'learning_rate': 0.01, 'momentum': 0.8, 'bptt_trunc': 6}\n",
      "Época 1/5 - Loss: 14.3326\n",
      "Época 2/5 - Loss: 14.2567\n",
      "Época 3/5 - Loss: 14.0455\n",
      "Época 4/5 - Loss: 12.5264\n",
      "Época 5/5 - Loss: 10.3488\n",
      "Accuracy com esses hiperparâmetros: 0.8794\n",
      "\n",
      "Melhor combinação encontrada: {'epochs': 5, 'batch_size': 8, 'learning_rate': 0.01, 'momentum': 0.8, 'bptt_trunc': 6} com accuracy 0.8794\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tipo de accuracy antes da chamada: {type(accuracy)}\")\n",
    "if not callable(accuracy):  # Se não for mais uma função\n",
    "    del accuracy  # Remover a variável sobrescrita\n",
    "    from helpers.metrics import accuracy  # Reimporte \n",
    "\n",
    "\n",
    "# Apenas com os melhores hiperparâmetros calculados anteriormente\n",
    "HYPERPARAMS = [\n",
    "    {\"epochs\": ep, \"batch_size\": bs, \"learning_rate\": lr, \"momentum\": mo, \"bptt_trunc\": bt}\n",
    "    for ep in [5]\n",
    "    for bs in [8]\n",
    "    for lr in [0.01]\n",
    "    for mo in [0.8]\n",
    "    for bt in [6]\n",
    "]\n",
    "\n",
    "best_accuracy = 0\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "start_time = time.time()\n",
    "MAX_TIME = 21600 #6 horas em segundos\n",
    "\n",
    "# Teste de hiperparâmetros \n",
    "for params in HYPERPARAMS:\n",
    "    if time.time() - start_time > MAX_TIME:\n",
    "        break\n",
    "    \n",
    "    print(f\"\\nA testar hiperparâmetros: {params}\")\n",
    "    \n",
    "    rnn = RNN(\n",
    "        n_units=20,\n",
    "        activation=TanhActivation(),\n",
    "        bptt_trunc=params[\"bptt_trunc\"],\n",
    "        input_shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM),\n",
    "        epochs=params[\"epochs\"],\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        momentum=params[\"momentum\"],\n",
    "        loss=BinaryCrossEntropy,\n",
    "        metric=accuracy\n",
    "    )\n",
    "    \n",
    "    optimizer = Optimizer(learning_rate=params[\"learning_rate\"])\n",
    "    rnn.initialize(optimizer)\n",
    "    \n",
    "    for epoch in range(params[\"epochs\"]):\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in get_mini_batches(X_train, y_train, params[\"batch_size\"]):\n",
    "            y_pred = rnn.forward_propagation(X_batch)\n",
    "            y_pred_final = sigmoid(y_pred[:, -1, :])\n",
    "\n",
    "            loss = binary_cross_entropy(y_batch.reshape(-1, 1), y_pred_final)\n",
    "            grad_loss = (y_pred_final - y_batch.reshape(-1, 1)) / y_batch.shape[0]\n",
    "            \n",
    "            grad_loss_expanded = np.zeros_like(y_pred)\n",
    "            grad_loss_expanded[:, -1, :] = grad_loss\n",
    "            \n",
    "            rnn.backward_propagation(grad_loss_expanded)\n",
    "            total_loss += loss\n",
    "        \n",
    "        print(f\"Época {epoch+1}/{params['epochs']} - Loss: {total_loss:.4f}\")\n",
    "    \n",
    "    preds = rnn.predict(X_val)\n",
    "    if preds.ndim == 1:\n",
    "        preds = preds[:, np.newaxis]\n",
    "    acc_value = accuracy(y_val, preds)\n",
    "    \n",
    "    print(f\"Accuracy com esses hiperparâmetros: {acc_value:.4f}\")\n",
    "    \n",
    "    if acc_value > best_accuracy:\n",
    "        best_accuracy = acc_value\n",
    "        best_params = params\n",
    "        best_model = rnn\n",
    "\n",
    "print(f\"\\nMelhor combinação encontrada: {best_params} com accuracy {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Treino do modelo final, com os melhores hiperparâmetros**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino final - Época 1/5 - Loss: 14.2591\n",
      "Treino final - Época 2/5 - Loss: 13.2834\n",
      "Treino final - Época 3/5 - Loss: 11.1703\n",
      "Treino final - Época 4/5 - Loss: 8.4915\n",
      "Treino final - Época 5/5 - Loss: 7.4557\n",
      "Formato de y_test_pred: (284,)\n",
      "\n",
      "Accuracy final no conjunto de teste: 0.8768\n",
      "\n",
      "Comparação entre valores esperados e previstos:\n",
      "     expected_value  predicted_value_raw  predicted_value\n",
      "0               1.0                  0.0                0\n",
      "1               1.0                  1.0                1\n",
      "2               1.0                  1.0                1\n",
      "3               0.0                  0.0                0\n",
      "4               0.0                  0.0                0\n",
      "..              ...                  ...              ...\n",
      "279             0.0                  0.0                0\n",
      "280             1.0                  1.0                1\n",
      "281             1.0                  1.0                1\n",
      "282             0.0                  0.0                0\n",
      "283             0.0                  0.0                0\n",
      "\n",
      "[284 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "final_rnn = RNN(\n",
    "    n_units=20,\n",
    "    activation=TanhActivation(),\n",
    "    bptt_trunc=best_params[\"bptt_trunc\"],\n",
    "    input_shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM),\n",
    "    epochs=best_params[\"epochs\"],\n",
    "    batch_size=best_params[\"batch_size\"],\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    momentum=best_params[\"momentum\"],\n",
    "    loss=BinaryCrossEntropy,\n",
    "    metric=accuracy\n",
    ")\n",
    "\n",
    "final_optimizer = Optimizer(learning_rate=best_params[\"learning_rate\"])\n",
    "final_rnn.initialize(final_optimizer)\n",
    "\n",
    "for epoch in range(best_params[\"epochs\"]):\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in get_mini_batches(X_train, y_train, best_params[\"batch_size\"]):\n",
    "        y_pred = final_rnn.forward_propagation(X_batch)\n",
    "        y_pred_final = sigmoid(y_pred[:, -1, :])  # Aplica Sigmoid na última saída\n",
    "\n",
    "        loss = binary_cross_entropy(y_batch.reshape(-1, 1), y_pred_final)\n",
    "\n",
    "        # Calcular o gradiente correto\n",
    "        grad_loss = (y_pred_final - y_batch.reshape(-1, 1)) / y_batch.shape[0]\n",
    "\n",
    "        # Expandir para 3 dimensões para ser compatível com a RNN\n",
    "        grad_loss_expanded = np.zeros_like(y_pred)  # (batch_size, timesteps, output_size)\n",
    "        grad_loss_expanded[:, -1, :] = grad_loss  # Apenas o último timestep recebe gradiente\n",
    "\n",
    "        # Passar o gradiente expandido\n",
    "        final_rnn.backward_propagation(grad_loss_expanded)\n",
    "\n",
    "        total_loss += loss\n",
    "\n",
    "    print(f\"Treino final - Época {epoch+1}/{best_params['epochs']} - Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Testar Modelo Final\n",
    "y_test_pred = final_rnn.predict(X_test)\n",
    "\n",
    "print(f\"Formato de y_test_pred: {y_test_pred.shape}\")  # Debug\n",
    "\n",
    "# Se for 1D, expandimos para 2D\n",
    "if y_test_pred.ndim == 1:\n",
    "    y_test_pred = y_test_pred[:, np.newaxis]\n",
    "\n",
    "# Se for 2D (batch_size, timesteps), pegamos o último timestep\n",
    "if y_test_pred.ndim == 2:\n",
    "    y_test_pred_final = y_test_pred[:, -1]  #  Sem `:` no final, pois já é 1D\n",
    "else:\n",
    "    y_test_pred_final = y_test_pred[:, -1, :]  #  Apenas se for 3D\n",
    "\n",
    "y_test_pred_labels = (y_test_pred_final > 0.5).astype(int)\n",
    "\n",
    "y_test_true = y_test.flatten()\n",
    "accuracy = np.mean(y_test_pred_labels == y_test_true)\n",
    "print(f\"\\nAccuracy final no conjunto de teste: {accuracy:.4f}\")\n",
    "\n",
    "# Criar DataFrame com Expected vs Predicted\n",
    "df_results = pd.DataFrame({\n",
    "    \"expected_value\": y_test_true,\n",
    "    \"predicted_value_raw\": y_test_pred_final.flatten(),  # Valor original antes do arredondamento\n",
    "    \"predicted_value\": y_test_pred_labels.flatten()  # Valor final binário (0 ou 1)\n",
    "})\n",
    "\n",
    "# Mostrar as previsões para comparação\n",
    "print(\"\\nComparação entre valores esperados e previstos:\")\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Previsão do melhor modelo para o dataset disponibilizado pelo professor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formato de y_test_pred2: (100,)\n"
     ]
    }
   ],
   "source": [
    "# Testar Modelo Final\n",
    "y_test_pred2 = final_rnn.predict(X_eval_final)\n",
    "\n",
    "print(f\"Formato de y_test_pred2: {y_test_pred2.shape}\")  # Debug\n",
    "\n",
    "# Se for 1D, expandimos para 2D\n",
    "if y_test_pred2.ndim == 1:\n",
    "    y_test_pred2 = y_test_pred2[:, np.newaxis]\n",
    "\n",
    "# Se for 2D (batch_size, timesteps), pegamos o último timestep\n",
    "if y_test_pred2.ndim == 2:\n",
    "    y_test_pred_final2 = y_test_pred2[:, -1]  #  Sem `:` no final, pois já é 1D\n",
    "else:\n",
    "    y_test_pred_final2 = y_test_pred2[:, -1, :]  #  Apenas se for 3D\n",
    "\n",
    "y_test_pred_labels2 = (y_test_pred_final2 > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Criação do Ficheiro CSV com a previsão final para o dataset disponibilizado pelo professor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions saved to submission3_outputs_rnn_model.csv successfully!\n",
      "Accuracy: 57.00%\n"
     ]
    }
   ],
   "source": [
    "# Generate IDs for each prediction in the format D2-1, D2-2, ...\n",
    "ids = [f\"D3-{i+1}\" for i in range(len(y_test_pred_labels2))]\n",
    "\n",
    "# Map 0 to \"Human\" and 1 to \"AI\"\n",
    "labels = [\"Human\" if pred == 0 else \"AI\" for pred in y_test_pred_labels2.flatten()]\n",
    "\n",
    "# Create a DataFrame with ID and Label columns\n",
    "df_predictions = pd.DataFrame({\n",
    "    \"ID\": ids,\n",
    "    \"Label\": labels\n",
    "})\n",
    "\n",
    "# Save the predictions to a CSV file using a tab separator to match the exact format\n",
    "df_predictions.to_csv(\"classify_output_datasets/submission3_outputs_rnn_model.csv\", sep=\"\\t\", index=False)\n",
    "\n",
    "print(\"\\nPredictions saved to submission3_outputs_rnn_model.csv successfully!\")\n",
    "\n",
    "# Load the validation dataset\n",
    "df_true = pd.read_csv(\"../tarefa_1/validation_dataset/dataset3_disclosed_output.csv\", sep=\"\\t\")\n",
    "\n",
    "# Merge the datasets on the \"ID\" column, adding suffixes to distinguish the identical column names\n",
    "df_merged = pd.merge(df_true, df_predictions, on=\"ID\", suffixes=('_true', '_pred'))\n",
    "\n",
    "# Calculate the number of correct predictions by comparing the \"Label\" columns\n",
    "num_correct = (df_merged[\"Label_true\"] == df_merged[\"Label_pred\"]).sum()\n",
    "\n",
    "# Calculate the percentage of correct predictions\n",
    "accuracy_percentage = (num_correct / len(df_merged)) * 100\n",
    "\n",
    "print(f\"Accuracy: {accuracy_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Análise de resultados da melhor combinação encontrada**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Melhor combinação encontrada: {'epochs': 5, 'batch_size': 8, 'learning_rate': 0.01, 'momentum': 0.8, 'bptt_trunc': 6} com accuracy 0.8929**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Treino com dataset: gpt_vs_human**\n",
    "\n",
    "- Durante o treino: 0.87 - 0.9\n",
    "\n",
    "- Para dataset1: 0.60\n",
    "\n",
    "- Para dataset2: 0.8 - 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
