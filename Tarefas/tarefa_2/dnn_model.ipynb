{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e117c95a",
      "metadata": {},
      "source": [
        "# DNN Model Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f7f3362",
      "metadata": {},
      "source": [
        "```md\n",
        "@authors: miguelrocha and Grupo 03\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00553163",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Notebook Imports\n",
        "import numpy as np\n",
        "\n",
        "from models.dnn_model import NeuralNetwork, hyperparameter_optimization\n",
        "from helpers.dataset import Dataset\n",
        "from helpers.enums import ModelRunMode\n",
        "from helpers.layers import DenseLayer\n",
        "from helpers.activation import ReLUActivation, SigmoidActivation\n",
        "from helpers.losses import BinaryCrossEntropy\n",
        "from helpers.optimizer import Optimizer\n",
        "from helpers.metrics import accuracy, confusion_matrix, balanced_accuracy, precision_recall_f1\n",
        "from helpers.enums import ModelType\n",
        "from helpers.model import save_dnn_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc6f6268",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model run mode\n",
        "# Options: \n",
        "#   ModelRunMode.TRAIN.value            (Train the model)\n",
        "#   ModelRunMode.CLASSIFY.value         (Classify data)\n",
        "mode = ModelRunMode.TRAIN.value\n",
        "# Prefix for saving the model files\n",
        "model_prefix = \"dnn_model_1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6adb728",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters cell\n",
        "if mode == ModelRunMode.TRAIN.value:\n",
        "    # TRAIN mode: Set parameters for training\n",
        "    input_csv = \"../tarefa_1/clean_input_datasets/gpt_vs_human_data_set_inputs.csv\"     # CSV file with training inputs (ID, Text)\n",
        "    output_csv = \"../tarefa_1/clean_output_datasets/gpt_vs_human_data_set_outputs.csv\"  # CSV file with training outputs (ID, Label)\n",
        "    test_size = 0.3                                                                     # Proportion of the dataset to use as test data\n",
        "    validation_size = 0.2                                                               # Proportion of the dataset reserved as validation data (used during hyperparameter tuning)\n",
        "    verbose = True                                                                      # Verbosity level enabler\n",
        "    random_state=42                                                                     # Seed for reproducible dataset splitting\n",
        "    max_vocab_size=1000                                                                 # Maximum vocabulary size (None implies no limit)\n",
        "    min_freq=1                                                                          # Minimum frequency required for a word to be included in the vocabulary\n",
        "elif mode == ModelRunMode.CLASSIFY.value:\n",
        "    # CLASSIFY mode: Set parameters for classification\n",
        "    input_csv = \"../tarefa_2/classify_input_datasets/dataset2_inputs.csv\"               # CSV file with texts for prediction (ID, Text)\n",
        "    output_csv = \"../tarefa_2/classify_output_datasets/dataset2_outputs_dnn_model.csv\"  # CSV file to store prediction result\n",
        "else:\n",
        "    print(\"The selected option is not valid. Options: \\\"train\\\" or \\\"classify\\\"!\")\n",
        "    SystemExit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e633c6f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "if mode == ModelRunMode.TRAIN.value:\n",
        "    # Load datasets using TF-IDF vectorization\n",
        "    X_train, y_train, X_test, y_test, vocab, idf = Dataset.prepare_train_test_tfidf(input_csv=input_csv, output_csv=output_csv, test_size=test_size, random_state=random_state, max_vocab_size=max_vocab_size, min_freq=min_freq)\n",
        "\n",
        "    # Create Dataset objects for training and test data\n",
        "    train_ds_full = Dataset(X=X_train, Y=y_train)\n",
        "    test_ds = Dataset(X=X_test, Y=y_test)\n",
        "\n",
        "    # Display dimensions of the training and test datasets\n",
        "    print(f\"Train set has {train_ds_full.nrows()} rows and {train_ds_full.ncols()} columns\")\n",
        "    print(f\"Test set has {test_ds.nrows()} rows and {test_ds.ncols()} columns\\n\")\n",
        "\n",
        "    # Split the full training dataset into training and validation subsets (70% training, 30% validation)\n",
        "    n_train = train_ds_full.X.shape[0]\n",
        "    indices = np.arange(n_train)\n",
        "    np.random.shuffle(indices)\n",
        "    split_idx = int((1 - validation_size) * n_train)\n",
        "    train_idx = indices[:split_idx]\n",
        "    val_idx = indices[split_idx:]\n",
        "    train_ds = Dataset(X=train_ds_full.X[train_idx], Y=train_ds_full.Y[train_idx])\n",
        "    val_ds = Dataset(X=train_ds_full.X[val_idx], Y=train_ds_full.Y[val_idx])\n",
        "\n",
        "    # More varied epochs to handle both simpler and more complex tasks\n",
        "    epochs_list = [100, 150]\n",
        "\n",
        "    # A wider range of batch sizes\n",
        "    batch_size_list = [32, 64]\n",
        "\n",
        "    # Learning rate from very small to moderate\n",
        "    learning_rate_list = [0.001, 0.01]\n",
        "\n",
        "    # Momentum slightly varied around typical defaults\n",
        "    momentum_list = [0.8, 0.9]\n",
        "\n",
        "    # Different hidden layer topologies\n",
        "    hidden_layers_list = [\n",
        "        [64],\n",
        "        [64, 32],\n",
        "        [128, 64]\n",
        "    ]\n",
        "\n",
        "    # Add mild to moderate dropout levels\n",
        "    dropout_list = [0.0, 0.1]\n",
        "\n",
        "    # Perform hyperparameter tuning using the training and validation sets\n",
        "    print(\"Starting hyperparameter tuning...\")\n",
        "    best_params = hyperparameter_optimization(train_ds, val_ds, epochs_list, batch_size_list, learning_rate_list, momentum_list, hidden_layers_list, dropout_list, n_iter=96)\n",
        "    print(\"\\nBest hyperparameters:\", best_params)\n",
        "\n",
        "    # Retrain DNN model on the full training data using the best hyperparameters\n",
        "    final_model = NeuralNetwork(epochs=best_params['epochs'], batch_size=best_params['batch_size'], optimizer=Optimizer(learning_rate=best_params['learning_rate'], momentum=best_params['momentum']), verbose=verbose, loss=BinaryCrossEntropy,  metric=accuracy)\n",
        "\n",
        "    # Build model Layers\n",
        "    n_features = train_ds.X.shape[1]\n",
        "    for i, units in enumerate(best_params['n_hidden']):\n",
        "        if i == 0:\n",
        "            final_model.add(DenseLayer(units, (n_features,),dropout_rate=best_params['dropout_rate']))\n",
        "        else:\n",
        "            final_model.add(DenseLayer(units,dropout_rate=best_params['dropout_rate']))\n",
        "        final_model.add(ReLUActivation())\n",
        "        \n",
        "    final_model.add(DenseLayer(1))\n",
        "    final_model.add(SigmoidActivation())\n",
        "\n",
        "    # Fit model\n",
        "    final_model.fit(train_ds)\n",
        "\n",
        "    # Get predictions on the test set\n",
        "    preds = final_model.predict(test_ds)\n",
        "\n",
        "    # Convert probabilities to binary class labels using a threshold of 0.5\n",
        "    preds_binary = (preds > 0.5).astype(int)\n",
        "\n",
        "    # Evaluate the final model on the test set\n",
        "    test_acc = final_model.score(test_ds, preds)\n",
        "    print(f\"\\nTest accuracy with best hyperparameters: {test_acc:.4f}\")\n",
        "\n",
        "    # Evaluate the final model using binary predictions for the confusion matrix and other metrics\n",
        "    TP, FP, TN, FN = confusion_matrix(y_test, preds_binary)\n",
        "    prec, rec, f1 = precision_recall_f1(y_test, preds_binary)\n",
        "    bal_acc = balanced_accuracy(y_test, preds_binary)\n",
        "\n",
        "    print(\"Confusion Matrix: TP={}, FP={}, TN={}, FN={}\".format(TP, FP, TN, FN))\n",
        "    print(\"Precision = {:.4f}, Recall = {:.4f}, F1 = {:.4f}\".format(prec, rec, f1))\n",
        "    print(\"Balanced Accuracy = {:.4f}\".format(bal_acc))\n",
        "\n",
        "    # Save the model, plus vocab & idf, so classification can replicate\n",
        "    save_dnn_model(dnn=final_model, vocab=vocab, idf=idf, model_prefix=model_prefix)\n",
        "    print(f\"DNN Model saved with prefix {model_prefix}\")\n",
        "\n",
        "if mode == ModelRunMode.CLASSIFY.value:\n",
        "    # Classify new texts using the saved model\n",
        "    Dataset.classify_texts(input_csv=input_csv, output_csv=output_csv, neural_net_class=NeuralNetwork, model_type=ModelType.DNN ,model_prefix=model_prefix)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
