{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e117c95a",
      "metadata": {},
      "source": [
        "# DNN Model Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f7f3362",
      "metadata": {},
      "source": [
        "```md\n",
        "@authors: miguelrocha and Grupo 03\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "00553163",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Notebook Imports\n",
        "import numpy as np\n",
        "\n",
        "from models.dnn_model import NeuralNetwork, hyperparameter_optimization\n",
        "from helpers.dataset import Dataset\n",
        "from helpers.enums import ModelRunMode\n",
        "from helpers.layers import DenseLayer\n",
        "from helpers.activation import ReLUActivation, SigmoidActivation\n",
        "from helpers.losses import BinaryCrossEntropy\n",
        "from helpers.optimizer import Optimizer\n",
        "from helpers.metrics import accuracy\n",
        "from helpers.enums import ModelType\n",
        "from helpers.model import save_dnn_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "fc6f6268",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model run mode\n",
        "# Options: \n",
        "#   ModelRunMode.TRAIN.value            (Train the model)\n",
        "#   ModelRunMode.CLASSIFY.value         (Classify data)\n",
        "mode = ModelRunMode.TRAIN.value\n",
        "# Prefix for saving the model files\n",
        "model_prefix = \"dnn_model_1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d6adb728",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters cell\n",
        "if mode == ModelRunMode.TRAIN.value:\n",
        "    # TRAIN mode: Set parameters for training\n",
        "    input_csv = \"../tarefa_1/clean_input_datasets/dataset1_enh_inputs.csv\"              # CSV file with training inputs (ID, Text)\n",
        "    output_csv = \"../tarefa_1/clean_output_datasets/dataset1_enh_outputs.csv\"           # CSV file with training outputs (ID, Label)\n",
        "    test_size = 0.3                                                                     # Proportion of the dataset to use as test data\n",
        "    validation_size = 0.2                                                               # Proportion of the dataset reserved as validation data (used during hyperparameter tuning)\n",
        "    verbose = True                                                                      # Verbosity level enabler\n",
        "    random_state=42                                                                     # Seed for reproducible dataset splitting\n",
        "    max_vocab_size=None                                                                 # Maximum vocabulary size (None implies no limit)\n",
        "    min_freq=1                                                                          # Minimum frequency required for a word to be included in the vocabulary\n",
        "elif mode == ModelRunMode.CLASSIFY.value:\n",
        "    # CLASSIFY mode: Set parameters for classification\n",
        "    input_csv = \"../tarefa_2/classify_input_datasets/dataset2_inputs.csv\"               # CSV file with texts for prediction (ID, Text)\n",
        "    output_csv = \"../tarefa_2/classify_output_datasets/dataset2_outputs_dnn_model.csv\"  # CSV file to store prediction result\n",
        "else:\n",
        "    print(\"The selected option is not valid. Options: \\\"train\\\" or \\\"classify\\\"!\")\n",
        "    SystemExit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e633c6f6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set has 70 rows and 1928 columns\n",
            "Test set has 30 rows and 1928 columns\n",
            "\n",
            "Starting hyperparameter tuning...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Hyperparameter Search:   7%|▋         | 1/15 [00:00<00:11,  1.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameters: {'epochs': 100, 'batch_size': 16, 'learning_rate': 0.001, 'momentum': 0.9, 'n_hidden': [64, 32], 'dropout_rate': 0.1}  |  Accuracy: 0.5000\n",
            "Parameters: {'epochs': 40, 'batch_size': 16, 'learning_rate': 0.001, 'momentum': 0.85, 'n_hidden': [32], 'dropout_rate': 0.01}  |  Accuracy: 0.3571\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Hyperparameter Search:  20%|██        | 3/15 [00:01<00:05,  2.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameters: {'epochs': 100, 'batch_size': 16, 'learning_rate': 0.001, 'momentum': 0.85, 'n_hidden': [64, 32], 'dropout_rate': 0.01}  |  Accuracy: 0.6429\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Hyperparameter Search:  27%|██▋       | 4/15 [00:02<00:06,  1.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameters: {'epochs': 60, 'batch_size': 8, 'learning_rate': 0.001, 'momentum': 0.9, 'n_hidden': [64, 32], 'dropout_rate': 0.01}  |  Accuracy: 0.4286\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Hyperparameter Search:  40%|████      | 6/15 [00:03<00:05,  1.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameters: {'epochs': 40, 'batch_size': 8, 'learning_rate': 0.1, 'momentum': 0.85, 'n_hidden': [128, 64], 'dropout_rate': 0.5}  |  Accuracy: 0.7857\n",
            "Parameters: {'epochs': 60, 'batch_size': 32, 'learning_rate': 0.1, 'momentum': 0.95, 'n_hidden': [64, 32], 'dropout_rate': 0.5}  |  Accuracy: 0.5000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Hyperparameter Search:  47%|████▋     | 7/15 [00:05<00:07,  1.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameters: {'epochs': 60, 'batch_size': 8, 'learning_rate': 0.01, 'momentum': 0.95, 'n_hidden': [128, 64], 'dropout_rate': 0.0}  |  Accuracy: 0.6429\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Hyperparameter Search:  53%|█████▎    | 8/15 [00:05<00:05,  1.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameters: {'epochs': 120, 'batch_size': 32, 'learning_rate': 0.1, 'momentum': 0.95, 'n_hidden': [64, 32], 'dropout_rate': 0.0}  |  Accuracy: 0.4286\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Hyperparameter Search:  60%|██████    | 9/15 [00:06<00:04,  1.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameters: {'epochs': 120, 'batch_size': 16, 'learning_rate': 0.01, 'momentum': 0.95, 'n_hidden': [64], 'dropout_rate': 0.1}  |  Accuracy: 0.7857\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Hyperparameter Search:  67%|██████▋   | 10/15 [00:06<00:02,  1.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameters: {'epochs': 100, 'batch_size': 32, 'learning_rate': 0.01, 'momentum': 0.85, 'n_hidden': [64, 32], 'dropout_rate': 0.01}  |  Accuracy: 0.4286\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Hyperparameter Search:  73%|███████▎  | 11/15 [00:07<00:02,  1.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameters: {'epochs': 60, 'batch_size': 16, 'learning_rate': 0.001, 'momentum': 0.85, 'n_hidden': [128, 64], 'dropout_rate': 0.5}  |  Accuracy: 0.5000\n",
            "Parameters: {'epochs': 60, 'batch_size': 32, 'learning_rate': 0.001, 'momentum': 0.9, 'n_hidden': [32], 'dropout_rate': 0.1}  |  Accuracy: 0.3571\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Hyperparameter Search:  87%|████████▋ | 13/15 [00:07<00:00,  2.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameters: {'epochs': 120, 'batch_size': 8, 'learning_rate': 0.01, 'momentum': 0.95, 'n_hidden': [32], 'dropout_rate': 0.01}  |  Accuracy: 0.6429\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Hyperparameter Search: 100%|██████████| 15/15 [00:08<00:00,  1.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameters: {'epochs': 80, 'batch_size': 8, 'learning_rate': 0.001, 'momentum': 0.9, 'n_hidden': [32], 'dropout_rate': 0.0}  |  Accuracy: 0.3571\n",
            "Parameters: {'epochs': 120, 'batch_size': 32, 'learning_rate': 0.1, 'momentum': 0.85, 'n_hidden': [32], 'dropout_rate': 0.01}  |  Accuracy: 0.7143\n",
            "\n",
            "Best Hyperparameters Found: {'epochs': 40, 'batch_size': 8, 'learning_rate': 0.1, 'momentum': 0.85, 'n_hidden': [128, 64], 'dropout_rate': 0.5}\n",
            "Best Accuracy: 0.7857\n",
            "\n",
            "Best hyperparameters: {'epochs': 40, 'batch_size': 8, 'learning_rate': 0.1, 'momentum': 0.85, 'n_hidden': [128, 64], 'dropout_rate': 0.5}\n",
            "Epoch 1/40 - loss: 0.7043 - accuracy: 0.5536\n",
            "Epoch 2/40 - loss: 0.7258 - accuracy: 0.4643\n",
            "Epoch 3/40 - loss: 0.7212 - accuracy: 0.4464\n",
            "Epoch 4/40 - loss: 0.7051 - accuracy: 0.5357\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/40 - loss: 0.6548 - accuracy: 0.5714\n",
            "Epoch 6/40 - loss: 0.6968 - accuracy: 0.5179\n",
            "Epoch 7/40 - loss: 0.6961 - accuracy: 0.5179\n",
            "Epoch 8/40 - loss: 0.6862 - accuracy: 0.5714\n",
            "Epoch 9/40 - loss: 0.6721 - accuracy: 0.6071\n",
            "Epoch 10/40 - loss: 0.6136 - accuracy: 0.6964\n",
            "Epoch 11/40 - loss: 0.6353 - accuracy: 0.5893\n",
            "Epoch 12/40 - loss: 0.5686 - accuracy: 0.7321\n",
            "Epoch 13/40 - loss: 0.6062 - accuracy: 0.7500\n",
            "Epoch 14/40 - loss: 0.5910 - accuracy: 0.6786\n",
            "Epoch 15/40 - loss: 0.5809 - accuracy: 0.6786\n",
            "Epoch 16/40 - loss: 0.5920 - accuracy: 0.7500\n",
            "Epoch 17/40 - loss: 0.5283 - accuracy: 0.7143\n",
            "Epoch 18/40 - loss: 0.5272 - accuracy: 0.7321\n",
            "Epoch 19/40 - loss: 0.4624 - accuracy: 0.8036\n",
            "Epoch 20/40 - loss: 0.4511 - accuracy: 0.8036\n",
            "Epoch 21/40 - loss: 0.4551 - accuracy: 0.8036\n",
            "Epoch 22/40 - loss: 0.4094 - accuracy: 0.8393\n",
            "Epoch 23/40 - loss: 0.4399 - accuracy: 0.7679\n",
            "Epoch 24/40 - loss: 0.4718 - accuracy: 0.7321\n",
            "Epoch 25/40 - loss: 0.3974 - accuracy: 0.8393\n",
            "Epoch 26/40 - loss: 0.3125 - accuracy: 0.8571\n",
            "Epoch 27/40 - loss: 0.5084 - accuracy: 0.7321\n",
            "Epoch 28/40 - loss: 0.5496 - accuracy: 0.7500\n",
            "Epoch 29/40 - loss: 0.3420 - accuracy: 0.8571\n",
            "Epoch 30/40 - loss: 0.3049 - accuracy: 0.8929\n",
            "Epoch 31/40 - loss: 0.2959 - accuracy: 0.8571\n",
            "Epoch 32/40 - loss: 0.2789 - accuracy: 0.8750\n",
            "Epoch 33/40 - loss: 0.2614 - accuracy: 0.8929\n",
            "Epoch 34/40 - loss: 0.2481 - accuracy: 0.9107\n",
            "Epoch 35/40 - loss: 0.1721 - accuracy: 0.9643\n",
            "Epoch 36/40 - loss: 0.1729 - accuracy: 0.9107\n",
            "Epoch 37/40 - loss: 0.2209 - accuracy: 0.9286\n",
            "Epoch 38/40 - loss: 0.2017 - accuracy: 0.9107\n",
            "Epoch 39/40 - loss: 0.1273 - accuracy: 0.9643\n",
            "Epoch 40/40 - loss: 0.1283 - accuracy: 0.9643\n",
            "Test Accuracy: 0.3333333333333333\n",
            "DNN Model saved to folder 'dnn_model_weights'. JSON = 'dnn_model_weights\\dnn_model_1_architecture.json'\n",
            "Model saved with prefix dnn_model_1\n"
          ]
        }
      ],
      "source": [
        "if mode == ModelRunMode.TRAIN.value:\n",
        "    # Load datasets using TF-IDF vectorization\n",
        "    X_train, y_train, X_test, y_test, vocab, idf = Dataset.prepare_train_test_tfidf(input_csv=input_csv, output_csv=output_csv, test_size=test_size, random_state=random_state, max_vocab_size=max_vocab_size, min_freq=min_freq)\n",
        "\n",
        "    # Create Dataset objects for training and test data\n",
        "    train_ds_full = Dataset(X=X_train, Y=y_train)\n",
        "    test_ds = Dataset(X=X_test, Y=y_test)\n",
        "\n",
        "    # Display dimensions of the training and test datasets\n",
        "    print(f\"Train set has {train_ds_full.nrows()} rows and {train_ds_full.ncols()} columns\")\n",
        "    print(f\"Test set has {test_ds.nrows()} rows and {test_ds.ncols()} columns\\n\")\n",
        "\n",
        "    # Split the full training dataset into training and validation subsets (80% training, 20% validation)\n",
        "    n_train = train_ds_full.X.shape[0]\n",
        "    indices = np.arange(n_train)\n",
        "    np.random.shuffle(indices)\n",
        "    split_idx = int((1 - validation_size) * n_train)\n",
        "    train_idx = indices[:split_idx]\n",
        "    val_idx = indices[split_idx:]\n",
        "    train_ds = Dataset(X=train_ds_full.X[train_idx], Y=train_ds_full.Y[train_idx])\n",
        "    val_ds = Dataset(X=train_ds_full.X[val_idx], Y=train_ds_full.Y[val_idx])\n",
        "\n",
        "    # More varied epochs to handle both simpler and more complex tasks\n",
        "    epochs_list = [40, 60, 80, 100, 120]\n",
        "\n",
        "    # A wider range of batch sizes\n",
        "    batch_size_list = [8, 16, 32]\n",
        "\n",
        "    # Learning rate from very small to moderate\n",
        "    learning_rate_list = [0.001, 0.01, 0.1]\n",
        "\n",
        "    # Momentum slightly varied around typical defaults\n",
        "    momentum_list = [0.85, 0.9, 0.95]\n",
        "\n",
        "    # Different hidden layer topologies\n",
        "    hidden_layers_list = [\n",
        "        [32],\n",
        "        [64],\n",
        "        [64, 32],\n",
        "        [128, 64]\n",
        "    ]\n",
        "\n",
        "    # Add mild to moderate dropout levels\n",
        "    dropout_list = [0.0, 0.01, 0.1, 0.5]\n",
        "\n",
        "    # Perform hyperparameter tuning using the training and validation sets\n",
        "    print(\"Starting hyperparameter tuning...\")\n",
        "    best_params = hyperparameter_optimization(train_ds, val_ds, epochs_list, batch_size_list, learning_rate_list, momentum_list, hidden_layers_list, dropout_list, n_iter=15)\n",
        "    print(\"\\nBest hyperparameters:\", best_params)\n",
        "\n",
        "    # Retrain DNN model on the full training data using the best hyperparameters\n",
        "    final_model = NeuralNetwork(epochs=best_params['epochs'], batch_size=best_params['batch_size'], optimizer=Optimizer(learning_rate=best_params['learning_rate'], momentum=best_params['momentum']), verbose=verbose, loss=BinaryCrossEntropy,  metric=accuracy)\n",
        "\n",
        "    # Build model Layers\n",
        "    n_features = train_ds.X.shape[1]\n",
        "    for i, units in enumerate(best_params['n_hidden']):\n",
        "        if i == 0:\n",
        "            final_model.add(DenseLayer(units, (n_features,),dropout_rate=best_params['dropout_rate']))\n",
        "        else:\n",
        "            final_model.add(DenseLayer(units,dropout_rate=best_params['dropout_rate']))\n",
        "        final_model.add(ReLUActivation())\n",
        "        \n",
        "    final_model.add(DenseLayer(1))\n",
        "    final_model.add(SigmoidActivation())\n",
        "\n",
        "    # Fit model\n",
        "    final_model.fit(train_ds)\n",
        "    out = final_model.predict(test_ds)\n",
        "\n",
        "    # Evaluate the final model on the test set\n",
        "    acc = final_model.score(test_ds, out)\n",
        "    print(\"Test Accuracy:\", acc)\n",
        "\n",
        "    # Save the model, plus vocab & idf, so classification can replicate\n",
        "    save_dnn_model(dnn=final_model, vocab=vocab, idf=idf, model_prefix=model_prefix)\n",
        "    print(f\"Model saved with prefix {model_prefix}\")\n",
        "\n",
        "if mode == ModelRunMode.CLASSIFY.value:\n",
        "    # Classify new texts using the saved model\n",
        "    Dataset.classify_texts(input_csv=input_csv, output_csv=output_csv, neural_net_class=NeuralNetwork, model_type=ModelType.DNN ,model_prefix=model_prefix)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "AP",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
