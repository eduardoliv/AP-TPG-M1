{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e117c95a",
      "metadata": {},
      "source": [
        "# Logistic Regression Model Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f7f3362",
      "metadata": {},
      "source": [
        "```md\n",
        "@authors: miguelrocha and Grupo 03\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00553163",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Notebook Imports\n",
        "import numpy as np\n",
        "\n",
        "from models.logistic_regression_model import LogisticRegression, hyperparameter_tuning\n",
        "from helpers.dataset import Dataset\n",
        "from helpers.model import save_model\n",
        "from helpers.metrics import confusion_matrix, balanced_accuracy, precision_recall_f1\n",
        "from helpers.enums import ModelRunMode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc6f6268",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model run mode\n",
        "# Options: \n",
        "#   ModelRunMode.TRAIN.value            (Train the model)\n",
        "#   ModelRunMode.CLASSIFY.value         (Classify data)\n",
        "mode = ModelRunMode.TRAIN.value\n",
        "# Prefix for saving the model files\n",
        "model_prefix = \"logistical_regression_model_1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94fdaeeb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters configuration\n",
        "if mode == ModelRunMode.TRAIN.value:\n",
        "    # TRAIN mode: Set parameters for training\n",
        "    input_csv = \"../tarefa_1/clean_input_datasets/gpt_vs_human_data_set_inputs.csv\"     # CSV file with training inputs (ID, Text)\n",
        "    output_csv = \"../tarefa_1/clean_output_datasets/gpt_vs_human_data_set_outputs.csv\"  # CSV file with training outputs (ID, Label)\n",
        "    test_size = 0.3                                                                     # Proportion of the dataset reserved as test data\n",
        "    validation_size = 0.2                                                               # Proportion of the dataset reserved as validation data (used during hyperparameter tuning)\n",
        "    regularization = True                                                               # Flag to enable L2 regularization during training\n",
        "    random_state=42                                                                     # Seed for reproducible dataset splitting\n",
        "    max_vocab_size=None                                                                 # Maximum vocabulary size (None implies no limit)\n",
        "    min_freq=1                                                                          # Minimum frequency required for a word to be included in the vocabulary\n",
        "elif mode == ModelRunMode.CLASSIFY.value:\n",
        "    # CLASSIFY mode: Set parameters for classification\n",
        "    input_csv = \"../tarefa_2/classify_input_datasets/dataset2_inputs.csv\"               # CSV file with texts for prediction (ID, Text)\n",
        "    output_csv = \"../tarefa_2/classify_output_datasets/dataset2_outputs_lr_model.csv\"   # CSV file to store prediction results\n",
        "else:\n",
        "    # Handle invalid mode selection\n",
        "    print(\"The selected option is not valid. Options: \\\"train\\\" or \\\"classify\\\"!\")\n",
        "    SystemExit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6de2fdb7",
      "metadata": {},
      "outputs": [],
      "source": [
        "if mode == ModelRunMode.TRAIN.value:\n",
        "    # Load datasets using TF-IDF vectorization\n",
        "    X_train, y_train, X_test, y_test, vocab, idf = Dataset.prepare_train_test_tfidf(input_csv=input_csv, output_csv=output_csv, test_size=test_size, random_state=random_state, max_vocab_size=max_vocab_size, min_freq=min_freq)\n",
        "\n",
        "    # Create Dataset objects for training and test data\n",
        "    train_ds_full = Dataset(X=X_train, Y=y_train)\n",
        "    test_ds = Dataset(X=X_test, Y=y_test)\n",
        "\n",
        "    # Display dimensions of the training and test datasets\n",
        "    print(f\"Train set has {train_ds_full.nrows()} rows and {train_ds_full.ncols()} columns\")\n",
        "    print(f\"Test set has {test_ds.nrows()} rows and {test_ds.ncols()} columns\\n\")\n",
        "\n",
        "    # Split the full training dataset into training and validation subsets (80% training, 20% validation)\n",
        "    n_train = train_ds_full.X.shape[0]\n",
        "    indices = np.arange(n_train)\n",
        "    np.random.shuffle(indices)\n",
        "    split_idx = int((1 - validation_size) * n_train)\n",
        "    train_idx = indices[:split_idx]\n",
        "    val_idx = indices[split_idx:]\n",
        "    train_ds = Dataset(X=train_ds_full.X[train_idx], Y=train_ds_full.Y[train_idx])\n",
        "    val_ds = Dataset(X=train_ds_full.X[val_idx], Y=train_ds_full.Y[val_idx])\n",
        "\n",
        "    # Define hyperparameter grids for tuning\n",
        "    alphas = [0.0001, 0.001, 0.01, 0.1]\n",
        "    lambdas = [0.01, 0.1, 0.5, 1.0, 1.5]\n",
        "    iters_list = [10000, 20000, 40000, 60000, 80000, 100000]\n",
        "\n",
        "    # Perform hyperparameter tuning using the training and validation sets\n",
        "    print(\"Starting hyperparameter tuning...\")\n",
        "    best_params, best_acc, results = hyperparameter_tuning(train_ds, val_ds, alphas, lambdas, iters_list)\n",
        "    print(\"\\nBest hyperparameters:\", best_params)\n",
        "    print(\"Best validation accuracy:\", best_acc)\n",
        "\n",
        "    # Retrain logistic regression model on the full training data using the best hyperparameters\n",
        "    final_model = LogisticRegression(train_ds_full, regularization=(best_params[\"lamda\"] > 0), lamda=best_params[\"lamda\"])\n",
        "    final_model.gradientDescent(alpha=best_params[\"alpha\"], iters=best_params[\"iters\"])\n",
        "\n",
        "    # Alternative model training using cost function-based approach\n",
        "    # final_model = LogisticRegression(train_ds_full, regularization=True, lamda=0.01)\n",
        "    # final_model.buildModel()  \n",
        "\n",
        "    # Save the trained model parameters along with the vocabulary and IDF values\n",
        "    save_model(final_model.theta, vocab, idf, model_prefix)\n",
        "    print(f\"Model saved with prefix {model_prefix}\")\n",
        "\n",
        "    # Evaluate the final model on the test set\n",
        "    ones_test = np.ones((test_ds.X.shape[0], 1))\n",
        "    X_test_bias = np.hstack((ones_test, test_ds.X))\n",
        "    test_acc = final_model.accuracy(X_test_bias, test_ds.Y)\n",
        "    print(f\"\\nTest accuracy with best hyperparameters: {test_acc:.4f}\")\n",
        "\n",
        "    preds = final_model.predictMany(X_test_bias)\n",
        "    TP, FP, TN, FN = confusion_matrix(y_test, preds)\n",
        "    prec, rec, f1 = precision_recall_f1(y_test, preds)\n",
        "    bal_acc = balanced_accuracy(y_test, preds)\n",
        "\n",
        "    print(\"Confusion Matrix: TP={}, FP={}, TN={}, FN={}\".format(TP, FP, TN, FN))\n",
        "    print(\"Precision = {:.4f}, Recall = {:.4f}, F1 = {:.4f}\".format(prec, rec, f1))\n",
        "    print(\"Balanced Accuracy = {:.4f}\".format(bal_acc))\n",
        "    final_model.plotModel()\n",
        "\n",
        "if mode == ModelRunMode.CLASSIFY.value:\n",
        "    # Classify new texts using the saved model\n",
        "    Dataset.classify_texts(input_csv, output_csv, model_prefix=model_prefix)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "AP",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
