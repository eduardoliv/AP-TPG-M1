{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RNN Model Notebook**\n",
    "\n",
    "@authors: miguelrocha and Grupo 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "from helpers.dataset import Dataset\n",
    "from helpers.activation import TanhActivation\n",
    "from helpers.losses import BinaryCrossEntropy\n",
    "from helpers.metrics import accuracy\n",
    "from helpers.activation import ReLUActivation\n",
    "from models.rnn_model import RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modifica√ß√£o na classe Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
    "        self.velocity = {}  # Dicion√°rio para armazenar velocidades dos gradientes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "\n",
    "    def update(self, param, grad):\n",
    "        \"\"\"Atualiza os pesos usando Gradient Descent com Momentum\"\"\"\n",
    "\n",
    "        param_id = id(param)  # üîπ Usar ID √∫nico do numpy array\n",
    "\n",
    "        if param_id not in self.velocity:\n",
    "            self.velocity[param_id] = np.zeros_like(grad)\n",
    "\n",
    "        # Atualiza√ß√£o com momentum\n",
    "        self.velocity[param_id] = self.momentum * self.velocity[param_id] + (1 - self.momentum) * grad\n",
    "        return param - self.learning_rate * self.velocity[param_id]  # üîπ Retorna os novos pesos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tratamento de Dados**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**An√°lise Inicial dos Datasets e Jun√ß√£o dos mesmos para tratamento simult√¢neo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Completo - Primeiras 5 linhas:\n",
      "  ID                                               Text  Label\n",
      "0  0  Advanced electromagnetic potentials are indige...  Human\n",
      "1  1  This research paper investigates the question ...     AI\n",
      "2  2  We give an algorithm for finding network encod...  Human\n",
      "3  3  The paper presents an efficient centralized bi...     AI\n",
      "4  4  We introduce an exponential random graph model...  Human\n",
      "\n",
      "Dataset Completo - Ultimas 5 linhas:\n",
      "          ID                                               Text Label\n",
      "4148   D2-96  Though a part of the continent of North Americ...   Nap\n",
      "4149   D2-97  There has been a steady increase in the number...   Nap\n",
      "4150   D2-98  Plasticizers like phthalates were thought to b...   Nap\n",
      "4151   D2-99  The main causes of lung cancer are multifacete...   Nap\n",
      "4152  D2-100  It is an approximation useful in chemistry, bu...   Nap\n"
     ]
    }
   ],
   "source": [
    "# Definir os caminhos dos arquivos de TREINO\n",
    "input_csv1 = \"../tarefa_1/clean_input_datasets/gpt_vs_human_data_set_inputs.csv\"\n",
    "output_csv1 = \"../tarefa_1/clean_output_datasets/gpt_vs_human_data_set_outputs.csv\"\n",
    "\n",
    "# Definir os caminhos dos arquivos de TESTE FINAL\n",
    "input_csv2 = \"classify_input_datasets/dataset2_inputs.csv\"\n",
    "output_csv2 = \"../tarefa_1/clean_output_datasets/dataset2_layout_outputs.csv\" # dataset apenas utilizado para adicionar o layout ID Label\n",
    " \n",
    "# Carregar os datasets de treino\n",
    "df_input1 = pd.read_csv(input_csv1, sep=\"\\t\")  \n",
    "df_output1 = pd.read_csv(output_csv1, sep=\"\\t\")\n",
    "\n",
    "# Carregar os datasets de teste\n",
    "df_input2 = pd.read_csv(input_csv2, sep=\"\\t\")\n",
    "df_output2 = pd.read_csv(output_csv2, sep=\"\\t\")\n",
    "\n",
    "# Jun√ß√£o com coluna ID\n",
    "df_train = pd.merge(df_input1, df_output1, on=\"ID\")\n",
    "df_test = pd.merge(df_input2, df_output2, on=\"ID\")\n",
    "\n",
    "# Concatenar treino e teste para aplicar as altera√ß√µes simultaneamente\n",
    "df_dataset1_merged = pd.concat([df_train, df_test], ignore_index=True)\n",
    "\n",
    "# Mostrar as primeiras 5 linhas do dataset completo\n",
    "print(\"\\nDataset Completo - Primeiras 5 linhas:\")\n",
    "print(df_dataset1_merged.head())\n",
    "\n",
    "print(\"\\nDataset Completo - Ultimas 5 linhas:\")\n",
    "print(df_dataset1_merged.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remover caracteres especiais e pontua√ß√£o e Converter em min√∫sculas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto limpo - primeiras 5 linhas:\n",
      "  ID                                               Text  Label\n",
      "0  0  advanced electromagnetic potentials are indige...  Human\n",
      "1  1  this research paper investigates the question ...     AI\n",
      "2  2  we give an algorithm for finding network encod...  Human\n",
      "3  3  the paper presents an efficient centralized bi...     AI\n",
      "4  4  we introduce an exponential random graph model...  Human\n"
     ]
    }
   ],
   "source": [
    "# Fun√ß√£o para limpar texto\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Converter para min√∫sculas\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Remover pontua√ß√£o\n",
    "    return text\n",
    "\n",
    "df_dataset1_merged[\"clean_text\"] = df_dataset1_merged[\"Text\"].apply(clean_text)\n",
    "\n",
    "# Manter apenas as colunas desejadas e renomear clean_text para Text\n",
    "df_dataset1_merged = df_dataset1_merged[[\"ID\", \"clean_text\", \"Label\"]].rename(columns={\"clean_text\": \"Text\"})\n",
    "\n",
    "print(\"Texto limpo - primeiras 5 linhas:\")\n",
    "print(df_dataset1_merged.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remover stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto sem stopwords - primeiras 5 linhas:\n",
      "  ID                                               Text  Label\n",
      "0  0  advanced electromagnetic potentials indigenous...  Human\n",
      "1  1  research paper investigates question whether a...     AI\n",
      "2  2  we give algorithm finding network encoding dec...  Human\n",
      "3  3  paper presents efficient centralized binary mu...     AI\n",
      "4  4  we introduce exponential random graph model ne...  Human\n"
     ]
    }
   ],
   "source": [
    "# Lista de stopwords comuns\n",
    "stopwords = {\n",
    "    \"the\", \"of\", \"and\", \"in\", \"to\", \"is\", \"a\", \"that\", \"for\", \"are\", \"on\", \"with\", \n",
    "    \"as\", \"at\", \"by\", \"from\", \"this\", \"it\", \"an\", \"be\", \"or\", \"which\", \"was\", \"were\"\n",
    "}\n",
    "\n",
    "# Fun√ß√£o para remover stopwords\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()  # Dividir em palavras\n",
    "    filtered_words = [word for word in words if word not in stopwords]  # Remover stopwords\n",
    "    return \" \".join(filtered_words)  # Juntar as palavras de novo\n",
    "\n",
    "# Aplicar ao dataset\n",
    "df_dataset1_merged[\"Text\"] = df_dataset1_merged[\"Text\"].apply(remove_stopwords)\n",
    "\n",
    "# Exibir as primeiras 5 linhas ap√≥s remo√ß√£o de stopwords\n",
    "print(\"Texto sem stopwords - primeiras 5 linhas:\")\n",
    "print(df_dataset1_merged.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Criar Embeddings e Label Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de palavras carregadas do GloVe: 400000\n"
     ]
    }
   ],
   "source": [
    "# Mapear labels para valores num√©ricos\n",
    "label_map = {\"Human\": 0, \"AI\": 1}\n",
    "df_dataset1_merged[\"Label\"] = df_dataset1_merged[\"Label\"].map(label_map)\n",
    "\n",
    "# Carregar o GloVe\n",
    "EMBEDDING_DIM = 50  # Dimens√£o do embedding\n",
    "\n",
    "# Diret√≥rio e nome do ficheiro GloVe\n",
    "glove_dir = \"helpers\"\n",
    "glove_filename = \"glove.6B.50d.txt\"\n",
    "glove_zip_url = \"http://nlp.stanford.edu/data/glove.6B.zip\"  # URL do GloVe oficial\n",
    "\n",
    "# Criar diret√≥rio se n√£o existir\n",
    "os.makedirs(glove_dir, exist_ok=True)\n",
    "\n",
    "# Caminho completo do ficheiro\n",
    "glove_path = os.path.join(glove_dir, glove_filename)\n",
    "glove_zip_path = os.path.join(glove_dir, \"glove.6B.zip\")\n",
    "\n",
    "# Verificar se o ficheiro j√° existe\n",
    "if not os.path.exists(glove_path):\n",
    "    print(\"Ficheiro GloVe n√£o encontrado. A fazer download...\")\n",
    "\n",
    "    # Download do ficheiro ZIP do GloVe\n",
    "    response = requests.get(glove_zip_url, stream=True)\n",
    "    with open(glove_zip_path, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    \n",
    "    print(\"Download conclu√≠do. A extrair ficheiros...\")\n",
    "\n",
    "    # Extrair apenas o ficheiro necess√°rio\n",
    "    with zipfile.ZipFile(glove_zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extract(glove_filename, path=glove_dir)\n",
    "\n",
    "    print(\"Extra√ß√£o conclu√≠da!\")\n",
    "\n",
    "# Agora podemos carregar o GloVe\n",
    "embedding_dict = {}\n",
    "with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.array(values[1:], dtype=\"float32\")\n",
    "        embedding_dict[word] = vector\n",
    "\n",
    "print(f\"Total de palavras carregadas do GloVe: {len(embedding_dict)}\")\n",
    "\n",
    "# Converter palavras para embeddings\n",
    "def text_to_embedding(text, embedding_dict, embedding_dim=50):\n",
    "    words = text.split()\n",
    "    embeddings = [embedding_dict.get(word, np.zeros(embedding_dim)) for word in words]  # Usa vetor do GloVe ou vetor zerado\n",
    "    \n",
    "    # Se a lista estiver vazia, retorna um vetor de zeros\n",
    "    if len(embeddings) == 0:\n",
    "        embeddings = [np.zeros(embedding_dim)]\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "df_dataset1_merged[\"Embedding\"] = df_dataset1_merged[\"Text\"].apply(lambda x: text_to_embedding(x, embedding_dict, EMBEDDING_DIM))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padronizar o comprimento das sequ√™ncias**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formato final dos dados para o modelo: (4153, 130, 50)\n",
      "Dataset ap√≥s embedding - primeiras 5 linhas:\n",
      "  ID                                               Text  Label\n",
      "0  0  [[-0.3009699881076813, -0.11428999900817871, 0...    0.0\n",
      "1  1  [[0.7125800251960754, 0.6449199914932251, 0.05...    1.0\n",
      "2  2  [[0.5738700032234192, -0.32728999853134155, 0....    0.0\n",
      "3  3  [[-0.7121599912643433, 0.028648000210523605, 0...    1.0\n",
      "4  4  [[0.5738700032234192, -0.32728999853134155, 0....    0.0\n"
     ]
    }
   ],
   "source": [
    "# Padronizar comprimento das sequ√™ncias\n",
    "MAX_SEQUENCE_LENGTH = 130  # foram testados v√°rios valores sendo o melhor 130\n",
    "\n",
    "def pad_embedding_sequence(seq, max_length, embedding_dim):\n",
    "    seq = np.array(seq)  # Garante que a sequ√™ncia √© um array NumPy\n",
    "    \n",
    "    if seq.shape[0] == 0:  # Se for uma sequ√™ncia vazia, criar um array de zeros\n",
    "        seq = np.zeros((1, embedding_dim))\n",
    "\n",
    "    if seq.shape[0] > max_length:  # Truncar se for maior\n",
    "        return seq[:max_length]\n",
    "    \n",
    "    padding = np.zeros((max_length - seq.shape[0], embedding_dim))  # Criar padding\n",
    "    return np.vstack([seq, padding])  # Adicionar padding no final\n",
    "\n",
    "# Aplicar padding √†s sequ√™ncias de embeddings\n",
    "df_dataset1_merged[\"Embedding\"] = df_dataset1_merged[\"Embedding\"].apply(\n",
    "    lambda x: pad_embedding_sequence(x, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)\n",
    ")\n",
    "\n",
    "# Converter para array NumPy para alimentar o modelo\n",
    "X = np.array(df_dataset1_merged[\"Embedding\"].tolist())\n",
    "y = np.array(df_dataset1_merged[\"Label\"])  # Labels num√©ricos\n",
    "\n",
    "print(\"Formato final dos dados para o modelo:\", X.shape)  # Deve ser (n_amostras, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)\n",
    "\n",
    "# Manter apenas as colunas desejadas e renomear \"Embedding\" para \"Text\"\n",
    "df_dataset1_merged = df_dataset1_merged[[\"ID\", \"Embedding\", \"Label\"]].rename(columns={\"Embedding\": \"Text\"})\n",
    "\n",
    "print(\"Dataset ap√≥s embedding - primeiras 5 linhas:\")\n",
    "print(df_dataset1_merged.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normaliza√ß√£o dos Embeddings**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formato final dos dados para o modelo: (4153, 130, 50)\n",
      "\n",
      "Dataset ap√≥s normaliza√ß√£o dos embeddings:\n",
      "  ID                                               Text  Label\n",
      "0  0  [[-1.3483198034668138, -0.22574247577337878, 1...    0.0\n",
      "1  1  [[1.4213576609868754, 1.7255806047138122, 0.11...    1.0\n",
      "2  2  [[0.905188811450096, -1.1959995997633175, -0.0...    0.0\n",
      "3  3  [[-2.165679718307827, 0.18190477259827562, -0....    1.0\n",
      "4  4  [[0.7698617016091831, -1.0859811428972335, -0....    0.0\n"
     ]
    }
   ],
   "source": [
    "# Fun√ß√£o para normalizar cada embedding (zero mean, unit variance)\n",
    "def normalize_embedding(emb):\n",
    "    mean = np.mean(emb, axis=0)  # M√©dia por dimens√£o do embedding\n",
    "    std = np.std(emb, axis=0) + 1e-8  # Desvio padr√£o (evita divis√£o por zero)\n",
    "    return (emb - mean) / std\n",
    "\n",
    "# Aplicar normaliza√ß√£o alternativa aos embeddings\n",
    "df_dataset1_merged[\"Text\"] = df_dataset1_merged[\"Text\"].apply(normalize_embedding)\n",
    "\n",
    "# Converter para array NumPy para treinar o modelo\n",
    "X = np.array(df_dataset1_merged[\"Text\"].tolist())\n",
    "y = np.array(df_dataset1_merged[\"Label\"])  # Labels num√©ricos\n",
    "\n",
    "print(\"Formato final dos dados para o modelo:\", X.shape)  # Deve ser (n_amostras, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)\n",
    "\n",
    "# Print do dataset atualizado\n",
    "print(\"\\nDataset ap√≥s normaliza√ß√£o dos embeddings:\")\n",
    "print(df_dataset1_merged.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop da coluna ID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formato final dos dados para o modelo: (4153, 130, 50)\n",
      "\n",
      "Dataset ap√≥s drop:\n",
      "                                                Text  Label\n",
      "0  [[-1.3483198034668138, -0.22574247577337878, 1...    0.0\n",
      "1  [[1.4213576609868754, 1.7255806047138122, 0.11...    1.0\n",
      "2  [[0.905188811450096, -1.1959995997633175, -0.0...    0.0\n",
      "3  [[-2.165679718307827, 0.18190477259827562, -0....    1.0\n",
      "4  [[0.7698617016091831, -1.0859811428972335, -0....    0.0\n"
     ]
    }
   ],
   "source": [
    "if \"ID\" in df_dataset1_merged.columns:\n",
    "    df_dataset1_merged = df_dataset1_merged.drop(columns=[\"ID\"])\n",
    "\n",
    "print(\"Formato final dos dados para o modelo:\", X.shape)  # Deve ser (n_amostras, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)\n",
    "\n",
    "# Print do dataset atualizado\n",
    "print(\"\\nDataset ap√≥s drop:\")\n",
    "print(df_dataset1_merged.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Divis√£o do Dataset**\n",
    "\n",
    "Dataset de Treino:\n",
    "\n",
    "- 70% : Treino\n",
    "- 15% : Valida√ß√£o\n",
    "- 15% : Teste\n",
    "\n",
    "Dataset de Avalia√ß√£o:\n",
    "\n",
    "- 100% : Teste Final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do conjunto de treino: (2837, 2)\n",
      "Tamanho do conjunto de valida√ß√£o: (607, 2)\n",
      "Tamanho do conjunto de teste: (609, 2)\n",
      "Tamanho do conjunto de avalia√ß√£o final: (100, 2)\n",
      "Formato dos dados:\n",
      "   Treino: (2837, 130, 50)\n",
      "   Valida√ß√£o: (607, 130, 50)\n",
      "   Teste: (609, 130, 50)\n",
      "   Avalia√ß√£o final: (100, 130, 50)\n"
     ]
    }
   ],
   "source": [
    "# Definir seed global para garantir reprodutibilidade\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "######################################################### dataset de teste\n",
    "# Separar as √∫ltimas linhas para avalia√ß√£o final\n",
    "df_eval_final = df_dataset1_merged.tail(100)\n",
    "\n",
    "# Remover essas linhas do dataset antes de embaralhar\n",
    "df_remaining = df_dataset1_merged.iloc[:-100]\n",
    "#########################################################\n",
    "\n",
    "# Embaralhar o dataset restante\n",
    "df_remaining = df_remaining.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "# Definir propor√ß√µes de treino (70%), valida√ß√£o (15%) e teste (15%)\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15  # 15% valida√ß√£o\n",
    "test_ratio = 0.15  # 15% teste\n",
    "\n",
    "# Definir √≠ndices para divis√£o\n",
    "train_index = int(len(df_remaining) * train_ratio)\n",
    "val_index = train_index + int(len(df_remaining) * val_ratio)\n",
    "\n",
    "# Separar os conjuntos de treino, valida√ß√£o e teste\n",
    "df_train = df_remaining.iloc[:train_index]\n",
    "df_val = df_remaining.iloc[train_index:val_index]\n",
    "df_test = df_remaining.iloc[val_index:]\n",
    "\n",
    "# Print dos tamanhos dos datasets\n",
    "print(f\"Tamanho do conjunto de treino: {df_train.shape}\")\n",
    "print(f\"Tamanho do conjunto de valida√ß√£o: {df_val.shape}\")\n",
    "print(f\"Tamanho do conjunto de teste: {df_test.shape}\")\n",
    "print(f\"Tamanho do conjunto de avalia√ß√£o final: {df_eval_final.shape}\")\n",
    "\n",
    "# Converter para arrays NumPy\n",
    "X_train, y_train = np.array(df_train[\"Text\"].tolist()), np.array(df_train[\"Label\"])\n",
    "X_val, y_val = np.array(df_val[\"Text\"].tolist()), np.array(df_val[\"Label\"])\n",
    "X_test, y_test = np.array(df_test[\"Text\"].tolist()), np.array(df_test[\"Label\"])\n",
    "X_eval_final, y_eval_final = np.array(df_eval_final[\"Text\"].tolist()), np.array(df_eval_final[\"Label\"])\n",
    "\n",
    "# Print dos formatos dos dados\n",
    "print(f\"Formato dos dados:\")\n",
    "print(f\"   Treino: {X_train.shape}\")\n",
    "print(f\"   Valida√ß√£o: {X_val.shape}\")\n",
    "print(f\"   Teste: {X_test.shape}\")\n",
    "print(f\"   Avalia√ß√£o final: {X_eval_final.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verifica√ß√£o Final do Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Primeiras 5 entradas do conjunto de TREINO:\n",
      "                                                Text  Label\n",
      "0  [[-0.6009678327132941, 0.12333267828806424, -0...    0.0\n",
      "1  [[0.9114800543113507, 1.4439433160725084, 0.09...    1.0\n",
      "2  [[1.3506935653698395, 1.6531935857404196, 0.29...    1.0\n",
      "3  [[1.310270918702656, 1.5259911587058368, 0.093...    1.0\n",
      "4  [[1.8283282955354128, 1.7640411958771838, 0.24...    1.0\n",
      "\n",
      " Primeiras 5 entradas do conjunto de VALIDA√á√ÉO:\n",
      "                                                   Text  Label\n",
      "2837  [[-1.9058246305264064, 1.747933859364225, 2.07...    0.0\n",
      "2838  [[1.068762991628599, 1.1935346455879021, 0.110...    1.0\n",
      "2839  [[-1.9646222319222544, -0.06567262018296684, 0...    0.0\n",
      "2840  [[1.0562077640374348, -1.1135655477551067, -0....    0.0\n",
      "2841  [[-1.6292355387267745, 0.9655902363131101, -1....    0.0\n",
      "\n",
      " Primeiras 5 entradas do conjunto de TESTE:\n",
      "                                                   Text  Label\n",
      "3444  [[1.2944574828066835, 1.107379356904149, 0.373...    1.0\n",
      "3445  [[0.983986233914766, -1.4472521990699847, 0.04...    0.0\n",
      "3446  [[1.564158564538378, 1.7268175712393599, 0.139...    1.0\n",
      "3447  [[0.17952199896128274, -0.04025626568971504, -...    0.0\n",
      "3448  [[1.2383829175184078, 1.8337278956451306, 0.08...    1.0\n",
      "\n",
      " Primeiras 5 entradas do conjunto de AVALIA√á√ÉO FINAL:\n",
      "                                                   Text  Label\n",
      "4053  [[1.706592867258123, 1.8928156584848357, 2.640...    NaN\n",
      "4054  [[-0.8597012381223492, -0.434393563298629, -0....    NaN\n",
      "4055  [[1.9381903379259318, -0.6941733552298998, -0....    NaN\n",
      "4056  [[1.06775060696425, -0.26751421964373573, -0.3...    NaN\n",
      "4057  [[0.9991870982963836, 0.5145215543077415, -0.3...    NaN\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Primeiras 5 entradas do conjunto de TREINO:\")\n",
    "print(df_train.head())\n",
    "\n",
    "print(\"\\n Primeiras 5 entradas do conjunto de VALIDA√á√ÉO:\")\n",
    "print(df_val.head())\n",
    "\n",
    "print(\"\\n Primeiras 5 entradas do conjunto de TESTE:\")\n",
    "print(df_test.head())\n",
    "\n",
    "print(\"\\n Primeiras 5 entradas do conjunto de AVALIA√á√ÉO FINAL:\")\n",
    "print(df_eval_final.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Constru√ß√£o do modelo RNN com c√≥digo raiz (Sem TensorFlow/SKLearn)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inicializa√ß√£o de Pesos**\n",
    "\n",
    "Antes de tudo, vamos definir os pesos da rede:\n",
    "\n",
    "- W_xh: Pesa a entrada para os neur√¥nios recorrentes.\n",
    "- W_hh: Pesa as conex√µes recorrentes.\n",
    "- W_hy: Pesa a sa√≠da do neur√¥nio recorrente para a previ√ß√£o final.\n",
    "- b_h e b_y: Bias da camada oculta e da sa√≠da."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos e Biases inicializados!\n"
     ]
    }
   ],
   "source": [
    "# Definir hiperpar√¢metros\n",
    "input_size = 50    # Dimens√£o dos embeddings\n",
    "hidden_size = 64   # N√∫mero de neur√¥nios na camada oculta\n",
    "output_size = 1    # Sa√≠da bin√°ria (0 ou 1)\n",
    "learning_rate = 0.01  \n",
    "\n",
    "# Inicializar pesos\n",
    "np.random.seed(42)  # Para reprodutibilidade\n",
    "W_xh = np.random.randn(input_size, hidden_size) * 0.01  # Pesos da entrada para a camada oculta\n",
    "W_hh = np.random.randn(hidden_size, hidden_size) * 0.01 # Pesos da camada oculta para ela mesma\n",
    "W_hy = np.random.randn(hidden_size, output_size) * 0.01 # Pesos da camada oculta para sa√≠da\n",
    "\n",
    "# Bias\n",
    "b_h = np.zeros((1, hidden_size))\n",
    "b_y = np.zeros((1, output_size))\n",
    "\n",
    "print(\"Pesos e Biases inicializados!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fun√ß√£o de Custo (Binary Cross-Entropy)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    y_pred = np.clip(y_pred, 1e-8, 1 - 1e-8)  # üîπ Evita log(0) ou log(1)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)) / y_pred.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mini-Batches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mini_batches(X, y, batch_size=16, shuffle=True):\n",
    "    \"\"\"Divide os dados em mini-batches.\"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    indices = np.arange(n_samples)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        yield X[indices[start:end]], y[indices[start:end]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Otimiza√ß√£o de Hiperpar√¢metros (Inicial)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testando hiperpar√¢metros: {'epochs': 5, 'batch_size': 8, 'learning_rate': 0.01, 'momentum': 0.9, 'bptt_trunc': 2}\n",
      "√âpoca 1/5 - Loss: 29.9907\n",
      "√âpoca 2/5 - Loss: 24.1640\n",
      "√âpoca 3/5 - Loss: 17.6788\n",
      "√âpoca 4/5 - Loss: 14.7431\n",
      "√âpoca 5/5 - Loss: 13.9409\n",
      "Formato de preds: (607,)\n",
      "Accuracy com esses hiperpar√¢metros: 0.8847\n",
      "\n",
      "Testando hiperpar√¢metros: {'epochs': 10, 'batch_size': 16, 'learning_rate': 0.005, 'momentum': 0.95, 'bptt_trunc': 3}\n",
      "√âpoca 1/10 - Loss: 7.8056\n",
      "√âpoca 2/10 - Loss: 7.7696\n",
      "√âpoca 3/10 - Loss: 7.6840\n",
      "√âpoca 4/10 - Loss: 7.2385\n",
      "√âpoca 5/10 - Loss: 6.9034\n",
      "√âpoca 6/10 - Loss: 6.4309\n",
      "√âpoca 7/10 - Loss: 5.7269\n",
      "√âpoca 8/10 - Loss: 5.2162\n",
      "√âpoca 9/10 - Loss: 4.6802\n",
      "√âpoca 10/10 - Loss: 4.3452\n",
      "Formato de preds: (607,)\n",
      "Accuracy com esses hiperpar√¢metros: 0.8633\n",
      "\n",
      "Testando hiperpar√¢metros: {'epochs': 7, 'batch_size': 8, 'learning_rate': 0.007, 'momentum': 0.8, 'bptt_trunc': 2}\n",
      "√âpoca 1/7 - Loss: 30.7896\n",
      "√âpoca 2/7 - Loss: 29.8365\n",
      "√âpoca 3/7 - Loss: 25.6312\n",
      "√âpoca 4/7 - Loss: 19.6351\n",
      "√âpoca 5/7 - Loss: 16.2401\n",
      "√âpoca 6/7 - Loss: 14.7619\n",
      "√âpoca 7/7 - Loss: 14.1463\n",
      "Formato de preds: (607,)\n",
      "Accuracy com esses hiperpar√¢metros: 0.8880\n",
      "\n",
      "Melhor combina√ß√£o encontrada: {'epochs': 7, 'batch_size': 8, 'learning_rate': 0.007, 'momentum': 0.8, 'bptt_trunc': 2} com accuracy 0.8880\n"
     ]
    }
   ],
   "source": [
    "# Fun√ß√£o de ativa√ß√£o Sigmoid\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Definir pesos corretamente (Xavier Initialization)\n",
    "W_xh = np.random.randn(input_size, hidden_size) * np.sqrt(1. / input_size)\n",
    "W_hh = np.random.randn(hidden_size, hidden_size) * np.sqrt(1. / hidden_size)\n",
    "W_hy = np.random.randn(hidden_size, output_size) * np.sqrt(1. / hidden_size)\n",
    "\n",
    "HYPERPARAMS = [\n",
    "    {\"epochs\": 5, \"batch_size\": 8, \"learning_rate\": 0.01, \"momentum\": 0.9, \"bptt_trunc\": 2},\n",
    "    {\"epochs\": 10, \"batch_size\": 16, \"learning_rate\": 0.005, \"momentum\": 0.95, \"bptt_trunc\": 3},\n",
    "    {\"epochs\": 7, \"batch_size\": 8, \"learning_rate\": 0.007, \"momentum\": 0.8, \"bptt_trunc\": 2},\n",
    "]\n",
    "\n",
    "best_accuracy = 0\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "# Testando hiperpar√¢metros\n",
    "for params in HYPERPARAMS:\n",
    "    print(f\"\\nTestando hiperpar√¢metros: {params}\")\n",
    "\n",
    "    rnn = RNN(\n",
    "        n_units=20,\n",
    "        # activation=ReLUActivation(),\n",
    "        activation=TanhActivation(),\n",
    "        bptt_trunc=params[\"bptt_trunc\"],\n",
    "        input_shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM),\n",
    "        epochs=params[\"epochs\"],\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        momentum=params[\"momentum\"],\n",
    "        loss=BinaryCrossEntropy,\n",
    "        metric=accuracy\n",
    "    )\n",
    "\n",
    "    optimizer = Optimizer(learning_rate=params[\"learning_rate\"])\n",
    "    rnn.initialize(optimizer)\n",
    "\n",
    "    for epoch in range(params[\"epochs\"]):\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in get_mini_batches(X_train, y_train, params[\"batch_size\"]):\n",
    "            y_pred = rnn.forward_propagation(X_batch)\n",
    "            y_pred_final = sigmoid(y_pred[:, -1, :])  # Aplica Sigmoid na √∫ltima sa√≠da\n",
    "\n",
    "            loss = binary_cross_entropy(y_batch.reshape(-1, 1), y_pred_final)\n",
    "            grad_loss = (y_pred_final - y_batch.reshape(-1, 1)) / y_batch.shape[0]\n",
    "\n",
    "            grad_loss_expanded = np.zeros_like(y_pred)\n",
    "            grad_loss_expanded[:, -1, :] = grad_loss\n",
    "\n",
    "            rnn.backward_propagation(grad_loss_expanded)\n",
    "\n",
    "            total_loss += loss\n",
    "\n",
    "        print(f\"√âpoca {epoch+1}/{params['epochs']} - Loss: {total_loss:.4f}\")\n",
    "\n",
    "    # Avalia√ß√£o\n",
    "    preds = rnn.predict(X_val)\n",
    "    \n",
    "    # Debug do formato de `preds`\n",
    "    print(f\"Formato de preds: {preds.shape}\")\n",
    "\n",
    "    # Corrigir caso `preds` seja 1D\n",
    "    if preds.ndim == 1:\n",
    "        preds = preds[:, np.newaxis]\n",
    "\n",
    "    acc = accuracy(y_val, preds)\n",
    "\n",
    "    print(f\"Accuracy com esses hiperpar√¢metros: {acc:.4f}\")\n",
    "\n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        best_params = params\n",
    "        best_model = rnn\n",
    "\n",
    "print(f\"\\nMelhor combina√ß√£o encontrada: {best_params} com accuracy {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Treinar o Modelo Final com melhor accuracy (obtido no passo anterior)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino final - √âpoca 1/7 - Loss: 30.6318\n",
      "Treino final - √âpoca 2/7 - Loss: 28.1696\n",
      "Treino final - √âpoca 3/7 - Loss: 23.2188\n",
      "Treino final - √âpoca 4/7 - Loss: 18.6278\n",
      "Treino final - √âpoca 5/7 - Loss: 15.8588\n",
      "Treino final - √âpoca 6/7 - Loss: 14.5323\n",
      "Treino final - √âpoca 7/7 - Loss: 13.7087\n",
      "Formato de y_test_pred: (609,)\n",
      "\n",
      "Accuracy final no conjunto de teste: 0.8719\n",
      "\n",
      "Compara√ß√£o entre valores esperados e previstos:\n",
      "     expected_value  predicted_value_raw  predicted_value\n",
      "0               1.0                  1.0                1\n",
      "1               0.0                  0.0                0\n",
      "2               1.0                  1.0                1\n",
      "3               0.0                  0.0                0\n",
      "4               1.0                  1.0                1\n",
      "..              ...                  ...              ...\n",
      "604             1.0                  1.0                1\n",
      "605             1.0                  1.0                1\n",
      "606             0.0                  0.0                0\n",
      "607             1.0                  1.0                1\n",
      "608             0.0                  0.0                0\n",
      "\n",
      "[609 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "final_rnn = RNN(\n",
    "    n_units=20,\n",
    "    # activation=ReLUActivation(),\n",
    "    activation=TanhActivation(),\n",
    "    bptt_trunc=best_params[\"bptt_trunc\"],\n",
    "    input_shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM),\n",
    "    epochs=best_params[\"epochs\"],\n",
    "    batch_size=best_params[\"batch_size\"],\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    momentum=best_params[\"momentum\"],\n",
    "    loss=BinaryCrossEntropy,\n",
    "    metric=accuracy\n",
    ")\n",
    "\n",
    "final_optimizer = Optimizer(learning_rate=best_params[\"learning_rate\"])\n",
    "final_rnn.initialize(final_optimizer)\n",
    "\n",
    "for epoch in range(best_params[\"epochs\"]):\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in get_mini_batches(X_train, y_train, best_params[\"batch_size\"]):\n",
    "        y_pred = final_rnn.forward_propagation(X_batch)\n",
    "        y_pred_final = sigmoid(y_pred[:, -1, :])  # Aplica Sigmoid na √∫ltima sa√≠da\n",
    "\n",
    "        loss = binary_cross_entropy(y_batch.reshape(-1, 1), y_pred_final)\n",
    "\n",
    "        # Calcular o gradiente correto\n",
    "        grad_loss = (y_pred_final - y_batch.reshape(-1, 1)) / y_batch.shape[0]\n",
    "\n",
    "        # Expandir para 3 dimens√µes para ser compat√≠vel com a RNN\n",
    "        grad_loss_expanded = np.zeros_like(y_pred)  # (batch_size, timesteps, output_size)\n",
    "        grad_loss_expanded[:, -1, :] = grad_loss  # Apenas o √∫ltimo timestep recebe gradiente\n",
    "\n",
    "        # Passar o gradiente expandido\n",
    "        final_rnn.backward_propagation(grad_loss_expanded)\n",
    "\n",
    "        total_loss += loss\n",
    "\n",
    "    print(f\"Treino final - √âpoca {epoch+1}/{best_params['epochs']} - Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Testar Modelo Final\n",
    "y_test_pred = final_rnn.predict(X_test)\n",
    "\n",
    "print(f\"Formato de y_test_pred: {y_test_pred.shape}\")  # üõ†Ô∏è Debug\n",
    "\n",
    "# Se for 1D, expandimos para 2D\n",
    "if y_test_pred.ndim == 1:\n",
    "    y_test_pred = y_test_pred[:, np.newaxis]\n",
    "\n",
    "# Se for 2D (batch_size, timesteps), pegamos o √∫ltimo timestep\n",
    "if y_test_pred.ndim == 2:\n",
    "    y_test_pred_final = y_test_pred[:, -1]  #  Sem `:` no final, pois j√° √© 1D\n",
    "else:\n",
    "    y_test_pred_final = y_test_pred[:, -1, :]  #  Apenas se for 3D\n",
    "\n",
    "y_test_pred_labels = (y_test_pred_final > 0.5).astype(int)\n",
    "\n",
    "y_test_true = y_test.flatten()\n",
    "accuracy = np.mean(y_test_pred_labels == y_test_true)\n",
    "print(f\"\\nAccuracy final no conjunto de teste: {accuracy:.4f}\")\n",
    "\n",
    "# Criar DataFrame com Expected vs Predicted\n",
    "df_results = pd.DataFrame({\n",
    "    \"expected_value\": y_test_true,\n",
    "    \"predicted_value_raw\": y_test_pred_final.flatten(),  # Valor original antes do arredondamento\n",
    "    \"predicted_value\": y_test_pred_labels.flatten()  # Valor final bin√°rio (0 ou 1)\n",
    "})\n",
    "\n",
    "# Mostrar as previs√µes para compara√ß√£o\n",
    "print(\"\\nCompara√ß√£o entre valores esperados e previstos:\")\n",
    "print(df_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Previs√£o para o Dataset2 (disponibilizado pelo professor)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formato de y_test_pred2: (100,)\n"
     ]
    }
   ],
   "source": [
    "# Testar Modelo Final\n",
    "y_test_pred2 = final_rnn.predict(X_eval_final)\n",
    "\n",
    "print(f\"Formato de y_test_pred2: {y_test_pred2.shape}\")  # üõ†Ô∏è Debug\n",
    "\n",
    "# Se for 1D, expandimos para 2D\n",
    "if y_test_pred2.ndim == 1:\n",
    "    y_test_pred2 = y_test_pred2[:, np.newaxis]\n",
    "\n",
    "# Se for 2D (batch_size, timesteps), pegamos o √∫ltimo timestep\n",
    "if y_test_pred2.ndim == 2:\n",
    "    y_test_pred_final2 = y_test_pred2[:, -1]  #  Sem `:` no final, pois j√° √© 1D\n",
    "else:\n",
    "    y_test_pred_final2 = y_test_pred2[:, -1, :]  #  Apenas se for 3D\n",
    "\n",
    "y_test_pred_labels2 = (y_test_pred_final2 > 0.5).astype(int)\n",
    "\n",
    "######################################################################### cria√ß√£o do ficheiro csv com a previs√£o\n",
    "\n",
    "# Criar IDs para cada amostra com o formato \"D2-1\", \"D2-2\", etc.\n",
    "id_column = [f\"D2-{i}\" for i in range(1, len(y_test_pred_labels2) + 1)]\n",
    "\n",
    "# Converter labels para \"Human\" e \"AI\"\n",
    "labels = np.where(y_test_pred_labels2.flatten() == 1, \"AI\", \"Human\")\n",
    "\n",
    "# Criar DataFrame com ID e LABEL\n",
    "df_output = pd.DataFrame({\n",
    "    \"ID\": id_column,\n",
    "    \"Label\": labels\n",
    "})\n",
    "\n",
    "# N√£o estamos a guardar a previs√£o em .csv visto que este n√£o √© o melhor modelo\n",
    "\n",
    "# Guardar em CSV com separa√ß√£o por tabula√ß√£o\n",
    "# df_output.to_csv(\"rnn_predictions.csv\", index=False, sep='\\t')\n",
    "\n",
    "# print(\"Ficheiro 'rnn_predictions.csv' gerado com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **An√°lise de resultados**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Treino com dataset: gpt_vs_human**\n",
    "\n",
    "- Durante o treino: 0.87 - 0.9\n",
    "\n",
    "- Para dataset1: 0.66\n",
    "\n",
    "- Para dataset2: 0.8 - 1.0\n",
    "\n",
    "- Para ai_human: 0.51\n",
    "\n",
    "**Treino com dataset: ai_human**\n",
    "\n",
    "- Durante o treino: 0.81 - 0.84\n",
    "\n",
    "- Para gpt_vs_human: 0.49"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Hypertuning com base no modelo anterior - teste com 3600 combina√ß√µes diferentes**\n",
    "\n",
    "Foi feito o loop apresentado abaixo, com 3600 combina√ß√µes, por√©m por uma quest√£o de brevidade, estamos neste momento a rodar o c√≥digo apenas com o melhor resultado obtido:\n",
    "\n",
    "**Melhor combina√ß√£o encontrada: {'epochs': 5, 'batch_size': 8, 'learning_rate': 0.01, 'momentum': 0.8, 'bptt_trunc': 6} com accuracy 0.8929**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipo de accuracy antes da chamada: <class 'numpy.float64'>\n",
      "\n",
      "A testar hiperpar√¢metros: {'epochs': 5, 'batch_size': 8, 'learning_rate': 0.01, 'momentum': 0.8, 'bptt_trunc': 6}\n",
      "√âpoca 1/5 - Loss: 30.6939\n",
      "√âpoca 2/5 - Loss: 26.5226\n",
      "√âpoca 3/5 - Loss: 18.1324\n",
      "√âpoca 4/5 - Loss: 15.5612\n",
      "√âpoca 5/5 - Loss: 14.0313\n",
      "Accuracy com esses hiperpar√¢metros: 0.8699\n",
      "\n",
      "Melhor combina√ß√£o encontrada: {'epochs': 5, 'batch_size': 8, 'learning_rate': 0.01, 'momentum': 0.8, 'bptt_trunc': 6} com accuracy 0.8699\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tipo de accuracy antes da chamada: {type(accuracy)}\")\n",
    "if not callable(accuracy):  # Se n√£o for mais uma fun√ß√£o\n",
    "    del accuracy  # Remover a vari√°vel sobrescrita\n",
    "    from helpers.metrics import accuracy  # Reimporte \n",
    "\n",
    "\n",
    "# Definir hiperpar√¢metros para busca extensa\n",
    "# HYPERPARAMS = [\n",
    "#     {\"epochs\": ep, \"batch_size\": bs, \"learning_rate\": lr, \"momentum\": mo, \"bptt_trunc\": bt}\n",
    "#     for ep in [5, 10, 15, 20, 25, 30]\n",
    "#     for bs in [8, 16, 32, 64]\n",
    "#     for lr in [0.01, 0.005, 0.001, 0.0005, 0.0001]\n",
    "#     for mo in [0.7, 0.8, 0.85, 0.9, 0.95, 0.99]\n",
    "#     for bt in [2, 3, 4, 5, 6]\n",
    "# ]\n",
    "\n",
    "# Apenas com os melhores hiperpar√¢metros calculados anteriormente\n",
    "HYPERPARAMS = [\n",
    "    {\"epochs\": ep, \"batch_size\": bs, \"learning_rate\": lr, \"momentum\": mo, \"bptt_trunc\": bt}\n",
    "    for ep in [5]\n",
    "    for bs in [8]\n",
    "    for lr in [0.01]\n",
    "    for mo in [0.8]\n",
    "    for bt in [6]\n",
    "]\n",
    "\n",
    "best_accuracy = 0\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "start_time = time.time()\n",
    "MAX_TIME = 21600 #6 horas em segundos\n",
    "\n",
    "# Teste de hiperpar√¢metros \n",
    "for params in HYPERPARAMS:\n",
    "    if time.time() - start_time > MAX_TIME:\n",
    "        break\n",
    "    \n",
    "    print(f\"\\nA testar hiperpar√¢metros: {params}\")\n",
    "    \n",
    "    rnn = RNN(\n",
    "        n_units=20,\n",
    "        activation=TanhActivation(),\n",
    "        bptt_trunc=params[\"bptt_trunc\"],\n",
    "        input_shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM),\n",
    "        epochs=params[\"epochs\"],\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        momentum=params[\"momentum\"],\n",
    "        loss=BinaryCrossEntropy,\n",
    "        metric=accuracy\n",
    "    )\n",
    "    \n",
    "    optimizer = Optimizer(learning_rate=params[\"learning_rate\"])\n",
    "    rnn.initialize(optimizer)\n",
    "    \n",
    "    for epoch in range(params[\"epochs\"]):\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in get_mini_batches(X_train, y_train, params[\"batch_size\"]):\n",
    "            y_pred = rnn.forward_propagation(X_batch)\n",
    "            y_pred_final = sigmoid(y_pred[:, -1, :])\n",
    "\n",
    "            loss = binary_cross_entropy(y_batch.reshape(-1, 1), y_pred_final)\n",
    "            grad_loss = (y_pred_final - y_batch.reshape(-1, 1)) / y_batch.shape[0]\n",
    "            \n",
    "            grad_loss_expanded = np.zeros_like(y_pred)\n",
    "            grad_loss_expanded[:, -1, :] = grad_loss\n",
    "            \n",
    "            rnn.backward_propagation(grad_loss_expanded)\n",
    "            total_loss += loss\n",
    "        \n",
    "        print(f\"√âpoca {epoch+1}/{params['epochs']} - Loss: {total_loss:.4f}\")\n",
    "    \n",
    "    preds = rnn.predict(X_val)\n",
    "    if preds.ndim == 1:\n",
    "        preds = preds[:, np.newaxis]\n",
    "    acc_value = accuracy(y_val, preds)\n",
    "    \n",
    "    print(f\"Accuracy com esses hiperpar√¢metros: {acc_value:.4f}\")\n",
    "    \n",
    "    if acc_value > best_accuracy:\n",
    "        best_accuracy = acc_value\n",
    "        best_params = params\n",
    "        best_model = rnn\n",
    "\n",
    "print(f\"\\nMelhor combina√ß√£o encontrada: {best_params} com accuracy {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Treino do modelo final, com os melhores hiperpar√¢metros**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino final - √âpoca 1/5 - Loss: 30.4046\n",
      "Treino final - √âpoca 2/5 - Loss: 22.8964\n",
      "Treino final - √âpoca 3/5 - Loss: 17.1387\n",
      "Treino final - √âpoca 4/5 - Loss: 14.9374\n",
      "Treino final - √âpoca 5/5 - Loss: 13.8010\n",
      "Formato de y_test_pred: (609,)\n",
      "\n",
      "Accuracy final no conjunto de teste: 0.8736\n",
      "\n",
      "Compara√ß√£o entre valores esperados e previstos:\n",
      "     expected_value  predicted_value_raw  predicted_value\n",
      "0               1.0                  1.0                1\n",
      "1               0.0                  0.0                0\n",
      "2               1.0                  1.0                1\n",
      "3               0.0                  0.0                0\n",
      "4               1.0                  1.0                1\n",
      "..              ...                  ...              ...\n",
      "604             1.0                  1.0                1\n",
      "605             1.0                  1.0                1\n",
      "606             0.0                  1.0                1\n",
      "607             1.0                  1.0                1\n",
      "608             0.0                  0.0                0\n",
      "\n",
      "[609 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "final_rnn = RNN(\n",
    "    n_units=20,\n",
    "    activation=TanhActivation(),\n",
    "    bptt_trunc=best_params[\"bptt_trunc\"],\n",
    "    input_shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM),\n",
    "    epochs=best_params[\"epochs\"],\n",
    "    batch_size=best_params[\"batch_size\"],\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    momentum=best_params[\"momentum\"],\n",
    "    loss=BinaryCrossEntropy,\n",
    "    metric=accuracy\n",
    ")\n",
    "\n",
    "final_optimizer = Optimizer(learning_rate=best_params[\"learning_rate\"])\n",
    "final_rnn.initialize(final_optimizer)\n",
    "\n",
    "for epoch in range(best_params[\"epochs\"]):\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in get_mini_batches(X_train, y_train, best_params[\"batch_size\"]):\n",
    "        y_pred = final_rnn.forward_propagation(X_batch)\n",
    "        y_pred_final = sigmoid(y_pred[:, -1, :])  # Aplica Sigmoid na √∫ltima sa√≠da\n",
    "\n",
    "        loss = binary_cross_entropy(y_batch.reshape(-1, 1), y_pred_final)\n",
    "\n",
    "        # Calcular o gradiente correto\n",
    "        grad_loss = (y_pred_final - y_batch.reshape(-1, 1)) / y_batch.shape[0]\n",
    "\n",
    "        # Expandir para 3 dimens√µes para ser compat√≠vel com a RNN\n",
    "        grad_loss_expanded = np.zeros_like(y_pred)  # (batch_size, timesteps, output_size)\n",
    "        grad_loss_expanded[:, -1, :] = grad_loss  # Apenas o √∫ltimo timestep recebe gradiente\n",
    "\n",
    "        # Passar o gradiente expandido\n",
    "        final_rnn.backward_propagation(grad_loss_expanded)\n",
    "\n",
    "        total_loss += loss\n",
    "\n",
    "    print(f\"Treino final - √âpoca {epoch+1}/{best_params['epochs']} - Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Testar Modelo Final\n",
    "y_test_pred = final_rnn.predict(X_test)\n",
    "\n",
    "print(f\"Formato de y_test_pred: {y_test_pred.shape}\")  # Debug\n",
    "\n",
    "# Se for 1D, expandimos para 2D\n",
    "if y_test_pred.ndim == 1:\n",
    "    y_test_pred = y_test_pred[:, np.newaxis]\n",
    "\n",
    "# Se for 2D (batch_size, timesteps), pegamos o √∫ltimo timestep\n",
    "if y_test_pred.ndim == 2:\n",
    "    y_test_pred_final = y_test_pred[:, -1]  #  Sem `:` no final, pois j√° √© 1D\n",
    "else:\n",
    "    y_test_pred_final = y_test_pred[:, -1, :]  #  Apenas se for 3D\n",
    "\n",
    "y_test_pred_labels = (y_test_pred_final > 0.5).astype(int)\n",
    "\n",
    "y_test_true = y_test.flatten()\n",
    "accuracy = np.mean(y_test_pred_labels == y_test_true)\n",
    "print(f\"\\nAccuracy final no conjunto de teste: {accuracy:.4f}\")\n",
    "\n",
    "# Criar DataFrame com Expected vs Predicted\n",
    "df_results = pd.DataFrame({\n",
    "    \"expected_value\": y_test_true,\n",
    "    \"predicted_value_raw\": y_test_pred_final.flatten(),  # Valor original antes do arredondamento\n",
    "    \"predicted_value\": y_test_pred_labels.flatten()  # Valor final bin√°rio (0 ou 1)\n",
    "})\n",
    "\n",
    "# Mostrar as previs√µes para compara√ß√£o\n",
    "print(\"\\nCompara√ß√£o entre valores esperados e previstos:\")\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Previs√£o do melhor modelo para o dataset disponibilizado pelo professor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formato de y_test_pred2: (100,)\n"
     ]
    }
   ],
   "source": [
    "# Testar Modelo Final\n",
    "y_test_pred2 = final_rnn.predict(X_eval_final)\n",
    "\n",
    "print(f\"Formato de y_test_pred2: {y_test_pred2.shape}\")  # Debug\n",
    "\n",
    "# Se for 1D, expandimos para 2D\n",
    "if y_test_pred2.ndim == 1:\n",
    "    y_test_pred2 = y_test_pred2[:, np.newaxis]\n",
    "\n",
    "# Se for 2D (batch_size, timesteps), pegamos o √∫ltimo timestep\n",
    "if y_test_pred2.ndim == 2:\n",
    "    y_test_pred_final2 = y_test_pred2[:, -1]  #  Sem `:` no final, pois j√° √© 1D\n",
    "else:\n",
    "    y_test_pred_final2 = y_test_pred2[:, -1, :]  #  Apenas se for 3D\n",
    "\n",
    "y_test_pred_labels2 = (y_test_pred_final2 > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cria√ß√£o do Ficheiro CSV com a previs√£o final para o dataset disponibilizado pelo professor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions saved to dataset2_outputs_rnn_model.csv successfully!\n"
     ]
    }
   ],
   "source": [
    "# Generate IDs for each prediction in the format D2-1, D2-2, ...\n",
    "ids = [f\"D2-{i+1}\" for i in range(len(y_test_pred_labels2))]\n",
    "\n",
    "# Map 0 to \"Human\" and 1 to \"AI\"\n",
    "labels = [\"Human\" if pred == 0 else \"AI\" for pred in y_test_pred_labels2.flatten()]\n",
    "\n",
    "# Create a DataFrame with ID and Label columns\n",
    "df_predictions = pd.DataFrame({\n",
    "    \"ID\": ids,\n",
    "    \"Label\": labels\n",
    "})\n",
    "\n",
    "# Save the predictions to a CSV file using a tab separator to match the exact format\n",
    "df_predictions.to_csv(\"classify_output_datasets/dataset2_outputs_rnn_model.csv\", sep=\"\\t\", index=False)\n",
    "\n",
    "print(\"\\nPredictions saved to dataset2_outputs_rnn_model.csv successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **An√°lise de resultados da melhor combina√ß√£o encontrada**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Melhor combina√ß√£o encontrada: {'epochs': 5, 'batch_size': 8, 'learning_rate': 0.01, 'momentum': 0.8, 'bptt_trunc': 6} com accuracy 0.8929**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Treino com dataset: gpt_vs_human**\n",
    "\n",
    "- Durante o treino: 0.87 - 0.9\n",
    "\n",
    "- Para dataset1: 0.60\n",
    "\n",
    "- Para dataset2: 0.8 - 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
