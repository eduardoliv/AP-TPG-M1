{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Notebook RNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from helpers.dataset import Dataset\n",
    "from helpers.activation import TanhActivation\n",
    "from helpers.losses import BinaryCrossEntropy\n",
    "from helpers.metrics import accuracy\n",
    "from helpers.activation import ReLUActivation\n",
    "from rnn_model import RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nova Classe Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
    "        self.velocity = {}  # Dicionﾃ｡rio para armazenar velocidades dos gradientes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "\n",
    "    def update(self, param, grad):\n",
    "        \"\"\"Atualiza os pesos usando Gradient Descent com Momentum\"\"\"\n",
    "\n",
    "        param_id = id(param)  # 隼 Usar ID ﾃｺnico do numpy array\n",
    "\n",
    "        if param_id not in self.velocity:\n",
    "            self.velocity[param_id] = np.zeros_like(grad)\n",
    "\n",
    "        # Atualizaﾃｧﾃ｣o com momentum\n",
    "        self.velocity[param_id] = self.momentum * self.velocity[param_id] + (1 - self.momentum) * grad\n",
    "        return param - self.learning_rate * self.velocity[param_id]  # 隼 Retorna os novos pesos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tratamento de Dados**\n",
    "\n",
    "- Transformar cada palavra num token\n",
    "(de forma a manter a ordem das palavras)\n",
    "- Fazer o padding\n",
    "- Dar print ao dataset a cada mudanﾃｧa (5 primeiras linhas)\n",
    "\n",
    "Possibilidades:\n",
    "- Fazer normalizaﾃｧao\n",
    "- Ver possiveis melhorias com chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Anﾃ｡lise Inicial do Dataset1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Completo - Primeiras 5 linhas:\n",
      "   ID                                               Text Label\n",
      "0   1  Cars. Cars have been around since they became ...    AI\n",
      "1   2  Transportation is a large necessity in most co...    AI\n",
      "2   3  \"America's love affair with it's vehicles seem...    AI\n",
      "3   4  How often do you ride in a car? Do you drive a...    AI\n",
      "4   5  Cars are a wonderful thing. They are perhaps o...    AI\n",
      "\n",
      "Dataset Completo - Ultimas 5 linhas:\n",
      "         ID                                               Text  Label\n",
      "34048  4048  This research paper investigates the vortex dy...     AI\n",
      "34049  4049  Given a remarkable representation of the gener...  Human\n",
      "34050  4050  The Veldkamp space of two-qubits is a mathemat...     AI\n",
      "34051  4051  The equilibration of macroscopic degrees of fr...  Human\n",
      "34052  4052  This research paper investigates the fusion pr...     AI\n"
     ]
    }
   ],
   "source": [
    "# Definir os caminhos dos arquivos de TREINO\n",
    "# input_csv1 = \"../tarefa_1/clean_input_datasets/gpt_vs_human_data_set_inputs.csv\"\n",
    "# output_csv1 = \"../tarefa_1/clean_output_datasets/gpt_vs_human_data_set_outputs.csv\"\n",
    "\n",
    "input_csv1 = \"../tarefa_1/clean_input_datasets/ai_human_input_sm.csv\"\n",
    "output_csv1 = \"../tarefa_1/clean_output_datasets/ai_human_output_sm.csv\"\n",
    "\n",
    "# Definir os caminhos dos arquivos de TESTE\n",
    "# input_csv2 = \"../tarefa_1/clean_input_datasets/ai_human_input_sm.csv\"\n",
    "# output_csv2 = \"../tarefa_1/clean_output_datasets/ai_human_output_sm.csv\"\n",
    "\n",
    "input_csv2 = \"../tarefa_1/clean_input_datasets/gpt_vs_human_data_set_inputs.csv\"\n",
    "output_csv2 = \"../tarefa_1/clean_output_datasets/gpt_vs_human_data_set_outputs.csv\"\n",
    " \n",
    "# Carregar os datasets de treino\n",
    "df_input1 = pd.read_csv(input_csv1, sep=\"\\t\")  # Ajuste o separador se necessﾃ｡rio\n",
    "df_output1 = pd.read_csv(output_csv1, sep=\"\\t\")\n",
    "\n",
    "# Carregar os datasets de teste\n",
    "df_input2 = pd.read_csv(input_csv2, sep=\"\\t\")\n",
    "df_output2 = pd.read_csv(output_csv2, sep=\"\\t\")\n",
    "\n",
    "# Assumindo que hﾃ｡ uma coluna de ID para junﾃｧﾃ｣o\n",
    "df_train = pd.merge(df_input1, df_output1, on=\"ID\")\n",
    "df_test = pd.merge(df_input2, df_output2, on=\"ID\")\n",
    "\n",
    "# Concatenar treino e teste\n",
    "df_dataset1_merged = pd.concat([df_train, df_test], ignore_index=True)\n",
    "\n",
    "# # Carregar os datasets\n",
    "# df_input = pd.read_csv(input_csv, sep=\"\\t\")  # Ajuste o separador se necessﾃ｡rio\n",
    "# df_output = pd.read_csv(output_csv, sep=\"\\t\")\n",
    "\n",
    "# # Mostrar as primeiras 5 linhas do input\n",
    "# print(\"Input CSV - Primeiras 5 linhas:\")\n",
    "# print(df_input.head())\n",
    "\n",
    "# # Mostrar as primeiras 5 linhas do output\n",
    "# print(\"\\nOutput CSV - Primeiras 5 linhas:\")\n",
    "# print(df_output.head())\n",
    "\n",
    "# # Juntar os datasets pelo ID\n",
    "# df_dataset1_merged = pd.merge(df_input, df_output, on=\"ID\")\n",
    "\n",
    "# Mostrar as primeiras 5 linhas do dataset completo\n",
    "print(\"\\nDataset Completo - Primeiras 5 linhas:\")\n",
    "print(df_dataset1_merged.head())\n",
    "\n",
    "print(\"\\nDataset Completo - Ultimas 5 linhas:\")\n",
    "print(df_dataset1_merged.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remover caracteres especiais e pontuaﾃｧﾃ｣o e Converter em minﾃｺsculas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto limpo - primeiras 5 linhas:\n",
      "   ID                                               Text Label\n",
      "0   1  cars cars have been around since they became f...    AI\n",
      "1   2  transportation is a large necessity in most co...    AI\n",
      "2   3  americas love affair with its vehicles seems t...    AI\n",
      "3   4  how often do you ride in a car do you drive a ...    AI\n",
      "4   5  cars are a wonderful thing they are perhaps on...    AI\n"
     ]
    }
   ],
   "source": [
    "# Funﾃｧﾃ｣o para limpar texto\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Converter para minﾃｺsculas\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Remover pontuaﾃｧﾃ｣o\n",
    "    return text\n",
    "\n",
    "df_dataset1_merged[\"clean_text\"] = df_dataset1_merged[\"Text\"].apply(clean_text)\n",
    "\n",
    "# Manter apenas as colunas desejadas e renomear clean_text para Text\n",
    "df_dataset1_merged = df_dataset1_merged[[\"ID\", \"clean_text\", \"Label\"]].rename(columns={\"clean_text\": \"Text\"})\n",
    "\n",
    "print(\"Texto limpo - primeiras 5 linhas:\")\n",
    "print(df_dataset1_merged.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remover stopwords (opcional)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto sem stopwords - primeiras 5 linhas:\n",
      "   ID                                               Text Label\n",
      "0   1  cars cars have been around since they became f...    AI\n",
      "1   2  transportation large necessity most countries ...    AI\n",
      "2   3  americas love affair its vehicles seems coolin...    AI\n",
      "3   4  how often do you ride car do you drive one any...    AI\n",
      "4   5  cars wonderful thing they perhaps one worlds g...    AI\n"
     ]
    }
   ],
   "source": [
    "# Lista de stopwords comuns (podes adicionar mais conforme necessﾃ｡rio)\n",
    "stopwords = {\n",
    "    \"the\", \"of\", \"and\", \"in\", \"to\", \"is\", \"a\", \"that\", \"for\", \"are\", \"on\", \"with\", \n",
    "    \"as\", \"at\", \"by\", \"from\", \"this\", \"it\", \"an\", \"be\", \"or\", \"which\", \"was\", \"were\"\n",
    "}\n",
    "\n",
    "# Funﾃｧﾃ｣o para remover stopwords\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()  # Dividir em palavras\n",
    "    filtered_words = [word for word in words if word not in stopwords]  # Remover stopwords\n",
    "    return \" \".join(filtered_words)  # Juntar as palavras de novo\n",
    "\n",
    "# Aplicar ao dataset\n",
    "df_dataset1_merged[\"Text\"] = df_dataset1_merged[\"Text\"].apply(remove_stopwords)\n",
    "\n",
    "# Exibir as primeiras 5 linhas apﾃｳs remoﾃｧﾃ｣o de stopwords\n",
    "print(\"Texto sem stopwords - primeiras 5 linhas:\")\n",
    "print(df_dataset1_merged.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Criar Embeddings e Label Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de palavras carregadas do GloVe: 400000\n",
      "Formato final dos dados para o modelo: (34053, 200, 50)\n",
      "Dataset apﾃｳs embedding - primeiras 5 linhas:\n",
      "   ID                                               Text  Label\n",
      "0   1  [[0.5172399878501892, -0.26030999422073364, 1....      1\n",
      "1   2  [[0.38842999935150146, 0.1005999967455864, 0.6...      1\n",
      "2   3  [[0.3543199896812439, 0.12161999940872192, -0....      1\n",
      "3   4  [[0.6893799901008606, -0.1064400002360344, 0.1...      1\n",
      "4   5  [[0.5172399878501892, -0.26030999422073364, 1....      1\n"
     ]
    }
   ],
   "source": [
    "# Mapear labels para valores numﾃｩricos\n",
    "label_map = {\"Human\": 0, \"AI\": 1}\n",
    "df_dataset1_merged[\"Label\"] = df_dataset1_merged[\"Label\"].map(label_map)\n",
    "\n",
    "# Carregar o GloVe\n",
    "EMBEDDING_DIM = 50  # Dimensﾃ｣o do embedding\n",
    "\n",
    "embedding_dict = {}\n",
    "with open(\"helpers/glove.6B.50d.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.array(values[1:], dtype=\"float32\")\n",
    "        embedding_dict[word] = vector\n",
    "\n",
    "print(f\"Total de palavras carregadas do GloVe: {len(embedding_dict)}\")\n",
    "\n",
    "# Converter palavras para embeddings\n",
    "def text_to_embedding(text, embedding_dict, embedding_dim=50):\n",
    "    words = text.split()\n",
    "    embeddings = [embedding_dict.get(word, np.zeros(embedding_dim)) for word in words]  # Usa vetor do GloVe ou vetor zerado\n",
    "    \n",
    "    # Se a lista estiver vazia, retorna um vetor de zeros\n",
    "    if len(embeddings) == 0:\n",
    "        embeddings = [np.zeros(embedding_dim)]\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "df_dataset1_merged[\"Embedding\"] = df_dataset1_merged[\"Text\"].apply(lambda x: text_to_embedding(x, embedding_dict, EMBEDDING_DIM))\n",
    "\n",
    "# Padronizar comprimento das sequﾃｪncias (Padding ou Truncamento)\n",
    "MAX_SEQUENCE_LENGTH = 200  # Ajustﾃ｡vel conforme necessﾃ｡rio\n",
    "\n",
    "# def pad_embedding_sequence(seq, max_length, embedding_dim):\n",
    "#     if len(seq) > max_length:\n",
    "#         return np.array(seq[:max_length])  # Truncar\n",
    "#     else:\n",
    "#         padding = np.zeros((max_length - len(seq), embedding_dim))  # Criar padding\n",
    "#         return np.vstack([seq, padding])  # Adicionar padding no final\n",
    "\n",
    "def pad_embedding_sequence(seq, max_length, embedding_dim):\n",
    "    seq = np.array(seq)  # Converter para NumPy array para evitar problemas de formato\n",
    "    \n",
    "    if seq.shape[0] == 0:  # Se for uma sequﾃｪncia vazia, criar um array de zeros\n",
    "        seq = np.zeros((1, embedding_dim))\n",
    "\n",
    "    if seq.shape[0] > max_length:  # Truncar se for maior que o mﾃ｡ximo permitido\n",
    "        return seq[:max_length]\n",
    "\n",
    "    padding = np.zeros((max_length - seq.shape[0], embedding_dim))  # Criar padding\n",
    "    return np.vstack([seq, padding])  # Adicionar padding no final\n",
    "\n",
    "\n",
    "df_dataset1_merged[\"Embedding\"] = df_dataset1_merged[\"Embedding\"].apply(lambda x: pad_embedding_sequence(x, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM))\n",
    "\n",
    "# Converter para array NumPy para alimentar o modelo\n",
    "X = np.array(df_dataset1_merged[\"Embedding\"].tolist())\n",
    "y = np.array(df_dataset1_merged[\"Label\"])  # Labels numﾃｩricos\n",
    "\n",
    "print(\"Formato final dos dados para o modelo:\", X.shape)  # Deve ser (n_amostras, 100, 50)\n",
    "\n",
    "# Manter apenas as colunas desejadas e renomear clean_text para Text\n",
    "df_dataset1_merged = df_dataset1_merged[[\"ID\", \"Embedding\", \"Label\"]].rename(columns={\"Embedding\": \"Text\"})\n",
    "\n",
    "print(\"Dataset apﾃｳs embedding - primeiras 5 linhas:\")\n",
    "print(df_dataset1_merged.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Converter palavras em nﾃｺmeros (Tokenizaﾃｧﾃ｣o)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Criar vocabulﾃ｡rio\n",
    "# word_counts = Counter()\n",
    "# for text in df_dataset1_merged[\"Text\"]:\n",
    "#     word_counts.update(text.split())\n",
    "\n",
    "# # Definir vocabulﾃ｡rio com um limite de palavras (ex: 5000 palavras mais frequentes)\n",
    "# MAX_VOCAB_SIZE = 10000\n",
    "# vocab = {word: idx + 1 for idx, (word, _) in enumerate(word_counts.most_common(MAX_VOCAB_SIZE))}\n",
    "\n",
    "# # Adicionar token especial para palavras desconhecidas\n",
    "# vocab[\"<OOV>\"] = len(vocab) + 1  \n",
    "\n",
    "# print(\"Exemplo de vocabulﾃ｡rio:\", list(vocab.items())[:10])\n",
    "\n",
    "# # Converter frases para sequﾃｪncias numﾃｩricas\n",
    "# def text_to_sequence(text, vocab):\n",
    "#     return [vocab.get(word, vocab[\"<OOV>\"]) for word in text.split()]\n",
    "\n",
    "# # Aplicar a tokenizaﾃｧﾃ｣o ao DataFrame\n",
    "# df_dataset1_merged[\"tokenized_text\"] = df_dataset1_merged[\"Text\"].apply(lambda x: text_to_sequence(x, vocab))\n",
    "\n",
    "# # Manter apenas as colunas desejadas e renomear clean_text para Text\n",
    "# df_dataset1_merged = df_dataset1_merged[[\"ID\", \"tokenized_text\", \"Label\"]].rename(columns={\"tokenized_text\": \"Text\"})\n",
    "\n",
    "# print(\"Texto tokenizado - primeiras 5 linhas:\")\n",
    "# print(df_dataset1_merged.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padronizar o comprimento das sequﾃｪncias**\n",
    "\n",
    "tentar padding a esquerda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formato final dos dados para o modelo: (34053, 120, 50)\n",
      "Texto padronizado - primeiras 5 linhas:\n",
      "   ID                                               Text  Label\n",
      "0   1  [[0.5172399878501892, -0.26030999422073364, 1....      1\n",
      "1   2  [[0.38842999935150146, 0.1005999967455864, 0.6...      1\n",
      "2   3  [[0.3543199896812439, 0.12161999940872192, -0....      1\n",
      "3   4  [[0.6893799901008606, -0.1064400002360344, 0.1...      1\n",
      "4   5  [[0.5172399878501892, -0.26030999422073364, 1....      1\n"
     ]
    }
   ],
   "source": [
    "# Definir tamanho mﾃ｡ximo das sequﾃｪncias\n",
    "MAX_SEQUENCE_LENGTH = 120  # Ajusta conforme necessﾃ｡rio\n",
    "EMBEDDING_DIM = 50  # Mantﾃｩm o mesmo tamanho dos embeddings\n",
    "\n",
    "# Funﾃｧﾃ｣o para aplicar padding corretamente\n",
    "def pad_embedding_sequence(seq, max_length, embedding_dim):\n",
    "    seq = np.array(seq)  # Garante que a sequﾃｪncia ﾃｩ um array NumPy\n",
    "\n",
    "    if len(seq) > max_length:\n",
    "        return seq[:max_length]  # Truncar se for maior\n",
    "    else:\n",
    "        padding = np.zeros((max_length - len(seq), embedding_dim))  # Criar padding de vetores de zeros\n",
    "        return np.vstack([seq, padding])  # Adicionar padding no final\n",
    "\n",
    "# Aplicar padding ﾃs sequﾃｪncias de embeddings\n",
    "df_dataset1_merged[\"Text\"] = df_dataset1_merged[\"Text\"].apply(lambda x: pad_embedding_sequence(x, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM))\n",
    "\n",
    "# Converter para NumPy array para treinar o modelo\n",
    "X = np.array(df_dataset1_merged[\"Text\"].tolist())\n",
    "y = np.array(df_dataset1_merged[\"Label\"])  # Labels numﾃｩricos\n",
    "\n",
    "print(\"Formato final dos dados para o modelo:\", X.shape)  # Deve ser (n_amostras, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)\n",
    "\n",
    "# Exibir primeiras 5 linhas apﾃｳs padding\n",
    "print(\"Texto padronizado - primeiras 5 linhas:\")\n",
    "print(df_dataset1_merged.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Criar Embeddings e Label Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mapear labels para valores numﾃｩricos\n",
    "# label_map = {\"Human\": 0, \"AI\": 1}\n",
    "# df_dataset1_merged[\"Label\"] = df_dataset1_merged[\"Label\"].map(label_map)\n",
    "\n",
    "# # Converter para array NumPy\n",
    "# X = np.array(df_dataset1_merged[\"Text\"].tolist())  # Lista de tokens (jﾃ｡ com padding)\n",
    "# y = np.array(df_dataset1_merged[\"Label\"])  # Labels numﾃｩricos\n",
    "\n",
    "# print(\"Formato dos dados de entrada:\", X.shape)  # Deve ser (n_amostras, 100)\n",
    "\n",
    "# EMBEDDING_DIM = 50  # Nﾃｺmero de dimensﾃｵes dos embeddings\n",
    "# VOCAB_SIZE = len(vocab) + 1  # O tamanho do vocabulﾃ｡rio + 1 para <OOV>\n",
    "\n",
    "# # Carregar o GloVe\n",
    "# embedding_dict = {}\n",
    "# with open(\"helpers/glove.6B.50d.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     for line in f:\n",
    "#         values = line.split()\n",
    "#         word = values[0]\n",
    "#         vector = np.array(values[1:], dtype=\"float32\")\n",
    "#         embedding_dict[word] = vector\n",
    "\n",
    "# # Criar a matriz de embeddings (substituir random embeddings)\n",
    "# embedding_matrix = np.random.uniform(-1, 1, (VOCAB_SIZE, EMBEDDING_DIM))  # Inicializa aleatoriamente\n",
    "# for word, idx in vocab.items():\n",
    "#     if word in embedding_dict:  # Se a palavra estiver no GloVe, usa o vetor prﾃｩ-treinado\n",
    "#         embedding_matrix[idx] = embedding_dict[word]\n",
    "\n",
    "# # Converter tokens para embeddings\n",
    "# X_embedded = np.array([[embedding_matrix[token] for token in seq] for seq in X])\n",
    "\n",
    "# print(\"Formato dos dados com embeddings:\", X_embedded.shape)  # Deve ser (n_amostras, 100, EMBEDDING_DIM)\n",
    "\n",
    "# # Adicionar os embeddings ao DataFrame para visualizar melhor\n",
    "# df_dataset1_merged[\"Embedding\"] = list(X_embedded)\n",
    "\n",
    "# # Manter apenas as colunas desejadas e renomear Embedding para Text\n",
    "# df_dataset1_merged = df_dataset1_merged[[\"ID\", \"Embedding\", \"Label\"]].rename(columns={\"Embedding\": \"Text\"})\n",
    "\n",
    "# # Print do dataframe atualizado\n",
    "# print(\"\\nDataset atualizado - Primeiras 5 linhas:\")\n",
    "# print(df_dataset1_merged.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalizaﾃｧﾃ｣o dos Embeddings**\n",
    "\n",
    "dar print as ultimas possiﾃｧoes, deve ter 0s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formato final dos dados para o modelo: (34053, 120, 50)\n",
      "\n",
      "Dataset apﾃｳs normalizaﾃｧﾃ｣o dos embeddings:\n",
      "   ID                                               Text  Label\n",
      "0   1  [[0.5612960064685145, -0.8805426099595246, 2.0...      1\n",
      "1   2  [[-0.21994956121576564, 0.13893814962304316, 0...      1\n",
      "2   3  [[0.07885863917045757, 0.030814049393885327, -...      1\n",
      "3   4  [[0.7642999363812941, -0.4599518049576957, -0....      1\n",
      "4   5  [[0.5979321950261304, -0.5366264554735934, 2.8...      1\n"
     ]
    }
   ],
   "source": [
    "# Funﾃｧﾃ｣o para normalizar cada embedding (zero mean, unit variance)\n",
    "def normalize_embedding(emb):\n",
    "    mean = np.mean(emb, axis=0)  # Mﾃｩdia por dimensﾃ｣o do embedding\n",
    "    std = np.std(emb, axis=0) + 1e-8  # Desvio padrﾃ｣o (evita divisﾃ｣o por zero)\n",
    "    return (emb - mean) / std\n",
    "\n",
    "# Aplicar normalizaﾃｧﾃ｣o alternativa aos embeddings\n",
    "df_dataset1_merged[\"Text\"] = df_dataset1_merged[\"Text\"].apply(normalize_embedding)\n",
    "\n",
    "# Converter para array NumPy para treinar o modelo\n",
    "X = np.array(df_dataset1_merged[\"Text\"].tolist())\n",
    "y = np.array(df_dataset1_merged[\"Label\"])  # Labels numﾃｩricos\n",
    "\n",
    "print(\"Formato final dos dados para o modelo:\", X.shape)  # Deve ser (n_amostras, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)\n",
    "\n",
    "# Print do dataset atualizado\n",
    "print(\"\\nDataset apﾃｳs normalizaﾃｧﾃ｣o dos embeddings:\")\n",
    "print(df_dataset1_merged.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop da coluna ID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formato final dos dados para o modelo: (34053, 120, 50)\n",
      "\n",
      "Dataset apﾃｳs drop:\n",
      "                                                Text  Label\n",
      "0  [[0.5612960064685145, -0.8805426099595246, 2.0...      1\n",
      "1  [[-0.21994956121576564, 0.13893814962304316, 0...      1\n",
      "2  [[0.07885863917045757, 0.030814049393885327, -...      1\n",
      "3  [[0.7642999363812941, -0.4599518049576957, -0....      1\n",
      "4  [[0.5979321950261304, -0.5366264554735934, 2.8...      1\n"
     ]
    }
   ],
   "source": [
    "if \"ID\" in df_dataset1_merged.columns:\n",
    "    df_dataset1_merged = df_dataset1_merged.drop(columns=[\"ID\"])\n",
    "\n",
    "print(\"Formato final dos dados para o modelo:\", X.shape)  # Deve ser (n_amostras, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)\n",
    "\n",
    "# Print do dataset atualizado\n",
    "print(\"\\nDataset apﾃｳs drop:\")\n",
    "print(df_dataset1_merged.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Divisﾃ｣o do Dataset (Train/Test Split) 70%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do conjunto de treino: (21000, 2)\n",
      "Tamanho do conjunto de validaﾃｧﾃ｣o: (4500, 2)\n",
      "Tamanho do conjunto de teste: (4500, 2)\n",
      "Tamanho do conjunto de avaliaﾃｧﾃ｣o final: (4053, 2)\n",
      "Formato dos dados:\n",
      "  隼 Treino: (21000, 120, 50)\n",
      "  隼 Validaﾃｧﾃ｣o: (4500, 120, 50)\n",
      "  隼 Teste: (4500, 120, 50)\n",
      "  隼 Avaliaﾃｧﾃ｣o final: (4053, 120, 50)\n"
     ]
    }
   ],
   "source": [
    "# # Definir seed global para garantir reprodutibilidade\n",
    "# SEED = 42\n",
    "# np.random.seed(SEED)\n",
    "# random.seed(SEED)\n",
    "\n",
    "# # Embaralhar o dataset antes de dividir\n",
    "# df_dataset1_merged = df_dataset1_merged.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "# # Definir proporﾃｧﾃｵes de treino, validaﾃｧﾃ｣o e teste\n",
    "# train_ratio = 0.7  \n",
    "# val_ratio = 0.15  # 15% para validaﾃｧﾃ｣o\n",
    "# test_ratio = 0.15  # 15% para teste\n",
    "\n",
    "# # Definir ﾃｭndices para divisﾃ｣o\n",
    "# train_index = int(len(df_dataset1_merged) * train_ratio)\n",
    "# val_index = train_index + int(len(df_dataset1_merged) * val_ratio)\n",
    "\n",
    "# # Separar os datasets\n",
    "# df_train = df_dataset1_merged.iloc[:train_index]\n",
    "# df_val = df_dataset1_merged.iloc[train_index:val_index]\n",
    "# df_test = df_dataset1_merged.iloc[val_index:]\n",
    "\n",
    "# # Print dos datasets separados\n",
    "# print(f\"Tamanho do conjunto de treino: {df_train.shape}\")\n",
    "# print(f\"Tamanho do conjunto de validaﾃｧﾃ｣o: {df_val.shape}\")\n",
    "# print(f\"Tamanho do conjunto de teste: {df_test.shape}\")\n",
    "\n",
    "# # Converter para arrays NumPy\n",
    "# X_train, y_train = np.array(df_train[\"Text\"].tolist()), np.array(df_train[\"Label\"])\n",
    "# X_val, y_val = np.array(df_val[\"Text\"].tolist()), np.array(df_val[\"Label\"])\n",
    "# X_test, y_test = np.array(df_test[\"Text\"].tolist()), np.array(df_test[\"Label\"])\n",
    "\n",
    "# print(f\"Formato dos dados - Treino: {X_train.shape}, Validaﾃｧﾃ｣o: {X_val.shape}, Teste: {X_test.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "# Definir seed global para garantir reprodutibilidade\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "######################################################### dataset de teste\n",
    "# Separar as ﾃｺltimas 5 linhas para avaliaﾃｧﾃ｣o final\n",
    "df_eval_final = df_dataset1_merged.tail(4053)\n",
    "\n",
    "# Remover essas 5 linhas do dataset antes de embaralhar\n",
    "df_remaining = df_dataset1_merged.iloc[:-4053]\n",
    "#########################################################\n",
    "\n",
    "# Embaralhar o dataset restante\n",
    "df_remaining = df_remaining.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "# Definir proporﾃｧﾃｵes de treino (70%), validaﾃｧﾃ｣o (15%) e teste (15%)\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15  # 15% validaﾃｧﾃ｣o\n",
    "test_ratio = 0.15  # 15% teste\n",
    "\n",
    "# Definir ﾃｭndices para divisﾃ｣o\n",
    "train_index = int(len(df_remaining) * train_ratio)\n",
    "val_index = train_index + int(len(df_remaining) * val_ratio)\n",
    "\n",
    "# Separar os conjuntos de treino, validaﾃｧﾃ｣o e teste\n",
    "df_train = df_remaining.iloc[:train_index]\n",
    "df_val = df_remaining.iloc[train_index:val_index]\n",
    "df_test = df_remaining.iloc[val_index:]\n",
    "\n",
    "# Print dos tamanhos dos datasets\n",
    "print(f\"Tamanho do conjunto de treino: {df_train.shape}\")\n",
    "print(f\"Tamanho do conjunto de validaﾃｧﾃ｣o: {df_val.shape}\")\n",
    "print(f\"Tamanho do conjunto de teste: {df_test.shape}\")\n",
    "print(f\"Tamanho do conjunto de avaliaﾃｧﾃ｣o final: {df_eval_final.shape}\")\n",
    "\n",
    "# Converter para arrays NumPy\n",
    "X_train, y_train = np.array(df_train[\"Text\"].tolist()), np.array(df_train[\"Label\"])\n",
    "X_val, y_val = np.array(df_val[\"Text\"].tolist()), np.array(df_val[\"Label\"])\n",
    "X_test, y_test = np.array(df_test[\"Text\"].tolist()), np.array(df_test[\"Label\"])\n",
    "X_eval_final, y_eval_final = np.array(df_eval_final[\"Text\"].tolist()), np.array(df_eval_final[\"Label\"])\n",
    "\n",
    "# Print dos formatos dos dados\n",
    "print(f\"Formato dos dados:\")\n",
    "print(f\"   Treino: {X_train.shape}\")\n",
    "print(f\"   Validaﾃｧﾃ｣o: {X_val.shape}\")\n",
    "print(f\"   Teste: {X_test.shape}\")\n",
    "print(f\"   Avaliaﾃｧﾃ｣o final: {X_eval_final.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verificaﾃｧﾃ｣o Final do Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "東 Primeiras 5 entradas do conjunto de TREINO:\n",
      "                                                Text  Label\n",
      "0  [[-0.661526360482493, 1.7914446075613137, 0.53...      0\n",
      "1  [[0.6163854, 0.819715, -0.69918394, -0.5064302...      0\n",
      "2  [[-2.130435077862449, 1.2084377298346007, -1.1...      0\n",
      "3  [[0.3332962829897395, 0.9475958753066612, -0.1...      0\n",
      "4  [[-1.9136734571069933, -0.554044880262533, 0.4...      1\n",
      "\n",
      "東 Primeiras 5 entradas do conjunto de VALIDAﾃﾃグ:\n",
      "                                                    Text  Label\n",
      "21000  [[-0.004062520204015047, 0.5759439172082973, -...      1\n",
      "21001  [[-0.5077380159703886, 1.8100763252645844, 0.0...      0\n",
      "21002  [[-0.8361131, 0.036529806, -0.0940976, -0.3258...      0\n",
      "21003  [[-0.572186263545021, 2.059250454773294, 0.377...      1\n",
      "21004  [[0.38177555798135215, 2.0771762419473268, 0.4...      1\n",
      "\n",
      "東 Primeiras 5 entradas do conjunto de TESTE:\n",
      "                                                    Text  Label\n",
      "25500  [[0.31073517628748204, 1.491471944512809, 0.20...      0\n",
      "25501  [[0.23333033638228107, -1.4136113823027785, -0...      0\n",
      "25502  [[1.004598230093955, 1.4211563919743235, -0.82...      0\n",
      "25503  [[-0.3156516783370601, 0.7481203676804131, -1....      0\n",
      "25504  [[0.3277765400029483, -0.23394658957847425, 0....      0\n",
      "\n",
      "東 Primeiras 5 entradas do conjunto de AVALIAﾃﾃグ FINAL:\n",
      "                                                    Text  Label\n",
      "30000  [[-1.3698944045903378, -0.21571150230566394, 1...      0\n",
      "30001  [[1.3399297433627972, 1.641083731323519, 0.107...      1\n",
      "30002  [[0.8333313156245512, -1.1525800493855234, -0....      0\n",
      "30003  [[-2.1854298404165733, 0.1840526870107126, -0....      1\n",
      "30004  [[0.7043765115026446, -1.0626766868644306, -0....      0\n",
      "\n",
      "隼 Primeiras 5 entradas de X_train: [[[-0.66152636  1.79144461  0.53513141 ... -1.03714859  0.57189036\n",
      "    1.55371464]\n",
      "  [-0.82683123  0.07077006  1.16625205 ... -1.18134703 -0.29087963\n",
      "    2.86928314]\n",
      "  [-0.86111422 -0.01217718  0.72075864 ...  1.04253903 -1.33221744\n",
      "    1.29160385]\n",
      "  ...\n",
      "  [-0.33253657 -0.33818636 -1.3765971  ... -0.17314125 -0.05816486\n",
      "   -0.26028221]\n",
      "  [ 0.61504349  0.73412074  0.58420091 ... -0.74861245 -1.10488211\n",
      "   -0.41828727]\n",
      "  [ 1.59931891  0.10401186 -1.01934551 ...  0.35335886 -0.8296527\n",
      "   -1.64059606]]\n",
      "\n",
      " [[ 0.6163854   0.81971502 -0.69918394 ...  0.33972266 -0.41141894\n",
      "    0.227295  ]\n",
      "  [-0.14954914  0.99723709 -0.54294437 ...  1.34194326  1.14292121\n",
      "    0.95535606]\n",
      "  [-0.11149679  0.32223257 -0.54173511 ...  1.86235356 -0.10145402\n",
      "   -0.24887457]\n",
      "  ...\n",
      "  [ 0.31913865 -1.28102314 -0.29603487 ...  0.59169048  3.2985487\n",
      "    1.97999609]\n",
      "  [-0.41623524  1.20844769 -2.01882815 ...  0.42390916  1.2302109\n",
      "    0.78039557]\n",
      "  [ 0.4941428   0.43416157  0.6631031  ...  0.24780469 -1.87596655\n",
      "    0.48351365]]\n",
      "\n",
      " [[-2.13043508  1.20843773 -1.18743169 ...  0.4492477   1.46413545\n",
      "    1.78388736]\n",
      "  [ 0.97350987  1.13633054 -1.60142962 ...  0.39284919  0.54290376\n",
      "   -0.21543729]\n",
      "  [ 0.24811386  0.30670213 -2.107716   ...  1.25844361  0.3532457\n",
      "    1.1324924 ]\n",
      "  ...\n",
      "  [ 0.23214782 -0.62348214 -0.22135597 ...  0.7027551  -0.07467729\n",
      "   -1.36919042]\n",
      "  [ 0.01731227 -0.30175888 -0.31243512 ...  0.36929912  0.48385531\n",
      "    0.35020387]\n",
      "  [-1.39580075  1.45397975  0.5911812  ... -0.50808464 -0.93143622\n",
      "   -0.40780821]]\n",
      "\n",
      " [[ 0.33329628  0.94759588 -0.16658217 ... -2.10879699  0.12149138\n",
      "   -1.17863843]\n",
      "  [ 0.12079718 -3.40968434  0.18735339 ... -1.03421294 -0.28756205\n",
      "   -0.05073453]\n",
      "  [ 0.78970662  1.19826924 -1.81940125 ...  0.90891069  0.33381004\n",
      "   -0.14976799]\n",
      "  ...\n",
      "  [ 0.33329628  0.94759588 -0.16658217 ... -2.10879699  0.12149138\n",
      "   -1.17863843]\n",
      "  [-0.40745111  0.12468591 -1.43841577 ...  0.85335284  3.12151882\n",
      "    1.62698313]\n",
      "  [ 0.48700757  1.66105551 -1.68581391 ... -1.98959726 -0.19559704\n",
      "    0.03306237]]\n",
      "\n",
      " [[-1.91367346 -0.55404488  0.47628615 ... -0.83098448  2.01765577\n",
      "   -0.42873986]\n",
      "  [-0.43184764 -0.63407296 -0.21487072 ... -0.76527397 -0.18553076\n",
      "   -1.20125299]\n",
      "  [ 0.52732502 -1.18753918  0.84953246 ...  0.47660357 -0.24534451\n",
      "   -1.12874985]\n",
      "  ...\n",
      "  [-2.02740719 -0.05499542  0.45432798 ...  2.06350977  0.87420408\n",
      "    0.02182429]\n",
      "  [ 1.69564054 -0.12930383  0.29635172 ...  1.80625737  1.73401083\n",
      "    0.67484859]\n",
      "  [-0.72726711  1.70457852  0.14673639 ... -0.85768538  0.530827\n",
      "    0.73307705]]]\n",
      "\n",
      "隼 Primeiras 5 entradas de y_train: [0 0 0 0 1]\n",
      "\n",
      "隼 Primeiras 5 entradas de X_val: [[[-4.06252020e-03  5.75943917e-01 -1.04772381e+00 ... -2.02412405e-01\n",
      "   -1.92552257e-01 -5.91319135e-01]\n",
      "  [-1.44663043e+00  2.23933843e+00 -9.28873858e-02 ... -1.37846343e+00\n",
      "    5.62703705e-01  7.97250825e-01]\n",
      "  [-6.06983222e-01  1.02714382e+00 -4.79187164e-01 ...  9.96221181e-01\n",
      "    4.76072652e-01 -2.63843036e-01]\n",
      "  ...\n",
      "  [-6.09967852e-01  7.24281423e-01  6.77705071e-01 ... -1.08655027e+00\n",
      "    2.80054583e-01  1.77904261e+00]\n",
      "  [-3.10587501e-01 -5.31737370e-01  7.11515848e-01 ...  4.53849066e-01\n",
      "   -1.37295173e-01  4.70748887e-01]\n",
      "  [-7.03871854e-01  5.31886522e-01 -1.33255617e-02 ...  3.93376646e-01\n",
      "    1.03674398e+00  1.23659606e+00]]\n",
      "\n",
      " [[-5.07738016e-01  1.81007633e+00  9.56064608e-02 ... -9.42671977e-01\n",
      "    5.19497601e-01  1.52172073e+00]\n",
      "  [-6.62620232e-01  2.34332113e-01  6.49130356e-01 ... -1.13916244e+00\n",
      "   -2.85236712e-01  2.82472258e+00]\n",
      "  [ 1.82484325e-01  1.78425933e-01 -6.19300546e-01 ... -1.65289272e+00\n",
      "   -6.96467953e-01  1.69236631e+00]\n",
      "  ...\n",
      "  [-1.62075187e+00 -1.39021193e+00 -1.11790177e+00 ... -1.74345787e-01\n",
      "   -9.95420060e-02 -4.93106808e-01]\n",
      "  [-6.23800679e-01 -1.26468314e+00 -3.09321910e-01 ...  1.67369157e+00\n",
      "    2.33380395e+00 -1.58940486e+00]\n",
      "  [-2.04012828e+00  2.20316406e+00 -1.67029498e+00 ... -9.14073396e-01\n",
      "   -8.63059654e-01  5.36734993e-01]]\n",
      "\n",
      " [[-8.36113095e-01  3.65298055e-02 -9.40975994e-02 ... -1.91275805e-01\n",
      "   -4.69793499e-01 -1.84850693e+00]\n",
      "  [ 8.62818882e-02 -7.84198567e-02 -2.36170292e+00 ... -2.13880301e+00\n",
      "   -6.37233496e-01 -2.52398944e+00]\n",
      "  [ 2.84132928e-01 -9.19206738e-01  2.39205813e+00 ... -1.17826796e+00\n",
      "    8.03116500e-01 -1.62764430e+00]\n",
      "  ...\n",
      "  [-1.19070458e+00 -1.27899694e+00 -7.13433743e-01 ...  4.67763454e-01\n",
      "    1.76067948e+00  1.52494633e+00]\n",
      "  [ 8.93406570e-01  6.61307454e-01  4.79996771e-01 ... -4.61913615e-01\n",
      "   -4.49748218e-01  1.03902471e+00]\n",
      "  [ 5.06992996e-01 -8.78851339e-02 -4.01100904e-01 ...  1.92336261e-01\n",
      "    3.78103065e-03  5.66256464e-01]]\n",
      "\n",
      " [[-5.72186264e-01  2.05925045e+00  3.77772392e-01 ... -7.05489898e-01\n",
      "    7.82301783e-01  1.21288450e+00]\n",
      "  [ 1.08709507e+00 -2.05668476e+00 -1.14959571e+00 ... -2.74754989e+00\n",
      "    3.05949722e+00  2.85545662e-01]\n",
      "  [-2.31760341e-01 -1.32169891e+00  6.44336423e-01 ... -1.66891113e+00\n",
      "    9.82489748e-01  1.38168518e+00]\n",
      "  ...\n",
      "  [ 1.84822742e-01  1.51450245e+00  1.20553785e+00 ... -4.30322650e-01\n",
      "    1.21583707e+00 -8.03841854e-01]\n",
      "  [-2.00322475e-01  1.39677394e-01  1.98190719e+00 ...  7.84147147e-01\n",
      "    3.20261572e-01 -4.09308387e-01]\n",
      "  [ 1.60103910e+00  6.65592985e-01  8.49614019e-01 ... -5.49081716e-01\n",
      "   -5.77891317e-01  4.96586705e-01]]\n",
      "\n",
      " [[ 3.81775558e-01  2.07717624e+00  4.25649865e-01 ... -2.77336926e+00\n",
      "   -1.00350643e+00 -4.11717356e-01]\n",
      "  [ 4.81661087e-02 -6.48667874e-01 -1.17061123e+00 ... -1.33606639e-01\n",
      "   -1.25155452e-01 -6.12150866e-02]\n",
      "  [ 3.68851360e-01  1.80439503e-01 -1.13469307e+00 ...  2.86664343e-03\n",
      "    8.58061300e-01  1.00849948e+00]\n",
      "  ...\n",
      "  [-1.67249735e-01 -7.34947703e-02 -6.98441450e-01 ... -9.50890865e-01\n",
      "   -4.91909247e-01  8.25022518e-01]\n",
      "  [ 1.17341570e+00  1.06037759e+00  1.50432099e+00 ... -1.87927559e+00\n",
      "   -1.00556553e+00 -1.57336902e+00]\n",
      "  [ 8.84374977e-01 -6.53152413e-01  8.41501227e-02 ...  1.17184271e+00\n",
      "   -1.26157753e+00 -8.02920300e-01]]]\n",
      "\n",
      "隼 Primeiras 5 entradas de y_val: [1 0 0 1 1]\n",
      "\n",
      "隼 Primeiras 5 entradas de X_test: [[[ 0.31073518  1.49147194  0.20450571 ... -2.15490586 -0.78393503\n",
      "   -0.14206179]\n",
      "  [ 1.60920814  0.56399143  1.18532389 ... -1.26270372  0.7336266\n",
      "    0.33113102]\n",
      "  [-0.16805227 -0.18327243 -0.56589374 ...  0.79255337  0.13037115\n",
      "    0.82814047]\n",
      "  ...\n",
      "  [-0.13240648 -0.73908329 -0.17233754 ...  0.3469555  -0.58222995\n",
      "    1.13461701]\n",
      "  [ 0.04211923 -0.21714775 -0.04080601 ...  0.23419109 -0.81575672\n",
      "    1.6595785 ]\n",
      "  [ 1.60920814  0.56399143  1.18532389 ... -1.26270372  0.7336266\n",
      "    0.33113102]]\n",
      "\n",
      " [[ 0.23333034 -1.41361138 -0.40081709 ... -0.03001219  0.32929899\n",
      "   -0.27939233]\n",
      "  [ 0.91562323  0.2799593  -0.14045703 ... -0.69015373 -0.0868543\n",
      "   -1.43594724]\n",
      "  [ 1.31999235 -1.09374836  0.65619175 ...  0.26928298 -1.703714\n",
      "   -1.47267004]\n",
      "  ...\n",
      "  [ 0.12600167 -1.14075363  0.78846051 ...  0.23260942 -0.82054652\n",
      "    0.24767923]\n",
      "  [-1.13614879  0.46744601 -0.51371503 ... -1.28752804  1.09784694\n",
      "    0.40079067]\n",
      "  [ 2.7079576   2.01042606  1.6268681  ... -1.10565423 -2.04751634\n",
      "    0.5026138 ]]\n",
      "\n",
      " [[ 1.00459823  1.42115639 -0.8242856  ...  0.03953052 -0.33291681\n",
      "   -0.14852152]\n",
      "  [ 1.0415399   0.66900977 -0.33204481 ... -0.45398014 -1.58897585\n",
      "   -1.75543073]\n",
      "  [-1.67132741 -1.03382327  0.48974461 ...  0.49043132  1.63100138\n",
      "    0.51471801]\n",
      "  ...\n",
      "  [-0.94318669  0.4125742  -0.46187213 ... -0.58397254  0.15578954\n",
      "   -0.28981826]\n",
      "  [-0.94318669  0.4125742  -0.46187213 ... -0.58397254  0.15578954\n",
      "   -0.28981826]\n",
      "  [-0.94318669  0.4125742  -0.46187213 ... -0.58397254  0.15578954\n",
      "   -0.28981826]]\n",
      "\n",
      " [[-0.31565168  0.74812037 -1.44567182 ... -1.23782088  0.01108936\n",
      "   -1.2071148 ]\n",
      "  [ 0.10321604  1.90342073 -2.61857339 ... -2.21847683 -0.03783313\n",
      "   -1.60703328]\n",
      "  [-2.12930446  2.19376267 -2.2192251  ...  0.8453432   0.34183388\n",
      "    0.30113158]\n",
      "  ...\n",
      "  [-0.2289581  -0.37417471  0.16074128 ...  0.02693988  0.09118619\n",
      "   -0.24104185]\n",
      "  [-0.2289581  -0.37417471  0.16074128 ...  0.02693988  0.09118619\n",
      "   -0.24104185]\n",
      "  [-0.2289581  -0.37417471  0.16074128 ...  0.02693988  0.09118619\n",
      "   -0.24104185]]\n",
      "\n",
      " [[ 0.32777654 -0.23394659  0.76132416 ...  0.46271382 -0.46605935\n",
      "   -1.17619124]\n",
      "  [ 1.10040862  1.08040354  1.4239397  ... -0.11115097 -1.0008387\n",
      "   -0.63106188]\n",
      "  [-1.68789105  0.07575742 -1.13599392 ...  1.16426156 -0.21633751\n",
      "   -0.17224567]\n",
      "  ...\n",
      "  [-0.93136311 -1.3049802   1.01694164 ... -0.33269376 -0.57434398\n",
      "    0.80935123]\n",
      "  [ 0.53910152 -0.42733393  0.35254465 ...  1.35914412  0.63458974\n",
      "    1.35425812]\n",
      "  [-0.19791781  0.41994411  1.08098719 ... -0.59507456  0.32719374\n",
      "    1.46070378]]]\n",
      "\n",
      "隼 Primeiras 5 entradas de y_test: [0 0 0 0 0]\n",
      "\n",
      "隼 Primeiras 5 entradas de X_eval_final: [[[-1.3698944  -0.2157115   1.76219063 ... -0.27723305 -0.93684741\n",
      "    0.43301854]\n",
      "  [-0.75977102  0.14281783  2.35868163 ...  0.41853127 -1.95731543\n",
      "   -0.76543053]\n",
      "  [ 0.18173536  1.53569138  0.56512502 ...  0.86655806 -1.10256161\n",
      "   -1.45671297]\n",
      "  ...\n",
      "  [-0.67529361  0.01529847  0.09744965 ... -0.52706625 -0.05198459\n",
      "   -0.03508213]\n",
      "  [-0.67529361  0.01529847  0.09744965 ... -0.52706625 -0.05198459\n",
      "   -0.03508213]\n",
      "  [-0.67529361  0.01529847  0.09744965 ... -0.52706625 -0.05198459\n",
      "   -0.03508213]]\n",
      "\n",
      " [[ 1.33992974  1.64108373  0.10745565 ...  2.01993656 -0.4003484\n",
      "    2.62578819]\n",
      "  [-2.36130054 -0.20058585  0.33747326 ...  0.74288786  1.64805983\n",
      "   -1.52656221]\n",
      "  [ 0.98007853 -1.67682146 -1.00291635 ...  1.63147788  3.15270503\n",
      "    0.47318374]\n",
      "  ...\n",
      "  [-0.51123099 -0.28619765 -0.02092303 ... -0.5729669   0.16748731\n",
      "    0.16121607]\n",
      "  [-0.51123099 -0.28619765 -0.02092303 ... -0.5729669   0.16748731\n",
      "    0.16121607]\n",
      "  [-0.51123099 -0.28619765 -0.02092303 ... -0.5729669   0.16748731\n",
      "    0.16121607]]\n",
      "\n",
      " [[ 0.83333132 -1.15258005 -0.04539569 ...  0.73987953 -0.65474947\n",
      "    1.86422113]\n",
      "  [ 0.13529973  0.97321307  0.34615444 ...  1.27736879 -0.1700837\n",
      "    0.35059031]\n",
      "  [ 2.21645459 -2.85437942  0.97846938 ... -0.14005933  3.48517802\n",
      "    1.36829186]\n",
      "  ...\n",
      "  [-0.66081041 -0.0443789  -0.22066978 ... -0.44560178 -0.03793679\n",
      "   -0.19202707]\n",
      "  [-0.66081041 -0.0443789  -0.22066978 ... -0.44560178 -0.03793679\n",
      "   -0.19202707]\n",
      "  [-0.66081041 -0.0443789  -0.22066978 ... -0.44560178 -0.03793679\n",
      "   -0.19202707]]\n",
      "\n",
      " [[-2.18542984  0.18405269 -0.27029117 ...  0.60221042  0.78182237\n",
      "   -1.85795843]\n",
      "  [ 0.22582104  2.38469618 -1.55955588 ...  0.10592665 -0.27867434\n",
      "    0.36730238]\n",
      "  [ 0.70029193 -2.25855386  0.72498694 ... -0.21421168  1.63970748\n",
      "    2.49225477]\n",
      "  ...\n",
      "  [-0.76269651  0.11944859 -0.52736499 ... -0.70369525 -0.26108996\n",
      "   -0.41965596]\n",
      "  [-0.76269651  0.11944859 -0.52736499 ... -0.70369525 -0.26108996\n",
      "   -0.41965596]\n",
      "  [-0.76269651  0.11944859 -0.52736499 ... -0.70369525 -0.26108996\n",
      "   -0.41965596]]\n",
      "\n",
      " [[ 0.70437651 -1.06267669 -0.0275169  ...  0.67952184 -0.73940207\n",
      "    1.63101594]\n",
      "  [-0.8271893  -1.22553345 -0.33354711 ...  0.40106014  0.25191818\n",
      "    1.30343902]\n",
      "  [ 2.68838509 -0.47073079  0.33544774 ... -0.43545878  0.6197819\n",
      "    0.58465884]\n",
      "  ...\n",
      "  [-0.5839842  -0.22460455 -0.171416   ... -0.42425149 -0.20530454\n",
      "   -0.32931646]\n",
      "  [-0.5839842  -0.22460455 -0.171416   ... -0.42425149 -0.20530454\n",
      "   -0.32931646]\n",
      "  [-0.5839842  -0.22460455 -0.171416   ... -0.42425149 -0.20530454\n",
      "   -0.32931646]]]\n",
      "\n",
      "隼 Primeiras 5 entradas de y_eval_final: [0 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Primeiras 5 entradas do conjunto de TREINO:\")\n",
    "print(df_train.head())\n",
    "\n",
    "print(\"\\n Primeiras 5 entradas do conjunto de VALIDAﾃﾃグ:\")\n",
    "print(df_val.head())\n",
    "\n",
    "print(\"\\n Primeiras 5 entradas do conjunto de TESTE:\")\n",
    "print(df_test.head())\n",
    "\n",
    "print(\"\\n Primeiras 5 entradas do conjunto de AVALIAﾃﾃグ FINAL:\")\n",
    "print(df_eval_final.head())\n",
    "\n",
    "##################################################################\n",
    "\n",
    "print(\"\\n Primeiras 5 entradas de X_train:\", X_train[:5])\n",
    "print(\"\\n Primeiras 5 entradas de y_train:\", y_train[:5])\n",
    "\n",
    "print(\"\\n Primeiras 5 entradas de X_val:\", X_val[:5])\n",
    "print(\"\\n Primeiras 5 entradas de y_val:\", y_val[:5])\n",
    "\n",
    "print(\"\\n Primeiras 5 entradas de X_test:\", X_test[:5])\n",
    "print(\"\\n Primeiras 5 entradas de y_test:\", y_test[:5])\n",
    "\n",
    "print(\"\\n Primeiras 5 entradas de X_eval_final:\", X_eval_final[:5])\n",
    "print(\"\\n Primeiras 5 entradas de y_eval_final:\", y_eval_final[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Construﾃｧﾃ｣o do modelo RNN com cﾃｳdigo raiz (Sem TensorFlow/SKLearn)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Passo 1 - Definiﾃｧﾃ｣o da Estrutura da RNN**\n",
    "\n",
    "A nossa RNN terﾃ｡:\n",
    "- Uma camada de entrada com 100 timesteps e 50 features (nosso embedding).\n",
    "- Uma camada recorrente com neurﾃｴnios que aprendem dependﾃｪncias temporais.\n",
    "- Uma camada de saﾃｭda para classificaﾃｧﾃ｣o (sigmoid para saﾃｭda binﾃ｡ria)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Passo 2 - Inicializaﾃｧﾃ｣o de Pesos**\n",
    "\n",
    "Antes de tudo, vamos definir os pesos da rede:\n",
    "\n",
    "- W_xh: Pesa a entrada para os neurﾃｴnios recorrentes.\n",
    "- W_hh: Pesa as conexﾃｵes recorrentes.\n",
    "- W_hy: Pesa a saﾃｭda do neurﾃｴnio recorrente para a prediﾃｧﾃ｣o final.\n",
    "- b_h e b_y: Bias da camada oculta e da saﾃｭda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos e Biases inicializados!\n"
     ]
    }
   ],
   "source": [
    "# Definir hiperparﾃ｢metros\n",
    "input_size = 50    # Dimensﾃ｣o dos embeddings\n",
    "hidden_size = 64   # Nﾃｺmero de neurﾃｴnios na camada oculta\n",
    "output_size = 1    # Saﾃｭda binﾃ｡ria (0 ou 1)\n",
    "learning_rate = 0.01  \n",
    "\n",
    "# Inicializar pesos\n",
    "np.random.seed(42)  # Para reprodutibilidade\n",
    "W_xh = np.random.randn(input_size, hidden_size) * 0.01  # Pesos da entrada para a camada oculta\n",
    "W_hh = np.random.randn(hidden_size, hidden_size) * 0.01 # Pesos da camada oculta para ela mesma\n",
    "W_hy = np.random.randn(hidden_size, output_size) * 0.01 # Pesos da camada oculta para saﾃｭda\n",
    "\n",
    "# Bias\n",
    "b_h = np.zeros((1, hidden_size))\n",
    "b_y = np.zeros((1, output_size))\n",
    "\n",
    "print(\"Pesos e Biases inicializados!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Passo 3 - Forward Propagation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sigmoid(x):\n",
    "#     return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# def forward_rnn(X):\n",
    "#     \"\"\"\n",
    "#     Executa a propagaﾃｧﾃ｣o para frente da RNN\n",
    "#     X: (batch_size, sequence_length, input_size) -> (21, 100, 50)\n",
    "#     \"\"\"\n",
    "#     batch_size, seq_length, _ = X.shape\n",
    "#     h_t = np.zeros((batch_size, hidden_size))  # Inicializar estados ocultos com 0\n",
    "\n",
    "#     # Percorrer a sequﾃｪncia temporal\n",
    "#     for t in range(seq_length):\n",
    "#         x_t = X[:, t, :]  # Pegar o vetor de embeddings no timestep t\n",
    "#         h_t = np.tanh(np.dot(x_t, W_xh) + np.dot(h_t, W_hh) + b_h)  # Atualizar estado oculto\n",
    "\n",
    "#     # Cﾃ｡lculo da saﾃｭda final\n",
    "#     y_pred = sigmoid(np.dot(h_t, W_hy) + b_y)\n",
    "#     return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Passo 4 - Funﾃｧﾃ｣o de Custo (Binary Cross-Entropy)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    y_pred = np.clip(y_pred, 1e-8, 1 - 1e-8)  # 隼 Evita log(0) ou log(1)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)) / y_pred.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Passo 5 - Backpropagation Through Time (BPTT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def backward_rnn(X, y_true, y_pred, h_t):\n",
    "#     \"\"\"\n",
    "#     Executa o cﾃ｡lculo dos gradientes (Backpropagation Through Time)\n",
    "#     \"\"\"\n",
    "#     global W_xh, W_hh, W_hy, b_h, b_y  # 屏ｸ Agora garantimos que estamos modificando as variﾃ｡veis globais\n",
    "\n",
    "#     batch_size, seq_length, _ = X.shape\n",
    "\n",
    "#     # Gradiente da saﾃｭda\n",
    "#     d_y = (y_pred - y_true) / batch_size  \n",
    "\n",
    "#     # Gradientes dos pesos\n",
    "#     d_W_hy = np.dot(h_t.T, d_y)\n",
    "#     d_b_y = np.sum(d_y, axis=0, keepdims=True)\n",
    "\n",
    "#     # Inicializar gradientes da camada oculta\n",
    "#     d_h_t = np.dot(d_y, W_hy.T) * (1 - h_t**2)  # Derivada da tanh\n",
    "\n",
    "#     d_W_xh = np.zeros_like(W_xh)\n",
    "#     d_W_hh = np.zeros_like(W_hh)\n",
    "#     d_b_h = np.zeros_like(b_h)\n",
    "\n",
    "#     # Backpropagation pela sequﾃｪncia temporal\n",
    "#     for t in reversed(range(seq_length)):\n",
    "#         x_t = X[:, t, :]\n",
    "#         d_W_xh += np.dot(x_t.T, d_h_t)\n",
    "#         d_W_hh += np.dot(h_t.T, d_h_t)\n",
    "#         d_b_h += np.sum(d_h_t, axis=0, keepdims=True)\n",
    "\n",
    "#         d_h_t = np.dot(d_h_t, W_hh.T) * (1 - h_t**2)\n",
    "\n",
    "#     # Atualizar pesos\n",
    "#     W_xh -= learning_rate * d_W_xh\n",
    "#     W_hh -= learning_rate * d_W_hh\n",
    "#     W_hy -= learning_rate * d_W_hy\n",
    "#     b_h -= learning_rate * d_b_h\n",
    "#     b_y -= learning_rate * d_b_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mini-Batches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "def get_mini_batches(X, y, batch_size=16, shuffle=True):\n",
    "    \"\"\"Divide os dados em mini-batches.\"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    indices = np.arange(n_samples)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        yield X[indices[start:end]], y[indices[start:end]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Otimizaﾃｧﾃ｣o de Hiperparﾃ｢metros**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "剥 Testando hiperparﾃ｢metros: {'epochs': 5, 'batch_size': 8, 'learning_rate': 0.01, 'momentum': 0.9, 'bptt_trunc': 2}\n",
      "暑ｸ ﾃ英oca 1/5 - Loss: 224.4958\n",
      "暑ｸ ﾃ英oca 2/5 - Loss: 199.5008\n",
      "暑ｸ ﾃ英oca 3/5 - Loss: 167.2965\n",
      "暑ｸ ﾃ英oca 4/5 - Loss: 148.1730\n",
      "暑ｸ ﾃ英oca 5/5 - Loss: 136.8105\n",
      "Formato de preds: (4500,)\n",
      "投 Accuracy com esses hiperparﾃ｢metros: 0.8436\n",
      "\n",
      "剥 Testando hiperparﾃ｢metros: {'epochs': 10, 'batch_size': 16, 'learning_rate': 0.005, 'momentum': 0.95, 'bptt_trunc': 3}\n",
      "暑ｸ ﾃ英oca 1/10 - Loss: 57.3542\n",
      "暑ｸ ﾃ英oca 2/10 - Loss: 56.8938\n",
      "暑ｸ ﾃ英oca 3/10 - Loss: 56.1153\n",
      "暑ｸ ﾃ英oca 4/10 - Loss: 54.2285\n",
      "暑ｸ ﾃ英oca 5/10 - Loss: 51.2881\n",
      "暑ｸ ﾃ英oca 6/10 - Loss: 47.4896\n",
      "暑ｸ ﾃ英oca 7/10 - Loss: 43.2062\n",
      "暑ｸ ﾃ英oca 8/10 - Loss: 39.8838\n",
      "暑ｸ ﾃ英oca 9/10 - Loss: 37.6044\n",
      "暑ｸ ﾃ英oca 10/10 - Loss: 35.8057\n",
      "Formato de preds: (4500,)\n",
      "投 Accuracy com esses hiperparﾃ｢metros: 0.8167\n",
      "\n",
      "剥 Testando hiperparﾃ｢metros: {'epochs': 7, 'batch_size': 8, 'learning_rate': 0.007, 'momentum': 0.8, 'bptt_trunc': 2}\n",
      "暑ｸ ﾃ英oca 1/7 - Loss: 227.8933\n",
      "暑ｸ ﾃ英oca 2/7 - Loss: 218.3796\n",
      "暑ｸ ﾃ英oca 3/7 - Loss: 199.4865\n",
      "暑ｸ ﾃ英oca 4/7 - Loss: 178.8309\n",
      "暑ｸ ﾃ英oca 5/7 - Loss: 157.7335\n",
      "暑ｸ ﾃ英oca 6/7 - Loss: 145.4708\n",
      "暑ｸ ﾃ英oca 7/7 - Loss: 139.3214\n",
      "Formato de preds: (4500,)\n",
      "投 Accuracy com esses hiperparﾃ｢metros: 0.8400\n",
      "\n",
      "笨 Melhor combinaﾃｧﾃ｣o encontrada: {'epochs': 5, 'batch_size': 8, 'learning_rate': 0.01, 'momentum': 0.9, 'bptt_trunc': 2} com accuracy 0.8436\n"
     ]
    }
   ],
   "source": [
    "# Funﾃｧﾃ｣o de ativaﾃｧﾃ｣o Sigmoid\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivada da funﾃｧﾃ｣o Sigmoid (ﾃｺtil para backpropagation)\n",
    "def sigmoid_derivative(x):\n",
    "    sig = sigmoid(x)\n",
    "    return sig * (1 - sig)\n",
    "\n",
    "\n",
    "# Definir pesos corretamente (Xavier Initialization)\n",
    "W_xh = np.random.randn(input_size, hidden_size) * np.sqrt(1. / input_size)\n",
    "W_hh = np.random.randn(hidden_size, hidden_size) * np.sqrt(1. / hidden_size)\n",
    "W_hy = np.random.randn(hidden_size, output_size) * np.sqrt(1. / hidden_size)\n",
    "\n",
    "HYPERPARAMS = [\n",
    "    {\"epochs\": 5, \"batch_size\": 8, \"learning_rate\": 0.01, \"momentum\": 0.9, \"bptt_trunc\": 2},\n",
    "    {\"epochs\": 10, \"batch_size\": 16, \"learning_rate\": 0.005, \"momentum\": 0.95, \"bptt_trunc\": 3},\n",
    "    {\"epochs\": 7, \"batch_size\": 8, \"learning_rate\": 0.007, \"momentum\": 0.8, \"bptt_trunc\": 2},\n",
    "]\n",
    "\n",
    "best_accuracy = 0\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "# Testando hiperparﾃ｢metros\n",
    "for params in HYPERPARAMS:\n",
    "    print(f\"\\nTestando hiperparﾃ｢metros: {params}\")\n",
    "\n",
    "    rnn = RNN(\n",
    "        n_units=20,\n",
    "        # activation=ReLUActivation(),\n",
    "        activation=TanhActivation(),\n",
    "        bptt_trunc=params[\"bptt_trunc\"],\n",
    "        input_shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM),\n",
    "        epochs=params[\"epochs\"],\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        momentum=params[\"momentum\"],\n",
    "        loss=BinaryCrossEntropy,\n",
    "        metric=accuracy\n",
    "    )\n",
    "\n",
    "    optimizer = Optimizer(learning_rate=params[\"learning_rate\"])\n",
    "    rnn.initialize(optimizer)\n",
    "\n",
    "    for epoch in range(params[\"epochs\"]):\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in get_mini_batches(X_train, y_train, params[\"batch_size\"]):\n",
    "            y_pred = rnn.forward_propagation(X_batch)\n",
    "            #y_pred_final = y_pred[:, -1, :]  # 隼 Selecionar apenas a ﾃｺltima saﾃｭda\n",
    "            y_pred_final = sigmoid(y_pred[:, -1, :])  # Aplica Sigmoid na ﾃｺltima saﾃｭda\n",
    "\n",
    "            loss = binary_cross_entropy(y_batch.reshape(-1, 1), y_pred_final)\n",
    "            grad_loss = (y_pred_final - y_batch.reshape(-1, 1)) / y_batch.shape[0]\n",
    "\n",
    "            grad_loss_expanded = np.zeros_like(y_pred)\n",
    "            grad_loss_expanded[:, -1, :] = grad_loss\n",
    "\n",
    "            rnn.backward_propagation(grad_loss_expanded)\n",
    "\n",
    "            total_loss += loss\n",
    "\n",
    "        print(f\"ﾃ英oca {epoch+1}/{params['epochs']} - Loss: {total_loss:.4f}\")\n",
    "\n",
    "    # Avaliaﾃｧﾃ｣o\n",
    "    preds = rnn.predict(X_val)\n",
    "    \n",
    "    # Debug do formato de `preds`\n",
    "    print(f\"Formato de preds: {preds.shape}\")\n",
    "\n",
    "    # Corrigir caso `preds` seja 1D\n",
    "    if preds.ndim == 1:\n",
    "        preds = preds[:, np.newaxis]\n",
    "\n",
    "    acc = accuracy(y_val, preds)\n",
    "\n",
    "    print(f\"Accuracy com esses hiperparﾃ｢metros: {acc:.4f}\")\n",
    "\n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        best_params = params\n",
    "        best_model = rnn\n",
    "\n",
    "print(f\"\\nMelhor combinaﾃｧﾃ｣o encontrada: {best_params} com accuracy {best_accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Passo 6 - Treinar o Modelo Final**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "暑ｸ Treino final - ﾃ英oca 1/5 - Loss: 228.0248\n",
      "暑ｸ Treino final - ﾃ英oca 2/5 - Loss: 216.9531\n",
      "暑ｸ Treino final - ﾃ英oca 3/5 - Loss: 187.3148\n",
      "暑ｸ Treino final - ﾃ英oca 4/5 - Loss: 167.3577\n",
      "暑ｸ Treino final - ﾃ英oca 5/5 - Loss: 151.5369\n",
      "Formato de y_test_pred: (4500,)\n",
      "\n",
      "識 Accuracy final no conjunto de teste: 0.8378\n",
      "\n",
      "投 Comparaﾃｧﾃ｣o entre valores esperados e previstos:\n",
      "      expected_value  predicted_value_raw  predicted_value\n",
      "0                  0                  0.0                0\n",
      "1                  0                  1.0                1\n",
      "2                  0                  0.0                0\n",
      "3                  0                  0.0                0\n",
      "4                  0                  0.0                0\n",
      "...              ...                  ...              ...\n",
      "4495               0                  0.0                0\n",
      "4496               1                  1.0                1\n",
      "4497               1                  1.0                1\n",
      "4498               1                  1.0                1\n",
      "4499               0                  0.0                0\n",
      "\n",
      "[4500 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "final_rnn = RNN(\n",
    "    n_units=20,\n",
    "    # activation=ReLUActivation(),\n",
    "    activation=TanhActivation(),\n",
    "    bptt_trunc=best_params[\"bptt_trunc\"],\n",
    "    input_shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM),\n",
    "    epochs=best_params[\"epochs\"],\n",
    "    batch_size=best_params[\"batch_size\"],\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    momentum=best_params[\"momentum\"],\n",
    "    loss=BinaryCrossEntropy,\n",
    "    metric=accuracy\n",
    ")\n",
    "\n",
    "final_optimizer = Optimizer(learning_rate=best_params[\"learning_rate\"])\n",
    "final_rnn.initialize(final_optimizer)\n",
    "\n",
    "for epoch in range(best_params[\"epochs\"]):\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in get_mini_batches(X_train, y_train, best_params[\"batch_size\"]):\n",
    "        y_pred = final_rnn.forward_propagation(X_batch)\n",
    "        #y_pred_final = y_pred[:, -1, :]  # 隼 Pegamos apenas a ﾃｺltima saﾃｭda da sequﾃｪncia\n",
    "        y_pred_final = sigmoid(y_pred[:, -1, :])  # Aplica Sigmoid na ﾃｺltima saﾃｭda\n",
    "\n",
    "        loss = binary_cross_entropy(y_batch.reshape(-1, 1), y_pred_final)\n",
    "\n",
    "        # Calcular o gradiente correto\n",
    "        grad_loss = (y_pred_final - y_batch.reshape(-1, 1)) / y_batch.shape[0]\n",
    "\n",
    "        # Expandir para 3 dimensﾃｵes para ser compatﾃｭvel com a RNN\n",
    "        grad_loss_expanded = np.zeros_like(y_pred)  # (batch_size, timesteps, output_size)\n",
    "        grad_loss_expanded[:, -1, :] = grad_loss  # Apenas o ﾃｺltimo timestep recebe gradiente\n",
    "\n",
    "        # Passar o gradiente expandido\n",
    "        final_rnn.backward_propagation(grad_loss_expanded)\n",
    "\n",
    "        total_loss += loss\n",
    "\n",
    "    print(f\"Treino final - ﾃ英oca {epoch+1}/{best_params['epochs']} - Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Testar Modelo Final\n",
    "y_test_pred = final_rnn.predict(X_test)\n",
    "\n",
    "print(f\"Formato de y_test_pred: {y_test_pred.shape}\")  # 屏ｸ Debug\n",
    "\n",
    "# Se for 1D, expandimos para 2D\n",
    "if y_test_pred.ndim == 1:\n",
    "    y_test_pred = y_test_pred[:, np.newaxis]\n",
    "\n",
    "# Se for 2D (batch_size, timesteps), pegamos o ﾃｺltimo timestep\n",
    "if y_test_pred.ndim == 2:\n",
    "    y_test_pred_final = y_test_pred[:, -1]  #  Sem `:` no final, pois jﾃ｡ ﾃｩ 1D\n",
    "else:\n",
    "    y_test_pred_final = y_test_pred[:, -1, :]  #  Apenas se for 3D\n",
    "\n",
    "y_test_pred_labels = (y_test_pred_final > 0.5).astype(int)\n",
    "\n",
    "y_test_true = y_test.flatten()\n",
    "accuracy = np.mean(y_test_pred_labels == y_test_true)\n",
    "print(f\"\\nAccuracy final no conjunto de teste: {accuracy:.4f}\")\n",
    "\n",
    "# Criar DataFrame com Expected vs Predicted\n",
    "df_results = pd.DataFrame({\n",
    "    \"expected_value\": y_test_true,\n",
    "    \"predicted_value_raw\": y_test_pred_final.flatten(),  # Valor original antes do arredondamento\n",
    "    \"predicted_value\": y_test_pred_labels.flatten()  # Valor final binﾃ｡rio (0 ou 1)\n",
    "})\n",
    "\n",
    "# Mostrar as previsﾃｵes para comparaﾃｧﾃ｣o\n",
    "print(\"\\nComparaﾃｧﾃ｣o entre valores esperados e previstos:\")\n",
    "print(df_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Passo 7 - Avaliaﾃｧﾃ｣o do Modelo com dados do Dataset1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formato de y_test_pred2: (4053,)\n",
      "\n",
      "識 Accuracy final no conjunto de teste: 0.4917\n",
      "\n",
      "投 Comparaﾃｧﾃ｣o entre valores esperados e previstos:\n",
      "      expected_value  predicted_value_raw  predicted_value\n",
      "0                  0                  0.0                0\n",
      "1                  1                  0.0                0\n",
      "2                  0                  0.0                0\n",
      "3                  1                  1.0                1\n",
      "4                  0                  0.0                0\n",
      "...              ...                  ...              ...\n",
      "4048               1                  0.0                0\n",
      "4049               0                  0.0                0\n",
      "4050               1                  0.0                0\n",
      "4051               0                  0.0                0\n",
      "4052               1                  0.0                0\n",
      "\n",
      "[4053 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# X_eval_final, y_eval_final\n",
    "\n",
    "\n",
    "# Testar Modelo Final\n",
    "y_test_pred2 = final_rnn.predict(X_eval_final)\n",
    "\n",
    "print(f\"Formato de y_test_pred2: {y_test_pred2.shape}\")  # 屏ｸ Debug\n",
    "\n",
    "# Se for 1D, expandimos para 2D\n",
    "if y_test_pred2.ndim == 1:\n",
    "    y_test_pred2 = y_test_pred2[:, np.newaxis]\n",
    "\n",
    "# Se for 2D (batch_size, timesteps), pegamos o ﾃｺltimo timestep\n",
    "if y_test_pred2.ndim == 2:\n",
    "    y_test_pred_final2 = y_test_pred2[:, -1]  #  Sem `:` no final, pois jﾃ｡ ﾃｩ 1D\n",
    "else:\n",
    "    y_test_pred_final2 = y_test_pred2[:, -1, :]  #  Apenas se for 3D\n",
    "\n",
    "y_test_pred_labels2 = (y_test_pred_final2 > 0.5).astype(int)\n",
    "\n",
    "y_test_true2 = y_eval_final.flatten()\n",
    "accuracy = np.mean(y_test_pred_labels2 == y_test_true2)\n",
    "print(f\"\\nAccuracy final no conjunto de teste: {accuracy:.4f}\")\n",
    "\n",
    "# Criar DataFrame com Expected vs Predicted\n",
    "df_results = pd.DataFrame({\n",
    "    \"expected_value\": y_test_true2,\n",
    "    \"predicted_value_raw\": y_test_pred_final2.flatten(),  # Valor original antes do arredondamento\n",
    "    \"predicted_value\": y_test_pred_labels2.flatten()  # Valor final binﾃ｡rio (0 ou 1)\n",
    "})\n",
    "\n",
    "# Mostrar as previsﾃｵes para comparaﾃｧﾃ｣o\n",
    "print(\"\\nComparaﾃｧﾃ｣o entre valores esperados e previstos:\")\n",
    "print(df_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Anﾃ｡lise de resultados**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Treino com dataset: gpt_vs_human**\n",
    "\n",
    "- Durante o treino: 0.87 - 0.9\n",
    "\n",
    "- Para dataset1: 0.66\n",
    "\n",
    "- Para dataset2: 0.8 - 1.0\n",
    "\n",
    "- Para ai_human: 0.51\n",
    "\n",
    "**Treino com dataset: ai_human**\n",
    "\n",
    "- Durante o treino: 0.81 - 0.84\n",
    "\n",
    "- Para gpt_vs_human: 0.49"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **To Do**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sugestﾃ｣o do professor:\n",
    "- embedding com one hot encoding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
