{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e117c95a",
      "metadata": {},
      "source": [
        "# DNN Model Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f7f3362",
      "metadata": {},
      "source": [
        "```md\n",
        "@author: miguelrocha\n",
        "(Adapted by: Grupo 03)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "00553163",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Notebook Imports\n",
        "import numpy as np\n",
        "\n",
        "from helpers.dataset import Dataset\n",
        "from helpers.enums import ModelRunMode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "fc6f6268",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model run mode\n",
        "# Options: \n",
        "#   ModelRunMode.TRAIN.value            (Train the model)\n",
        "#   ModelRunMode.CLASSIFY.value         (Classify data)\n",
        "mode = ModelRunMode.TRAIN.value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d6adb728",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters cell\n",
        "if mode == ModelRunMode.TRAIN.value:\n",
        "    # Train mode\n",
        "    input_csv = '../tarefa_1/clean_input_datasets/ai_human_input_sm.csv'\n",
        "    output_csv = '../tarefa_1/clean_output_datasets/ai_human_output_sm.csv'         # CSV for training output (ID, Label)\n",
        "    test_size = 0.2                                                                 # Proportion of the dataset to use as test data\n",
        "    # epochs = 100                                                                    # Number of epochs for training\n",
        "    # learning_rate = 0.001                                                           # Learning rate for gradient descent\n",
        "    # momentum = 0.9                                                                  # Momentum for gradient descent\n",
        "    # verbose = 1                                                                     # Verbosity level for training\n",
        "    # batch_size = 32                                                                 # Batch size for gradient descent\n",
        "elif mode == ModelRunMode.CLASSIFY.value:\n",
        "    # Classify mode\n",
        "    input_csv = \"../tarefa_1/clean_input_datasets/dataset2_inputs.csv\"              # CSV for training input (ID, Text)\n",
        "    output_csv = \"../tarefa_1/classify_output_datasets/dataset2_outputs.csv\"        # CSV for predictions output\n",
        "    model_prefix = \"logreg_model\"                                                   # Prefix for loading the model files\n",
        "else:\n",
        "    print(\"The selected option is not valid. Options: \\\"train\\\" or \\\"classify\\\"!\")\n",
        "    SystemExit()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8ee00a6",
      "metadata": {},
      "source": [
        "# DNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "88a3ceab",
      "metadata": {},
      "outputs": [],
      "source": [
        "from helpers.layers import DenseLayer\n",
        "from helpers.activation import SigmoidActivation, ReLUActivation\n",
        "from helpers.losses import LossFunction, MeanSquaredError, BinaryCrossEntropy\n",
        "from helpers.optimizer import Optimizer\n",
        "from helpers.metrics import accuracy, mse\n",
        "from helpers.dataset import Dataset\n",
        "from helpers.regularizer import L1Regularizer\n",
        "\n",
        "class NeuralNetwork:\n",
        " \n",
        "    def __init__(self, epochs = 100, batch_size = 128, optimizer: Optimizer = None, verbose = False, loss: LossFunction = MeanSquaredError, metric:callable = mse):\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.optimizer = optimizer\n",
        "        self.verbose = verbose\n",
        "        self.loss = loss()\n",
        "        self.metric = metric\n",
        "\n",
        "        # attributes\n",
        "        self.layers = []\n",
        "        self.history = {}\n",
        "\n",
        "    def add(self, layer):\n",
        "        if self.layers:\n",
        "            layer.set_input_shape(input_shape=self.layers[-1].output_shape())\n",
        "        if hasattr(layer, 'initialize'):\n",
        "            layer.initialize(self.optimizer)\n",
        "        self.layers.append(layer)\n",
        "        return self\n",
        "\n",
        "    def get_mini_batches(self, X, y = None,shuffle = True):\n",
        "        n_samples = X.shape[0]\n",
        "        indices = np.arange(n_samples)\n",
        "        assert self.batch_size <= n_samples, \"Batch size cannot be greater than the number of samples\"\n",
        "        if shuffle:\n",
        "            np.random.shuffle(indices)\n",
        "        for start in range(0, n_samples - self.batch_size + 1, self.batch_size):\n",
        "            if y is not None:\n",
        "                yield X[indices[start:start + self.batch_size]], y[indices[start:start + self.batch_size]]\n",
        "            else:\n",
        "                yield X[indices[start:start + self.batch_size]], None\n",
        "\n",
        "    def forward_propagation(self, X, training):\n",
        "        output = X\n",
        "        for layer in self.layers:\n",
        "            output = layer.forward_propagation(output, training)\n",
        "        return output\n",
        "\n",
        "    def backward_propagation(self, output_error):\n",
        "        error = output_error\n",
        "        for layer in reversed(self.layers):\n",
        "            error = layer.backward_propagation(error)\n",
        "        return error\n",
        "\n",
        "    def fit(self, dataset):\n",
        "        X = dataset.X\n",
        "        y = dataset.Y\n",
        "        if np.ndim(y) == 1:\n",
        "            y = np.expand_dims(y, axis=1)\n",
        "\n",
        "        self.history = {}\n",
        "        for epoch in range(1, self.epochs + 1):\n",
        "            # store mini-batch data for epoch loss and quality metrics calculation\n",
        "            output_x_ = []\n",
        "            y_ = []\n",
        "            for X_batch, y_batch in self.get_mini_batches(X, y):\n",
        "                # Forward propagation\n",
        "                output = self.forward_propagation(X_batch, training=True)\n",
        "                # Backward propagation\n",
        "                error = self.loss.derivative(y_batch, output)\n",
        "                self.backward_propagation(error)\n",
        "\n",
        "                output_x_.append(output)\n",
        "                y_.append(y_batch)\n",
        "\n",
        "            output_x_all = np.concatenate(output_x_)\n",
        "            y_all = np.concatenate(y_)\n",
        "\n",
        "            # compute loss\n",
        "            loss = self.loss.loss(y_all, output_x_all)\n",
        "\n",
        "            if self.metric is not None:\n",
        "                metric = self.metric(y_all, output_x_all)\n",
        "                metric_s = f\"{self.metric.__name__}: {metric:.4f}\"\n",
        "            else:\n",
        "                metric_s = \"NA\"\n",
        "                metric = 'NA'\n",
        "\n",
        "            # save loss and metric for each epoch\n",
        "            self.history[epoch] = {'loss': loss, 'metric': metric}\n",
        "\n",
        "            if self.verbose:\n",
        "                print(f\"Epoch {epoch}/{self.epochs} - loss: {loss:.4f} - {metric_s}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, dataset):\n",
        "        if isinstance(dataset, Dataset):\n",
        "            return self.forward_propagation(dataset.X, training=False)\n",
        "        \n",
        "        return self.forward_propagation(dataset, training=False)\n",
        "\n",
        "    def score(self, dataset, predictions):\n",
        "        if self.metric is not None:\n",
        "            return self.metric(dataset.Y, predictions)\n",
        "        else:\n",
        "            raise ValueError(\"No metric specified for the neural network.\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38c98de9",
      "metadata": {},
      "source": [
        "# Hiper Params fine-tunning function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b606fbc0",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "def hyperparameter_optimization(train_ds, validation_ds, param_grid, n_iter=10):\n",
        "    '''\n",
        "    Make a random search for hyperparameters optimization.\n",
        "    '''\n",
        "    best_acc = 0\n",
        "    best_params = {}\n",
        "    \n",
        "    param_combinations = [\n",
        "        [random.choice(values) for _, values in param_grid.items()]\n",
        "        for _ in range(n_iter)\n",
        "    ]\n",
        "    \n",
        "    for params in tqdm(param_combinations):\n",
        "        param_dict = dict(zip(param_grid.keys(), params))\n",
        "        \n",
        "        net = NeuralNetwork(\n",
        "            epochs=param_dict['epochs'],\n",
        "            batch_size=param_dict['batch_size'],\n",
        "            optimizer=Optimizer(learning_rate=param_dict['learning_rate'], momentum=param_dict['momentum']),\n",
        "            verbose=False,\n",
        "            loss=BinaryCrossEntropy,\n",
        "            metric=accuracy\n",
        "        )\n",
        "        \n",
        "        n_features = train_ds.X.shape[1]\n",
        "        \n",
        "        for i, units in enumerate(param_dict['n_hidden']):\n",
        "            if i == 0:\n",
        "                net.add(DenseLayer(units, (n_features,),dropout_rate=param_dict['dropout_rate']))\n",
        "            else:\n",
        "                net.add(DenseLayer(units,dropout_rate=param_dict['dropout_rate']))\n",
        "            net.add(ReLUActivation())\n",
        "            \n",
        "        net.add(DenseLayer(1))\n",
        "        net.add(SigmoidActivation())\n",
        "        \n",
        "        \n",
        "        net.fit(train_ds)\n",
        "        out = net.predict(validation_ds)\n",
        "        val_acc = net.score(validation_ds,out)\n",
        "        \n",
        "        print(\"Hiperparâmetros:\", param_dict, end=\" \")\n",
        "        print(\"Acurácia:\", val_acc)\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            best_params = param_dict\n",
        "    \n",
        "    print(\"Melhores Hiperparâmetros:\", best_params)\n",
        "    print(\"Melhor Acurácia:\", best_acc)\n",
        "    return best_params\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8411ba42",
      "metadata": {},
      "source": [
        "# Preparing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "32c0bdc1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded\n",
            "Train set has 19200 rows and 4000 columns\n",
            "Validation set has 4800 rows and 4000 columns\n",
            "Test set has 6000 rows and 4000 columns\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load Datasets\n",
        "X_train, y_train, X_test, y_test, vocab = Dataset.prepare_train_test_bow(input_csv=input_csv, output_csv=output_csv, test_size=test_size, max_vocab_size=4000, min_freq=1)\n",
        "\n",
        "# Wrap Dataset object\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "train_ds = Dataset(X=X_train, Y=y_train)\n",
        "validation_ds = Dataset(X=X_val, Y=y_val)\n",
        "test_ds = Dataset(X=X_test, Y=y_test)\n",
        "\n",
        "print(\"Dataset loaded\")\n",
        "print(f\"Train set has {train_ds.nrows()} rows and {train_ds.ncols()} columns\")\n",
        "print(f\"Validation set has {validation_ds.nrows()} rows and {validation_ds.ncols()} columns\")\n",
        "print(f\"Test set has {test_ds.nrows()} rows and {test_ds.ncols()} columns\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c89ea38b",
      "metadata": {},
      "source": [
        "# Fine-Tunning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "11094315",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 1/10 [01:49<16:29, 109.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hiperparâmetros: {'epochs': 40, 'n_hidden': [100, 50], 'dropout_rate': 0.1, 'learning_rate': 0.001, 'momentum': 0.5, 'batch_size': 64} Acurácia: 0.9885416666666667\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 2/10 [02:19<08:22, 62.81s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hiperparâmetros: {'epochs': 20, 'n_hidden': [50, 25], 'dropout_rate': 0.1, 'learning_rate': 0.001, 'momentum': 0.5, 'batch_size': 64} Acurácia: 0.9845833333333334\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 3/10 [03:13<06:51, 58.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hiperparâmetros: {'epochs': 20, 'n_hidden': [100], 'dropout_rate': 0.1, 'learning_rate': 0.001, 'momentum': 0.9, 'batch_size': 64} Acurácia: 0.9852083333333334\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 4/10 [03:43<04:43, 47.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hiperparâmetros: {'epochs': 20, 'n_hidden': [50, 25], 'dropout_rate': 0.5, 'learning_rate': 0.01, 'momentum': 0.9, 'batch_size': 64} Acurácia: 0.99125\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 5/10 [04:12<03:24, 40.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hiperparâmetros: {'epochs': 20, 'n_hidden': [50, 25], 'dropout_rate': 0.1, 'learning_rate': 0.001, 'momentum': 0.9, 'batch_size': 64} Acurácia: 0.984375\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 6/10 [06:11<04:28, 67.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hiperparâmetros: {'epochs': 40, 'n_hidden': [100], 'dropout_rate': 0.1, 'learning_rate': 0.001, 'momentum': 0.5, 'batch_size': 64} Acurácia: 0.9889583333333334\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 7/10 [07:11<03:14, 64.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hiperparâmetros: {'epochs': 20, 'n_hidden': [100], 'dropout_rate': 0.1, 'learning_rate': 0.005, 'momentum': 0.1, 'batch_size': 64} Acurácia: 0.9872916666666667\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/bdlemos/Projetos/AP-TPG-M1/tarefa_2/helpers/activation.py:39: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-input))\n",
            " 80%|████████  | 8/10 [09:16<02:48, 84.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hiperparâmetros: {'epochs': 40, 'n_hidden': [100, 50], 'dropout_rate': 0.1, 'learning_rate': 0.01, 'momentum': 0.5, 'batch_size': 64} Acurácia: 0.4685416666666667\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 9/10 [10:17<01:16, 76.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hiperparâmetros: {'epochs': 40, 'n_hidden': [50, 25], 'dropout_rate': 0.1, 'learning_rate': 0.01, 'momentum': 0.5, 'batch_size': 64} Acurácia: 0.9902083333333334\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [10:46<00:00, 64.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hiperparâmetros: {'epochs': 20, 'n_hidden': [50, 25], 'dropout_rate': 0.5, 'learning_rate': 0.01, 'momentum': 0.9, 'batch_size': 64} Acurácia: 0.9922916666666667\n",
            "Melhores Hiperparâmetros: {'epochs': 20, 'n_hidden': [50, 25], 'dropout_rate': 0.5, 'learning_rate': 0.01, 'momentum': 0.9, 'batch_size': 64}\n",
            "Melhor Acurácia: 0.9922916666666667\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "param_grid = {\n",
        "'epochs': [20, 40],\n",
        "'n_hidden': [[50, 25], [100, 50], [100]],\n",
        "'dropout_rate': [0.1, 0.5],\n",
        "'learning_rate': [0.01, 0.001, 0.005],\n",
        "'momentum': [0.9, 0.5,0.1],\n",
        "'batch_size': [64]\n",
        "}\n",
        "\n",
        "best_params = hyperparameter_optimization(train_ds,validation_ds, param_grid, 10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7985d495",
      "metadata": {},
      "source": [
        "# Training with best params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "544f6a1f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20 - loss: 0.4891 - accuracy: 0.7647\n",
            "Epoch 2/20 - loss: 0.1992 - accuracy: 0.9295\n",
            "Epoch 3/20 - loss: 0.1329 - accuracy: 0.9557\n",
            "Epoch 4/20 - loss: 0.0987 - accuracy: 0.9653\n",
            "Epoch 5/20 - loss: 0.0824 - accuracy: 0.9729\n",
            "Epoch 6/20 - loss: 0.0629 - accuracy: 0.9791\n",
            "Epoch 7/20 - loss: 0.0590 - accuracy: 0.9808\n",
            "Epoch 8/20 - loss: 0.0478 - accuracy: 0.9844\n",
            "Epoch 9/20 - loss: 0.0461 - accuracy: 0.9850\n",
            "Epoch 10/20 - loss: 0.0372 - accuracy: 0.9877\n",
            "Epoch 11/20 - loss: 0.0357 - accuracy: 0.9871\n",
            "Epoch 12/20 - loss: 0.0305 - accuracy: 0.9901\n",
            "Epoch 13/20 - loss: 0.0286 - accuracy: 0.9903\n",
            "Epoch 14/20 - loss: 0.0262 - accuracy: 0.9916\n",
            "Epoch 15/20 - loss: 0.0243 - accuracy: 0.9927\n",
            "Epoch 16/20 - loss: 0.0201 - accuracy: 0.9930\n",
            "Epoch 17/20 - loss: 0.0215 - accuracy: 0.9940\n",
            "Epoch 18/20 - loss: 0.0194 - accuracy: 0.9935\n",
            "Epoch 19/20 - loss: 0.0184 - accuracy: 0.9938\n",
            "Epoch 20/20 - loss: 0.0154 - accuracy: 0.9946\n",
            "Test Accuracy: 0.9908333333333333\n"
          ]
        }
      ],
      "source": [
        "net = NeuralNetwork(\n",
        "    epochs=best_params['epochs'],\n",
        "    batch_size=best_params['batch_size'],\n",
        "    optimizer=Optimizer(learning_rate=best_params['learning_rate'], momentum=best_params['momentum']),\n",
        "    verbose=True,\n",
        "    loss=BinaryCrossEntropy,\n",
        "    metric=accuracy\n",
        ")\n",
        "\n",
        "n_features = train_ds.X.shape[1]\n",
        "for i, units in enumerate(best_params['n_hidden']):\n",
        "    if i == 0:\n",
        "        net.add(DenseLayer(units, (n_features,),dropout_rate=best_params['dropout_rate']))\n",
        "    else:\n",
        "        net.add(DenseLayer(units,dropout_rate=best_params['dropout_rate']))\n",
        "    net.add(ReLUActivation())\n",
        "    \n",
        "net.add(DenseLayer(1))\n",
        "net.add(SigmoidActivation())\n",
        "\n",
        "net.fit(train_ds)\n",
        "net.predict(test_ds)\n",
        "out = net.predict(test_ds)\n",
        "acc = net.score(test_ds,out)\n",
        "print(\"Test Accuracy:\", acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "831b16a2",
      "metadata": {},
      "source": [
        "# Test with other dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "ad8d2ed8",
      "metadata": {},
      "outputs": [],
      "source": [
        "input_test_csv = '../tarefa_1/clean_input_datasets/ai_human_input_sm.csv'\n",
        "output_test_csv = '../tarefa_1/clean_output_datasets/ai_human_output_sm.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "a6380e8e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "def load_data(input_path, output_path, sep=\"\\t\"):\n",
        "        # read input and output csv's\n",
        "        df_input = pd.read_csv(input_path, sep=sep)\n",
        "        df_output = pd.read_csv(output_path, sep=sep)\n",
        "        # handle rows thet might have an empty Text or missing Label\n",
        "        df_input.dropna(subset=[\"ID\", \"Text\"], inplace=True)\n",
        "        df_output.dropna(subset=[\"ID\", \"Label\"], inplace=True)\n",
        "        # remove duplicated ID's\n",
        "        df_input.drop_duplicates(subset=[\"ID\"], inplace=True)\n",
        "        df_output.drop_duplicates(subset=[\"ID\"], inplace=True)\n",
        "        # merge datasets on ID column\n",
        "        df_merged = pd.merge(df_input, df_output, on=\"ID\")\n",
        "        return df_merged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "39c8c6f3",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Text</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>cars cars around since became famous henry for...</td>\n",
              "      <td>AI</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>transportation large necessity countries world...</td>\n",
              "      <td>AI</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>americas love affair vehicles seems cooling sa...</td>\n",
              "      <td>AI</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>often ride car drive one motor vehicle work st...</td>\n",
              "      <td>AI</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>cars wonderful thing perhaps one worlds greate...</td>\n",
              "      <td>AI</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ID                                               Text Label\n",
              "0   1  cars cars around since became famous henry for...    AI\n",
              "1   2  transportation large necessity countries world...    AI\n",
              "2   3  americas love affair vehicles seems cooling sa...    AI\n",
              "3   4  often ride car drive one motor vehicle work st...    AI\n",
              "4   5  cars wonderful thing perhaps one worlds greate...    AI"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load test dataset\n",
        "df_merged = load_data(input_test_csv, output_test_csv)\n",
        "df_merged[\"Text\"] = df_merged[\"Text\"].apply(Dataset.clean_text)\n",
        "\n",
        "# Convert label \"Human\"/\"AI\" to 0/1\n",
        "labels = np.where(df_merged[\"Label\"] == \"AI\", 1.0, 0.0)\n",
        "train_texts = df_merged[\"Text\"].astype(str).tolist()\n",
        "df_merged.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "ad035f67",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test shape: (30000, 4000) (30000,)\n",
            "Test Accuracy: 0.9968333333333333\n"
          ]
        }
      ],
      "source": [
        "# Vectorize text using the same vocabulary as the training set\n",
        "X = Dataset.vectorize_text_bow(train_texts, vocab)\n",
        "\n",
        "# Wrap Dataset object\n",
        "test_ds = Dataset(X=X, Y=labels)\n",
        "\n",
        "print(\"Test shape:\", X.shape, labels.shape)\n",
        "out = net.predict(test_ds)\n",
        "acc = net.score(test_ds,out)\n",
        "print(\"Test Accuracy:\", acc)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
