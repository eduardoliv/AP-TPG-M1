{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e117c95a",
      "metadata": {},
      "source": [
        "# DNN Model Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f7f3362",
      "metadata": {},
      "source": [
        "```md\n",
        "@authors: miguelrocha and Grupo 03\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "00553163",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Notebook Imports\n",
        "\n",
        "from models.dnn_model import NeuralNetwork, hyperparameter_optimization\n",
        "from helpers.dataset import Dataset\n",
        "from helpers.enums import ModelRunMode\n",
        "from helpers.layers import DenseLayer\n",
        "from helpers.activation import ReLUActivation, SigmoidActivation\n",
        "from helpers.losses import BinaryCrossEntropy\n",
        "from helpers.optimizer import Optimizer\n",
        "from helpers.metrics import accuracy\n",
        "from helpers.enums import ModelType\n",
        "from helpers.model import save_dnn_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fc6f6268",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model run mode\n",
        "# Options: \n",
        "#   ModelRunMode.TRAIN.value            (Train the model)\n",
        "#   ModelRunMode.CLASSIFY.value         (Classify data)\n",
        "mode = ModelRunMode.CLASSIFY.value\n",
        "# Prefix for saving the model files\n",
        "model_prefix = \"dnn_model_1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d6adb728",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters cell\n",
        "if mode == ModelRunMode.TRAIN.value:\n",
        "    # TRAIN mode: Set parameters for training\n",
        "    input_csv = \"../tarefa_1/clean_input_datasets/dataset1_enh_inputs.csv\"              # CSV file with training inputs (ID, Text)\n",
        "    output_csv = \"../tarefa_1/clean_output_datasets/dataset1_enh_outputs.csv\"           # CSV file with training outputs (ID, Label)\n",
        "    test_size = 0.3                                                                     # Proportion of the dataset to use as test data\n",
        "    verbose = True                                                                      # Verbosity level enabler\n",
        "    random_state=42                                                                     # Seed for reproducible dataset splitting\n",
        "    max_vocab_size=None                                                                 # Maximum vocabulary size (None implies no limit)\n",
        "    min_freq=1                                                                          # Minimum frequency required for a word to be included in the vocabulary\n",
        "elif mode == ModelRunMode.CLASSIFY.value:\n",
        "    # CLASSIFY mode: Set parameters for classification\n",
        "    input_csv = \"../tarefa_2/classify_input_datasets/dataset2_inputs.csv\"               # CSV file with texts for prediction (ID, Text)\n",
        "    output_csv = \"../tarefa_2/classify_output_datasets/dataset2_outputs_lr_model.csv\"   # CSV file to store prediction result\n",
        "else:\n",
        "    print(\"The selected option is not valid. Options: \\\"train\\\" or \\\"classify\\\"!\")\n",
        "    SystemExit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e633c6f6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DNN Model loaded from 'dnn_model_weights\\dnn_model_1_architecture.json'\n",
            "[DNN] Predictions saved to ../tarefa_2/classify_output_datasets/dataset2_outputs_lr_model.csv\n"
          ]
        }
      ],
      "source": [
        "if mode == ModelRunMode.TRAIN.value:\n",
        "    # Load datasets using TF-IDF vectorization\n",
        "    X_train, y_train, X_test, y_test, vocab, idf = Dataset.prepare_train_test_tfidf(input_csv=input_csv, output_csv=output_csv, test_size=test_size, random_state=random_state, max_vocab_size=max_vocab_size, min_freq=min_freq)\n",
        "\n",
        "    # Create Dataset objects for training and test data\n",
        "    train_ds = Dataset(X=X_train, Y=y_train)\n",
        "    test_ds = Dataset(X=X_test, Y=y_test)\n",
        "\n",
        "    # Display dimensions of the training and test datasets\n",
        "    print(f\"Train set has {train_ds.nrows()} rows and {train_ds.ncols()} columns\")\n",
        "    print(f\"Test set has {test_ds.nrows()} rows and {test_ds.ncols()} columns\\n\")\n",
        "\n",
        "    # Define hyperparameter grids for tuning\n",
        "    epochs_list = [20, 40, 60, 80, 100, 200, 500]\n",
        "    batch_size_list = [16, 32, 64]\n",
        "    learning_rate_list = [0.001, 0.01, 0.1]\n",
        "    momentum_list = [0.1, 0.5, 0.9]\n",
        "    hidden_layers_list = [[32], [64, 32], [64, 64], [128, 64]]\n",
        "    dropout_list = [0.0, 0.01, 0.1, 0.5]\n",
        "\n",
        "    # Perform hyperparameter tuning using the training and validation sets\n",
        "    print(\"Starting hyperparameter tuning...\")\n",
        "    best_params = hyperparameter_optimization(train_ds, test_ds, epochs_list, batch_size_list, learning_rate_list, momentum_list, hidden_layers_list, dropout_list, n_iter=50)\n",
        "    print(\"\\nBest hyperparameters:\", best_params)\n",
        "\n",
        "    # Retrain DNN model on the full training data using the best hyperparameters\n",
        "    final_model = NeuralNetwork(epochs=best_params['epochs'], batch_size=best_params['batch_size'], optimizer=Optimizer(learning_rate=best_params['learning_rate'], momentum=best_params['momentum']), verbose=verbose, loss=BinaryCrossEntropy,  metric=accuracy)\n",
        "\n",
        "    # Build model Layers\n",
        "    n_features = train_ds.X.shape[1]\n",
        "    for i, units in enumerate(best_params['n_hidden']):\n",
        "        if i == 0:\n",
        "            final_model.add(DenseLayer(units, (n_features,),dropout_rate=best_params['dropout_rate']))\n",
        "        else:\n",
        "            final_model.add(DenseLayer(units,dropout_rate=best_params['dropout_rate']))\n",
        "        final_model.add(ReLUActivation())\n",
        "        \n",
        "    final_model.add(DenseLayer(1))\n",
        "    final_model.add(SigmoidActivation())\n",
        "\n",
        "    # Fit model\n",
        "    final_model.fit(train_ds)\n",
        "    final_model.predict(test_ds)\n",
        "    out = final_model.predict(test_ds)\n",
        "\n",
        "    # Evaluate the final model on the test set\n",
        "    acc = final_model.score(test_ds, out)\n",
        "    print(\"Test Accuracy:\", acc)\n",
        "\n",
        "    # Save the model, plus vocab & idf, so classification can replicate\n",
        "    save_dnn_model(dnn=final_model, vocab=vocab, idf=idf, model_prefix=model_prefix)\n",
        "    print(f\"Model saved with prefix {model_prefix}\")\n",
        "\n",
        "if mode == ModelRunMode.CLASSIFY.value:\n",
        "    # Classify new texts using the saved model\n",
        "    Dataset.classify_texts(input_csv=input_csv, output_csv=output_csv, neural_net_class=NeuralNetwork, model_type=ModelType.DNN ,model_prefix=model_prefix)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "AP",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
