{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e117c95a",
      "metadata": {},
      "source": [
        "# Logistic Regression Model Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f7f3362",
      "metadata": {},
      "source": [
        "```md\n",
        "@authors: miguelrocha and Grupo 03\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "00553163",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Notebook Imports\n",
        "import numpy as np\n",
        "\n",
        "from models.logistic_regression_model import LogisticRegression, hyperparameter_tuning\n",
        "from helpers.dataset import Dataset\n",
        "from helpers.model import save_model\n",
        "from helpers.metrics import confusion_matrix, balanced_accuracy, precision_recall_f1\n",
        "from helpers.enums import ModelRunMode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fc6f6268",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model run mode\n",
        "# Options: \n",
        "#   ModelRunMode.TRAIN.value            (Train the model)\n",
        "#   ModelRunMode.CLASSIFY.value         (Classify data)\n",
        "mode = ModelRunMode.TRAIN.value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "94fdaeeb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters cell\n",
        "if mode == ModelRunMode.TRAIN.value:\n",
        "    # Train mode\n",
        "    input_csv = \"../tarefa_1/clean_input_datasets/gpt_vs_human_data_set_inputs.csv\"     # CSV for training input (ID, Text)\n",
        "    output_csv = \"../tarefa_1/clean_output_datasets/gpt_vs_human_data_set_outputs.csv\"  # CSV for training output (ID, Label)\n",
        "    model_prefix = \"logreg_model\"                                                       # Prefix for saving the model files\n",
        "    test_size = 0.3                                                                     # Proportion of the dataset to use as test data\n",
        "    regularization = True                                                               # Use L2 regularization approach\n",
        "    random_state=42                                                                     # Random State for dataset\n",
        "    max_vocab_size=None                                                                 # Maximum vocabulary size\n",
        "    min_freq=2                                                                          # Minimum word frequency\n",
        "elif mode == ModelRunMode.CLASSIFY.value:\n",
        "    # Classify mode\n",
        "    input_csv = \"../tarefa_1/classify_input_datasets/dataset2_inputs.csv\"               # CSV for predictions input (ID, Text)\n",
        "    output_csv = \"../tarefa_1/classify_output_datasets/dataset2_outputs.csv\"            # CSV name for predictions output\n",
        "    model_prefix = \"logreg_model\"                                                       # Prefix for loading the model files\n",
        "else:\n",
        "    print(\"The selected option is not valid. Options: \\\"train\\\" or \\\"classify\\\"!\")\n",
        "    SystemExit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6de2fdb7",
      "metadata": {},
      "outputs": [],
      "source": [
        "if mode == ModelRunMode.TRAIN.value:\n",
        "    # Load Datasets\n",
        "    X_train, y_train, X_test, y_test, vocab, idf = Dataset.prepare_train_test_tfidf(input_csv=input_csv, output_csv=output_csv, test_size=test_size, random_state=random_state, max_vocab_size=max_vocab_size, min_freq=min_freq)\n",
        "\n",
        "    # Create Dataset objects for training and testing\n",
        "    train_ds_full = Dataset(X=X_train, Y=y_train)\n",
        "    test_ds = Dataset(X=X_test, Y=y_test)\n",
        "\n",
        "    # Validate Train and Test dataset division\n",
        "    print(f\"Train set has {train_ds_full.nrows()} rows and {train_ds_full.ncols()} columns\")\n",
        "    print(f\"Test set has {test_ds.nrows()} rows and {test_ds.ncols()} columns\\n\")\n",
        "\n",
        "    # Split the full training data into training and validation sets\n",
        "    # n_train = train_ds_full.X.shape[0]\n",
        "    # indices = np.arange(n_train)\n",
        "    # np.random.shuffle(indices)\n",
        "    # split_idx = int((1-test_size) * n_train)\n",
        "    # train_idx = indices[:split_idx]\n",
        "    # val_idx = indices[split_idx:]\n",
        "    # train_ds = Dataset(X=train_ds_full.X[train_idx], Y=train_ds_full.Y[train_idx])\n",
        "    # val_ds = Dataset(X=train_ds_full.X[val_idx], Y=train_ds_full.Y[val_idx])\n",
        "\n",
        "    # Define hyperparameter grids\n",
        "    # alphas = [0.0001, 0.001, 0.01, 0.1]\n",
        "    # lambdas = [0, 0.01, 0.1, 0.5, 1.0, 1.5]\n",
        "    # iters_list = [10000, 20000, 40000, 80000]\n",
        "\n",
        "    # print(\"Starting hyperparameter tuning...\")\n",
        "    # best_params, best_acc, results = hyperparameter_tuning(train_ds, val_ds, alphas, lambdas, iters_list)\n",
        "    # print(\"\\nBest hyperparameters:\", best_params)\n",
        "    # print(\"Best validation accuracy:\", best_acc)\n",
        "\n",
        "    # Retrain model on full training data with best hyperparameters\n",
        "    final_model = LogisticRegression(train_ds_full, regularization=True, lamda=0.01)\n",
        "    final_model.buildModel()  \n",
        "    #final_model.gradientDescent(alpha=best_params[\"alpha\"], iters=best_params[\"iters\"])\n",
        "\n",
        "    # Save the model\n",
        "    save_model(final_model.theta, vocab, idf, model_prefix)\n",
        "    print(f\"Model saved with prefix {model_prefix}\")\n",
        "\n",
        "    # Evaluate on test set\n",
        "    ones_test = np.ones((test_ds.X.shape[0], 1))\n",
        "    X_test_bias = np.hstack((ones_test, test_ds.X))\n",
        "    test_acc = final_model.accuracy(X_test_bias, test_ds.Y)\n",
        "    print(f\"\\nTest accuracy with best hyperparameters: {test_acc:.4f}\")\n",
        "\n",
        "    preds = final_model.predictMany(X_test_bias)\n",
        "    TP, FP, TN, FN = confusion_matrix(y_test, preds)\n",
        "    prec, rec, f1 = precision_recall_f1(y_test, preds)\n",
        "    bal_acc = balanced_accuracy(y_test, preds)\n",
        "\n",
        "    print(\"Confusion Matrix: TP={}, FP={}, TN={}, FN={}\".format(TP, FP, TN, FN))\n",
        "    print(\"Precision = {:.4f}, Recall = {:.4f}, F1 = {:.4f}\".format(prec, rec, f1))\n",
        "    print(\"Balanced Accuracy = {:.4f}\".format(bal_acc))\n",
        "    final_model.plotModel()\n",
        "\n",
        "if mode == ModelRunMode.CLASSIFY.value:\n",
        "    Dataset.classify_texts(input_csv, output_csv, model_prefix=model_prefix)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
