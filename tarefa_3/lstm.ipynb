{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Input, RNN, Dropout, Bidirectional\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow.keras.backend as ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 120\n",
    "n = 30000  # Set the number of most frequent words to keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining fuctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text, stopwords = True):\n",
    "    def normalize(text):\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove numbers, special characters, e o caractere '\n",
    "        text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "        # Replace multiple spaces with a single space\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        # Add start and end sequence tokens\n",
    "        # text = 'startseq ' + \" \".join([word for word in text.split() if len(word) > 1]) + ' endseq'\n",
    "        return text\n",
    "\n",
    "    def remove_stopwords(text):\n",
    "        stopwords = [\n",
    "        \"the\", \"of\", \"and\", \"in\", \"to\", \"is\", \"a\", \"that\", \"for\", \"are\", \"on\", \"with\", \n",
    "        \"as\", \"at\", \"by\", \"from\", \"this\", \"it\", \"an\", \"be\", \"or\", \"which\", \"was\", \"were\"\n",
    "        ]\n",
    "        text = ' '.join([word for word in text.split() if word not in stopwords])\n",
    "        return text\n",
    "    \n",
    "    text = normalize(text)\n",
    "    if stopwords:\n",
    "        text = remove_stopwords(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, texts, n_words=None):\n",
    "        self.n_words = n_words\n",
    "        self.texts = texts\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = set()\n",
    "        self.create_index()\n",
    "        \n",
    "\n",
    "    def create_index(self):\n",
    "        word_counter = Counter()\n",
    "        for caption in self.texts:\n",
    "            for word in caption.split():\n",
    "                word_counter[word] += 1\n",
    "\n",
    "        # Sort words by frequency and alphabetically for ties\n",
    "        most_common = word_counter.most_common(self.n_words) if self.n_words else word_counter.items()\n",
    "        self.vocab = [word for word, _ in sorted(most_common, key=lambda x: (-x[1], x[0]))]\n",
    "\n",
    "        # Add special tokens\n",
    "        self.word2idx['<pad>'] = 0\n",
    "        self.word2idx['<unk>'] = 1\n",
    "        for index, word in enumerate(self.vocab):\n",
    "            self.word2idx[word] = index + 2\n",
    "\n",
    "        for word, index in self.word2idx.items():\n",
    "            self.idx2word[index] = word\n",
    "\n",
    "\n",
    "    def encode(self, caption):\n",
    "        tokens = []\n",
    "        for word in caption.split():\n",
    "            if word in self.word2idx:\n",
    "                tokens.append(self.word2idx[word])\n",
    "            else:\n",
    "                tokens.append(self.word2idx['<unk>'])\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return ' '.join([self.idx2word.get(token, '<unk>') for token in tokens])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, tokenizer,max_length=120):\n",
    "\n",
    "    X = df['Text']\n",
    "    # y = df['Label'].apply(lambda x: 0 if x == 0 else 1)\n",
    "    y = df['Label']\n",
    "    X = X.apply(lambda x: tokenizer.encode(x))\n",
    "    X = pad_sequences(X, maxlen=max_length, padding='pre')\n",
    "    # y = to_categorical(y)[:,1]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opening and cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>paper we enumerate prime graphs respect cartes...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>research paper investigates resummed cross sec...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>red supergiant stars represent key phase evolu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we investigate effects all orders planck lengt...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>we introduce novel extension gutzwiller variat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4048</th>\n",
       "      <td>effect li substitution mg lic cosubstitution s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4049</th>\n",
       "      <td>research paper titled energy efficiency perspe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4050</th>\n",
       "      <td>research paper presents results optical spectr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4051</th>\n",
       "      <td>using molecular simulations we show aperiodic ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4052</th>\n",
       "      <td>abstract quantum chromodynamics qcd fundamenta...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4053 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  Label\n",
       "0     paper we enumerate prime graphs respect cartes...      0\n",
       "1     research paper investigates resummed cross sec...      1\n",
       "2     red supergiant stars represent key phase evolu...      0\n",
       "3     we investigate effects all orders planck lengt...      0\n",
       "4     we introduce novel extension gutzwiller variat...      0\n",
       "...                                                 ...    ...\n",
       "4048  effect li substitution mg lic cosubstitution s...      0\n",
       "4049  research paper titled energy efficiency perspe...      1\n",
       "4050  research paper presents results optical spectr...      1\n",
       "4051  using molecular simulations we show aperiodic ...      0\n",
       "4052  abstract quantum chromodynamics qcd fundamenta...      1\n",
       "\n",
       "[4053 rows x 2 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.read_csv('../tarefa_1/clean_input_datasets/gpt_vs_human_data_set_inputs.csv',sep='\\t', index_col=False)['Text']\n",
    "y = pd.read_csv('../tarefa_1/clean_output_datasets/gpt_vs_human_data_set_outputs.csv',sep='\\t', index_col=False)['Label']\n",
    "df = pd.concat([X, y], axis=1)\n",
    "\n",
    "df['Text'] = df['Text'].str.replace('\\n', ' ')\n",
    "df['Text'] = df['Text'].astype(str)\n",
    "df['Label'] = df['Label'].apply(lambda x: 1 if x == \"AI\" else 0)\n",
    "df['Text'] = df['Text'].apply(clean)\n",
    "#shuffle\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cell cycle celldivision cycle sequential serie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cell cycle process cell grows duplicates its d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>photons many atomic models physics particles t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>photon fundamental particle light other electr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>according theory plate tectonics earths lithos...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>research paper explores concept topological in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>we report experimental realization oneway quan...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>research paper presents experimental realizati...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>airwater interface alkylbiphenylcarbonitrile c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>research paper investigates line tension struc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Text  Label\n",
       "0   cell cycle celldivision cycle sequential serie...      0\n",
       "1   cell cycle process cell grows duplicates its d...      1\n",
       "2   photons many atomic models physics particles t...      0\n",
       "3   photon fundamental particle light other electr...      1\n",
       "4   according theory plate tectonics earths lithos...      0\n",
       "..                                                ...    ...\n",
       "95  research paper explores concept topological in...      1\n",
       "96  we report experimental realization oneway quan...      0\n",
       "97  research paper presents experimental realizati...      1\n",
       "98  airwater interface alkylbiphenylcarbonitrile c...      0\n",
       "99  research paper investigates line tension struc...      1\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_1 = pd.read_csv('../tarefa_1/clean_input_datasets/dataset1_enh_inputs.csv',sep='\\t', index_col=False)['Text']\n",
    "y_val_1 = pd.read_csv('../tarefa_1/clean_output_datasets/dataset1_enh_outputs.csv',sep='\\t', index_col=False)['Label']\n",
    "df_val_1 = pd.concat([X_val_1, y_val_1], axis=1)\n",
    "df_val_1['Text'] = df_val_1['Text'].str.replace('\\n', ' ')\n",
    "df_val_1['Text'] = df_val_1['Text'].astype(str)\n",
    "df_val_1['Label'] = df_val_1['Label'].apply(lambda x: 1 if x == \"AI\" else 0)\n",
    "df_val_1['Text'] = df_val_1['Text'].apply(clean)\n",
    "df_val_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>approximation useful chemistry but not strictl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>these nutrients needed keep bones teeth muscle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vitamin d essential maintaining healthy bones ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>within million years pressure density hydrogen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>there estimated trillion galaxies known univer...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>research paper explores concept topological in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>we report experimental realization oneway quan...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>research paper presents experimental realizati...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>airwater interface alkylbiphenylcarbonitrile c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>research paper investigates line tension struc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  Label\n",
       "0    approximation useful chemistry but not strictl...      0\n",
       "1    these nutrients needed keep bones teeth muscle...      0\n",
       "2    vitamin d essential maintaining healthy bones ...      1\n",
       "3    within million years pressure density hydrogen...      0\n",
       "4    there estimated trillion galaxies known univer...      0\n",
       "..                                                 ...    ...\n",
       "145  research paper explores concept topological in...      1\n",
       "146  we report experimental realization oneway quan...      0\n",
       "147  research paper presents experimental realizati...      1\n",
       "148  airwater interface alkylbiphenylcarbonitrile c...      0\n",
       "149  research paper investigates line tension struc...      1\n",
       "\n",
       "[150 rows x 2 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df = pd.read_csv('dataset2_disclosed.csv', sep=';')\n",
    "val_df['Text'] = val_df['Text'].str.replace('\\n', ' ')\n",
    "val_df['Text'] = val_df['Text'].astype(str)\n",
    "val_df['Label'] = val_df['Label'].apply(lambda x: 1 if x == \"AI\" else 0)\n",
    "val_df['Text'] = val_df['Text'].apply(clean)\n",
    "val_df = val_df[['Text', 'Label']]\n",
    "val_df = pd.concat([val_df, df_val_1], axis=0).reset_index(drop=True)\n",
    "val_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the vocabulary: 19081\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(df['Text'], n)\n",
    "vocab_size = len(tokenizer)\n",
    "print('Number of words in the vocabulary:', len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = prepare_data(df, tokenizer, max_length)\n",
    "X_val, y_val = prepare_data(val_df, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     5     3    64   355  1801    48  3266  1085   795  2334  7598   914\n",
      "    18   146  6353   168 18248    46    78   355  1801   258   873  1085\n",
      "   795   752    10   141   914  2334  3193   176   180    65 11532  3980\n",
      "   219  2390  3266    55    46   413   355  1801   729   252   100    58\n",
      "   244  2334    10    33    41   797    20    24  1085   795    19  1748]\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> research paper investigates line tension structure smectic liquid crystal multilayers airwater interface using xray reflectivity surface tensiometry authors found line tension related thickness liquid crystal layers their interactions interface multilayers exhibited range structures including wellordered hexagonal lattice disordered smectic phase authors conclude line tension plays crucial role formation stability multilayers their findings provide insight into behavior liquid crystal systems interfaces\n"
     ]
    }
   ],
   "source": [
    "print(X_val[-1])\n",
    "print(tokenizer.decode(X_val[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "def hyperparameter_optimization_lstm(train_ds, validation_ds, \n",
    "                                     epochs_list, batch_size_list, \n",
    "                                     learning_rate_list, lstm_units_list,\n",
    "                                     embed_dim_list, \n",
    "                                     dropout_list, n_iter=10):\n",
    "\n",
    "    keras.backend.clear_session()\n",
    "    best_acc = 0.0\n",
    "    best_params = {}\n",
    "\n",
    "    # Prepare random combinations\n",
    "    param_combinations = []\n",
    "    for _ in range(n_iter):\n",
    "        param_combinations.append({\n",
    "            'epochs': random.choice(epochs_list),\n",
    "            'batch_size': random.choice(batch_size_list),\n",
    "            'learning_rate': random.choice(learning_rate_list),\n",
    "            'lstm_units': random.choice(lstm_units_list),\n",
    "            'dropout_rate': random.choice(dropout_list),\n",
    "            'embed_dim': random.choice(embed_dim_list)\n",
    "        })\n",
    "\n",
    "    # Extract input shape and number of classes\n",
    "    input_shape = train_ds.element_spec[0].shape\n",
    "    print(\"Input Shape:\", input_shape)\n",
    "\n",
    "    for i,params in enumerate(param_combinations):\n",
    "        print(f\"\\nIteration {i+1}/{n_iter}\", end=' ')\n",
    "        # Unpack parameters\n",
    "        epochs = params['epochs']\n",
    "        batch_size = params['batch_size']\n",
    "        learning_rate = params['learning_rate']\n",
    "        lstm_units = params['lstm_units']\n",
    "        dropout_rate = params['dropout_rate']\n",
    "        embed_dim = params['embed_dim']\n",
    "\n",
    "        # Build LSTM model\n",
    "        inputs = keras.Input(shape=(input_shape[1],))  # Assuming (timesteps, features)\n",
    "\n",
    "        x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
    "\n",
    "        for units in lstm_units[:-1]:\n",
    "            x = layers.Bidirectional(layers.LSTM(units, return_sequences=True))(x)\n",
    "            x = layers.Dropout(dropout_rate)(x)\n",
    "        \n",
    "        x = layers.Bidirectional(layers.LSTM(lstm_units[-1]))(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "\n",
    "        outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=0, min_lr=1e-6)\n",
    "        ]\n",
    "\n",
    "        model.compile(optimizer=optimizer,\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        # Train the model\n",
    "        history = model.fit(train_ds, epochs=epochs, batch_size=batch_size,\n",
    "                            verbose=0, validation_data=validation_ds, callbacks=callbacks)\n",
    "\n",
    "        # Get validation accuracy\n",
    "        val_acc = max(history.history.get('val_accuracy', [0]))\n",
    "        print(f\"Validation Accuracy: {val_acc:.4f}\", end=' ')\n",
    "        print(f\"Train Accuracy: {max(history.history.get('accuracy', [0])):.4f}\")\n",
    "\n",
    "        # Update best model if improved\n",
    "        if val_acc > best_acc:\n",
    "            best_model = model\n",
    "            best_acc = val_acc\n",
    "            best_params = params\n",
    "\n",
    "    print(\"\\nBest Hyperparameters Found:\", best_params)\n",
    "    print(f\"Best Accuracy: {best_acc:.4f}\")\n",
    "    return best_params, best_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: (None, 120)\n",
      "\n",
      "Iteration 1/10 Validation Accuracy: 0.7867 Train Accuracy: 0.9931\n",
      "\n",
      "Iteration 2/10 Validation Accuracy: 0.7333 Train Accuracy: 0.9998\n",
      "\n",
      "Iteration 3/10 Validation Accuracy: 0.7333 Train Accuracy: 1.0000\n",
      "\n",
      "Iteration 4/10 Validation Accuracy: 0.7467 Train Accuracy: 1.0000\n",
      "\n",
      "Iteration 5/10 Validation Accuracy: 0.7333 Train Accuracy: 1.0000\n",
      "\n",
      "Iteration 6/10 Validation Accuracy: 0.7667 Train Accuracy: 0.9958\n",
      "\n",
      "Iteration 7/10 Validation Accuracy: 0.7867 Train Accuracy: 0.9998\n",
      "\n",
      "Iteration 8/10 Validation Accuracy: 0.7800 Train Accuracy: 0.9998\n",
      "\n",
      "Iteration 9/10 Validation Accuracy: 0.7733 Train Accuracy: 1.0000\n",
      "\n",
      "Iteration 10/10 Validation Accuracy: 0.8133 Train Accuracy: 0.9995\n",
      "\n",
      "Best Hyperparameters Found: {'epochs': 20, 'batch_size': 64, 'learning_rate': 0.001, 'lstm_units': [256, 256], 'dropout_rate': 0.7, 'embed_dim': 50}\n",
      "Best Accuracy: 0.8133\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(64)\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(64)\n",
    "\n",
    "# Hyperparameter search\n",
    "epochs_list = [10, 20, 30]\n",
    "batch_size_list = [64]\n",
    "learning_rate_list = [1e-2, 1e-3, 1e-4]\n",
    "lstm_units_list = [[64, 64], [128, 128], [256, 256]]\n",
    "embed_dim_list = [50, 10, 150]\n",
    "dropout_list = [0.3, 0.5, 0.7]\n",
    "hiperparams, model = hyperparameter_optimization_lstm(train_ds, val_ds,\n",
    "                                                      epochs_list, batch_size_list,\n",
    "                                                      learning_rate_list, lstm_units_list,\n",
    "                                                      embed_dim_list, dropout_list, n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_9\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_9\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">954,050</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_18                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">628,736</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_19                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_9 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_9 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │       \u001b[38;5;34m954,050\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_18                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │       \u001b[38;5;34m628,736\u001b[0m │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_18 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_19                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m1,574,912\u001b[0m │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_19 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_18 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m65,664\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_19 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,670,475</span> (36.89 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,670,475\u001b[0m (36.89 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,223,491</span> (12.30 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,223,491\u001b[0m (12.30 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,446,984</span> (24.59 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m6,446,984\u001b[0m (24.59 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('models/best_lstm_model.h5')\n",
    "# save params and tokenizer\n",
    "import pickle\n",
    "with open('models/best_lstm_model_params.pkl', 'wb') as f:\n",
    "    pickle.dump(hiperparams, f)\n",
    "\n",
    "with open('models/tokenizer_lstm.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epochs': 20, 'batch_size': 64, 'learning_rate': 0.001, 'lstm_units': [256, 256], 'dropout_rate': 0.7, 'embed_dim': 50}\n",
      "<__main__.Tokenizer object at 0x75272c3c7070>\n"
     ]
    }
   ],
   "source": [
    "# open best params and tokenizer\n",
    "with open('models/best_lstm_model_params.pkl', 'rb') as f:\n",
    "    hiperparams = pickle.load(f)\n",
    "\n",
    "with open('models/tokenizer_lstm.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "print(hiperparams)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Human'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(text, model, tokenizer):\n",
    "    text = clean(text)\n",
    "    encoded = tokenizer.encode(text)\n",
    "    padded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "    pred = model.predict(padded)\n",
    "    return 'Human' if pred[0] <= 0.5 else 'Machine'\n",
    "\n",
    "predict('The origin of the COVID-19 pandemic, caused by SARS-CoV-2, remains uncertain, with two main theories under discussion. The dominant hypothesis suggests a natural origin through zoonotic spillover, where the virus jumped from bats, its likely reservoir, to humans, possibly via an intermediate species such as pangolins. This theory links the early outbreak to a seafood market in Wuhan, China, where live wild animals were sold. Another theory suggests an accidental leak from a laboratory, like the Wuhan Institute of Virology, which studies bat coronaviruses, though no direct evidence supports this claim. While most scientists favor the natural origin hypothesis, limited data and restricted early access make the exact source of the virus difficult to confirm.', model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test in another dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_1(X, tokenizer,max_length=120):\n",
    "    X = X.apply(lambda x: tokenizer.encode(x))\n",
    "    X = pad_sequences(X, maxlen=max_length, padding='pre')\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_stor = pd.read_csv('../dataset3_inputs.csv',sep=';', index_col=False)['Text']\n",
    "X_stor = X_stor.str.replace('\\n', ' ')\n",
    "X_stor = X_stor.astype(str)\n",
    "X_stor = X_stor.apply(clean)\n",
    "X_stor = prepare_data_1(X_stor, tokenizer,max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 120)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_stor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step \n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(X_stor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1]), array([66, 34]))\n"
     ]
    }
   ],
   "source": [
    "#print the numbers of each class\n",
    "preds = [1 if pred > 0.5 else 0 for pred in preds]\n",
    "print(np.unique(preds, return_counts=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
