{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e117c95a",
      "metadata": {},
      "source": [
        "# Gated Recurrent Units (GRU) Model Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f7f3362",
      "metadata": {},
      "source": [
        "```md\n",
        "@authors: Grupo 03\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "id": "00553163",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Notebook Imports\n",
        "import os\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from enum import Enum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "id": "7cee9d6c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model run enum\n",
        "class ModelRunMode(Enum):\n",
        "    \"\"\"\n",
        "    Enumeration of Model Run Mode.\n",
        "    \"\"\"\n",
        "    TRAIN           = \"train\"           # Train Mode\n",
        "    CLASSIFY        = \"classify\"        # Classify Mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "id": "fc6f6268",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model run mode\n",
        "# Options: \n",
        "#   ModelRunMode.TRAIN.value            (Train the model)\n",
        "#   ModelRunMode.CLASSIFY.value         (Classify data)\n",
        "mode = ModelRunMode.CLASSIFY.value\n",
        "# Prefix for saving the model files\n",
        "model_prefix = \"gru_model_1\"\n",
        "file_path = \"gru_model_weights\"\n",
        "separator_char = \"\\t\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "id": "d6adb728",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters cell\n",
        "if mode == ModelRunMode.TRAIN.value:\n",
        "    # TRAIN mode: Set parameters for training\n",
        "    input_csv = \"../tarefa_1/clean_input_datasets/gpt_vs_human_data_set_inputs.csv\"              # CSV file with training inputs (ID, Text)\n",
        "    output_csv = \"../tarefa_1/clean_output_datasets/gpt_vs_human_data_set_outputs.csv\"           # CSV file with training outputs (ID, Label)\n",
        "    test_size = 0.3                                                                     # Proportion of the dataset to use as test data\n",
        "    validation_size = 0.2                                                               # Proportion of the dataset reserved as validation data\n",
        "    random_state=42                                                                     # Seed for reproducible dataset splitting\n",
        "    verbose = True                                                                      # Verbosity level enabler\n",
        "    embedding_dim = 32                                                                  # Embedding dimension.\n",
        "    gru_units = 32                                                                      # Number of GRU units.\n",
        "    vocab_size = 1000                                                                   # Max vocabulary size.\n",
        "    epochs = 100                                                                        # Number of training epochs.\n",
        "    batch_size = 16                                                                     # Batch size.\n",
        "    max_len = 100\n",
        "elif mode == ModelRunMode.CLASSIFY.value:\n",
        "    # CLASSIFY mode: Set parameters for classification\n",
        "    input_csv = \"../tarefa_3/classify_input_datasets/dataset2_inputs.csv\"               # CSV file with texts for prediction (ID, Text)\n",
        "    output_csv = \"../tarefa_3/classify_output_datasets/dataset2_outputs_gru_model.csv\"  # CSV file to store prediction result\n",
        "    max_len = 100\n",
        "else:\n",
        "    print(\"The selected option is not valid. Options: \\\"train\\\" or \\\"classify\\\"!\")\n",
        "    SystemExit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "id": "7713c68b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method to load and merge two datasets by ID column\n",
        "def merge_data_by_id(input_csv, output_csv, sep=\"\\t\"):\n",
        "    df_in = pd.read_csv(input_csv, sep=sep)\n",
        "    df_out = pd.read_csv(output_csv, sep=sep)\n",
        "\n",
        "    # Remove duplicates or NaNs if needed\n",
        "    df_in.dropna(subset=[\"ID\", \"Text\"], inplace=True)\n",
        "    df_out.dropna(subset=[\"ID\", \"Label\"], inplace=True)\n",
        "    df_in.drop_duplicates(subset=[\"ID\"], inplace=True)\n",
        "    df_out.drop_duplicates(subset=[\"ID\"], inplace=True)\n",
        "\n",
        "    df_merged = pd.merge(df_in, df_out, on=\"ID\", how=\"inner\")\n",
        "    return df_merged\n",
        "\n",
        "# Method for text cleaning\n",
        "def text_cleaning(text):\n",
        "        # Download required NLTK resources\n",
        "        nltk.download('stopwords', quiet=True)\n",
        "        nltk.download('punkt_tab', quiet=True)\n",
        "        nltk.download('wordnet', quiet=True)\n",
        "        # Convert text to lowercase\n",
        "        text = text.lower()\n",
        "        # Remove URLs\n",
        "        text = re.sub(r'http[s]?://\\S+', \"\", text)\n",
        "        # Remove HTML tags\n",
        "        text = re.sub(r\"<[^>]*>\", \"\", text)\n",
        "        # Remove common LaTeX commands\n",
        "        text = re.sub(r\"\\\\[a-zA-Z]+(\\{.*?\\})?\", \"\", text)\n",
        "        # Remove email addresses\n",
        "        text = re.sub(r'\\S+@\\S+', \"\", text)\n",
        "        # Remove punctuation\n",
        "        text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "        # Remove digits\n",
        "        text = re.sub(r\"\\d+\", \"\", text)\n",
        "        # Replace newlines and extra whitespace with a single space\n",
        "        text = re.sub(r\"\\s+\", \" \", text).replace('\\n', \" \")\n",
        "        # Trim leading and trailing whitespace\n",
        "        text = text.strip()\n",
        "        # Tokenize text and remove stopwords using NLTK's English stopwords list\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        # Tokenize text\n",
        "        tokens = word_tokenize(text)\n",
        "        # Remove stopwords\n",
        "        filtered_tokens = [tok for tok in tokens if tok not in stop_words]\n",
        "        # Lemmatize tokens\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        lemmatized_tokens = [lemmatizer.lemmatize(tok) for tok in filtered_tokens]\n",
        "        # Return the cleaned text as a string\n",
        "        return \" \".join(lemmatized_tokens)\n",
        "\n",
        "# Method to convert labels to binary\n",
        "def convert_labels_to_binary_and_get_text(df_merged):\n",
        "    df_merged[\"Label\"] = df_merged[\"Label\"].str.lower().str.strip()\n",
        "    y = np.where(df_merged[\"Label\"] == \"ai\", 1.0, 0.0)\n",
        "    texts = df_merged[\"Text\"].tolist()\n",
        "    return y, texts\n",
        "\n",
        "# Method to plot the learning curves\n",
        "def plot_learning_curves(history):\n",
        "    # Loss\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    if 'val_loss' in history.history:\n",
        "        plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "    plt.title(\"Loss\")\n",
        "    plt.legend()\n",
        "    \n",
        "    # Accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='Train Acc')\n",
        "    if 'val_accuracy' in history.history:\n",
        "        plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
        "    plt.title(\"Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "id": "98f21a18",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method to create a GRU-Based classification model\n",
        "def build_gru_model(vocab_size, max_len, embedding_dim=32, gru_units=64, learning_rate=1e-3):\n",
        "    \"\"\"\n",
        "    Create a GRU-based classification model\n",
        "    \"\"\"\n",
        "    model = keras.Sequential()\n",
        "    \n",
        "    # Embedding layer\n",
        "    model.add(layers.Embedding(input_dim=vocab_size, \n",
        "                               output_dim=embedding_dim, \n",
        "                               input_length=max_len))\n",
        "    \n",
        "    # GRU layer\n",
        "    model.add(layers.GRU(units=gru_units, return_sequences=False))\n",
        "    \n",
        "    # Final output for binary classification\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    \n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "id": "e633c6f6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading tokenizer: gru_model_weights\\gru_model_1_tokenizer\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"C:\\Users\\Eduardo\\anaconda3\\envs\\ap\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Eduardo\\anaconda3\\envs\\ap\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Eduardo\\anaconda3\\envs\\ap\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\Eduardo\\anaconda3\\envs\\ap\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\Eduardo\\anaconda3\\envs\\ap\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Eduardo\\anaconda3\\envs\\ap\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_12\" is incompatible with the layer: expected shape=(None, 127), found shape=(None, 100)\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[169], line 80\u001b[0m\n\u001b[0;32m     77\u001b[0m X_new \u001b[38;5;241m=\u001b[39m preprocessing\u001b[38;5;241m.\u001b[39msequence\u001b[38;5;241m.\u001b[39mpad_sequences(sequences, maxlen\u001b[38;5;241m=\u001b[39mmax_len, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_new\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m pred_bin \u001b[38;5;241m=\u001b[39m (preds \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     82\u001b[0m pred_label \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(pred_bin \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuman\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32m~\\anaconda3\\envs\\ap\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileovzxn2te.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Eduardo\\anaconda3\\envs\\ap\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Eduardo\\anaconda3\\envs\\ap\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Eduardo\\anaconda3\\envs\\ap\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\Eduardo\\anaconda3\\envs\\ap\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\Eduardo\\anaconda3\\envs\\ap\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Eduardo\\anaconda3\\envs\\ap\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_12\" is incompatible with the layer: expected shape=(None, 127), found shape=(None, 100)\n"
          ]
        }
      ],
      "source": [
        "if mode == ModelRunMode.TRAIN.value:\n",
        "    # Load data\n",
        "    df_merged = merge_data_by_id(input_csv, output_csv, sep=separator_char)\n",
        "\n",
        "    # Text cleaning\n",
        "    df_merged[\"Text\"] = df_merged[\"Text\"].apply(text_cleaning)\n",
        "\n",
        "    # Convert Label: \"AI\" -> 1, \"Human\" -> 0\n",
        "    y, texts = convert_labels_to_binary_and_get_text(df_merged)\n",
        "\n",
        "    # Creating tokenizer\n",
        "    tokenizer = preprocessing.text.Tokenizer(num_words=vocab_size, oov_token=\"<UNK>\")\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "\n",
        "    # Convert to sequences\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "    # Pad sequences\n",
        "    X = preprocessing.sequence.pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "    # Train/test split\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
        "\n",
        "    # Create a GRU-Based classification model\n",
        "    model = build_gru_model(vocab_size=vocab_size, max_len=max_len, embedding_dim=embedding_dim, gru_units=gru_units)\n",
        "\n",
        "    # Display model sumary\n",
        "    model.summary()\n",
        "\n",
        "    # Training\n",
        "    history = model.fit( X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
        "\n",
        "    # Plot model learning curves\n",
        "    plot_learning_curves(history)\n",
        "\n",
        "    # Construct the file paths\n",
        "    model_path = os.path.join(file_path, f\"{model_prefix}_model\")\n",
        "    tokenizer_path = os.path.join(file_path, f\"{model_prefix}_tokenizer\")\n",
        "\n",
        "    # Save the model weights\n",
        "    print(\"Saving model to:\", model_path)\n",
        "    model.save(model_path)\n",
        "\n",
        "    print(\"Saving tokenizer to:\", tokenizer_path)\n",
        "    with open(tokenizer_path, \"wb\") as f:\n",
        "        pickle.dump(tokenizer, f)\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    loss_val, acc_val = model.evaluate(X_val, y_val, verbose=verbose)\n",
        "    print(f\"Validation Loss: {loss_val:.4f}, Validation Accuracy: {acc_val:.4f}\")\n",
        "\n",
        "if mode == ModelRunMode.CLASSIFY.value:\n",
        "    # Construct the file paths\n",
        "    model_path = os.path.join(file_path, f\"{model_prefix}_model\")\n",
        "    tokenizer_path = os.path.join(file_path, f\"{model_prefix}_tokenizer\")\n",
        "\n",
        "    # Loading model\n",
        "    model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "    print(\"Loading tokenizer:\", tokenizer_path)\n",
        "    with open(tokenizer_path, \"rb\") as f:\n",
        "        tokenizer = pickle.load(f)\n",
        "\n",
        "    # Reading input for classification\n",
        "    df_new = pd.read_csv(input_csv, sep=separator_char)\n",
        "    if \"ID\" not in df_new.columns or \"Text\" not in df_new.columns:\n",
        "        raise ValueError(\"Input CSV must have 'ID' and 'Text' columns for classification.\")\n",
        "\n",
        "    # Clean text\n",
        "    df_new[\"Text\"] = df_new[\"Text\"].apply(text_cleaning)\n",
        "    texts = df_new[\"Text\"].tolist()\n",
        "\n",
        "    # Convert to sequences\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "    # Pad\n",
        "    X_new = preprocessing.sequence.pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "    # Predict\n",
        "    preds = model.predict(X_new)\n",
        "    pred_bin = (preds >= 0.5).astype(int).flatten()\n",
        "    pred_label = np.where(pred_bin == 1, \"AI\", \"Human\")\n",
        "\n",
        "    # Save result\n",
        "    df_out = pd.DataFrame({\"ID\": df_new[\"ID\"], \"Label\": pred_label})\n",
        "    df_out.to_csv(output_csv, sep=separator_char, index=False)\n",
        "    print(f\"Predictions saved to {output_csv}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
