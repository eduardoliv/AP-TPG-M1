{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e117c95a",
      "metadata": {},
      "source": [
        "# Gated Recurrent Units Model Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f7f3362",
      "metadata": {},
      "source": [
        "```md\n",
        "@authors: Grupo 03\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "00553163",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Notebook Imports\n",
        "import os\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from enum import Enum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "7cee9d6c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model run enum\n",
        "class ModelRunMode(Enum):\n",
        "    \"\"\"\n",
        "    Enumeration of Model Run Mode.\n",
        "    \"\"\"\n",
        "    TRAIN           = \"train\"           # Train Mode\n",
        "    CLASSIFY        = \"classify\"        # Classify Mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "fc6f6268",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model run mode\n",
        "# Options: \n",
        "#   ModelRunMode.TRAIN.value            (Train the model)\n",
        "#   ModelRunMode.CLASSIFY.value         (Classify data)\n",
        "mode = ModelRunMode.TRAIN.value\n",
        "# Prefix for saving the model files\n",
        "model_prefix = \"gru_model_1\"\n",
        "file_path = \"gru_model_weights\"\n",
        "separator_char = \"\\t\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6adb728",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters cell\n",
        "if mode == ModelRunMode.TRAIN.value:\n",
        "    # TRAIN mode: Set parameters for training\n",
        "    input_csv = \"../tarefa_1/clean_input_datasets/dataset1_enh_inputs.csv\"              # CSV file with training inputs (ID, Text)\n",
        "    output_csv = \"../tarefa_1/clean_output_datasets/dataset1_enh_outputs.csv\"           # CSV file with training outputs (ID, Label)\n",
        "    test_size = 0.3                                                                     # Proportion of the dataset to use as test data\n",
        "    validation_size = 0.2                                                               # Proportion of the dataset reserved as validation data\n",
        "    random_state=42                                                                     # Seed for reproducible dataset splitting\n",
        "    verbose = True                                                                      # Verbosity level enabler\n",
        "    max_len = None                                                                      # Max sequence length (padding).\n",
        "    embedding_dim = 32                                                                  # Embedding dimension.\n",
        "    gru_units = 64                                                                      # Number of GRU units.\n",
        "    vocab_size = None                                                                   # Max vocabulary size.\n",
        "    epochs = 80                                                                         # Number of training epochs.\n",
        "    batch_size = 16                                                                     # Batch size.\n",
        "elif mode == ModelRunMode.CLASSIFY.value:\n",
        "    # CLASSIFY mode: Set parameters for classification\n",
        "    input_csv = \"../tarefa_3/classify_input_datasets/dataset2_inputs.csv\"               # CSV file with texts for prediction (ID, Text)\n",
        "    output_csv = \"../tarefa_3/classify_output_datasets/dataset2_outputs_gru_model.csv\"  # CSV file to store prediction result\n",
        "else:\n",
        "    print(\"The selected option is not valid. Options: \\\"train\\\" or \\\"classify\\\"!\")\n",
        "    SystemExit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "7713c68b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method to load and merge two datasets by ID column\n",
        "def merge_data_by_id(input_csv, output_csv, sep=\"\\t\"):\n",
        "    df_in = pd.read_csv(input_csv, sep=sep)\n",
        "    df_out = pd.read_csv(output_csv, sep=sep)\n",
        "\n",
        "    # Remove duplicates or NaNs if needed\n",
        "    df_in.dropna(subset=[\"ID\", \"Text\"], inplace=True)\n",
        "    df_out.dropna(subset=[\"ID\", \"Label\"], inplace=True)\n",
        "    df_in.drop_duplicates(subset=[\"ID\"], inplace=True)\n",
        "    df_out.drop_duplicates(subset=[\"ID\"], inplace=True)\n",
        "\n",
        "    df_merged = pd.merge(df_in, df_out, on=\"ID\", how=\"inner\")\n",
        "    return df_merged\n",
        "\n",
        "# Method for text cleaning\n",
        "def text_cleaning(text):\n",
        "        # Download required NLTK resources\n",
        "        nltk.download('stopwords', quiet=True)\n",
        "        nltk.download('punkt_tab', quiet=True)\n",
        "        nltk.download('wordnet', quiet=True)\n",
        "        # Convert text to lowercase\n",
        "        text = text.lower()\n",
        "        # Remove URLs\n",
        "        text = re.sub(r'http[s]?://\\S+', \"\", text)\n",
        "        # Remove HTML tags\n",
        "        text = re.sub(r\"<[^>]*>\", \"\", text)\n",
        "        # Remove common LaTeX commands\n",
        "        text = re.sub(r\"\\\\[a-zA-Z]+(\\{.*?\\})?\", \"\", text)\n",
        "        # Remove email addresses\n",
        "        text = re.sub(r'\\S+@\\S+', \"\", text)\n",
        "        # Remove punctuation\n",
        "        text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "        # Remove digits\n",
        "        text = re.sub(r\"\\d+\", \"\", text)\n",
        "        # Replace newlines and extra whitespace with a single space\n",
        "        text = re.sub(r\"\\s+\", \" \", text).replace('\\n', \" \")\n",
        "        # Trim leading and trailing whitespace\n",
        "        text = text.strip()\n",
        "        # Tokenize text and remove stopwords using NLTK's English stopwords list\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        # Tokenize text\n",
        "        tokens = word_tokenize(text)\n",
        "        # Remove stopwords\n",
        "        filtered_tokens = [tok for tok in tokens if tok not in stop_words]\n",
        "        # Lemmatize tokens\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        lemmatized_tokens = [lemmatizer.lemmatize(tok) for tok in filtered_tokens]\n",
        "        # Return the cleaned text as a string\n",
        "        return \" \".join(lemmatized_tokens)\n",
        "\n",
        "# Method to convert labels to binary\n",
        "def convert_labels_to_binary_and_get_text(df_merged):\n",
        "    df_merged[\"Label\"] = df_merged[\"Label\"].str.lower().str.strip()\n",
        "    y = np.where(df_merged[\"Label\"] == \"ai\", 1.0, 0.0)\n",
        "    texts = df_merged[\"Text\"].tolist()\n",
        "    return y, texts\n",
        "\n",
        "# Method to plot the learning curves\n",
        "def plot_learning_curves(history):\n",
        "    # Loss\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    if 'val_loss' in history.history:\n",
        "        plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "    plt.title(\"Loss\")\n",
        "    plt.legend()\n",
        "    \n",
        "    # Accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='Train Acc')\n",
        "    if 'val_accuracy' in history.history:\n",
        "        plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
        "    plt.title(\"Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "98f21a18",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method to create a GRU-Based classification model\n",
        "def build_gru_model(vocab_size, max_len, embedding_dim=32, gru_units=64, learning_rate=1e-3):\n",
        "    \"\"\"\n",
        "    Create a GRU-based classification model\n",
        "    \"\"\"\n",
        "    model = keras.Sequential()\n",
        "    \n",
        "    # Embedding layer\n",
        "    model.add(layers.Embedding(input_dim=vocab_size, \n",
        "                               output_dim=embedding_dim, \n",
        "                               input_length=max_len))\n",
        "    \n",
        "    # GRU layer\n",
        "    model.add(layers.GRU(units=gru_units, return_sequences=False))\n",
        "    \n",
        "    # Final output for binary classification\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    \n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "e633c6f6",
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Both `input_dim` and `output_dim` should be positive, Received input_dim = 0 and output_dim = 32",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[21], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m X_train, X_val, y_train, y_val \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39mtest_size, random_state\u001b[38;5;241m=\u001b[39mrandom_state, stratify\u001b[38;5;241m=\u001b[39my)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Create a GRU-Based classification model\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_gru_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgru_units\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgru_units\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Display model sumary\u001b[39;00m\n\u001b[0;32m     27\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
            "Cell \u001b[1;32mIn[20], line 9\u001b[0m, in \u001b[0;36mbuild_gru_model\u001b[1;34m(vocab_size, max_len, embedding_dim, gru_units, learning_rate)\u001b[0m\n\u001b[0;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mSequential()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Embedding layer\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m model\u001b[38;5;241m.\u001b[39madd(\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                           \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                           \u001b[49m\u001b[43minput_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_len\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# GRU layer\u001b[39;00m\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39madd(layers\u001b[38;5;241m.\u001b[39mGRU(units\u001b[38;5;241m=\u001b[39mgru_units, return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
            "File \u001b[1;32m~\\anaconda3\\envs\\ap\\lib\\site-packages\\keras\\dtensor\\utils.py:96\u001b[0m, in \u001b[0;36mallow_initializer_layout.<locals>._wrap_function\u001b[1;34m(layer_instance, *args, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m layout:\n\u001b[0;32m     94\u001b[0m             layout_args[variable_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_layout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m layout\n\u001b[1;32m---> 96\u001b[0m init_method(layer_instance, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Inject the layout parameter after the invocation of __init__()\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layout_param_name, layout \u001b[38;5;129;01min\u001b[39;00m layout_args\u001b[38;5;241m.\u001b[39mitems():\n",
            "File \u001b[1;32m~\\anaconda3\\envs\\ap\\lib\\site-packages\\keras\\layers\\core\\embedding.py:132\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[1;34m(self, input_dim, output_dim, embeddings_initializer, embeddings_regularizer, activity_regularizer, embeddings_constraint, mask_zero, input_length, **kwargs)\u001b[0m\n\u001b[0;32m    130\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mNone\u001b[39;00m,)\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_dim \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m output_dim \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 132\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBoth `input_dim` and `output_dim` should be positive, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived input_dim = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand output_dim = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    136\u001b[0m     )\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m base_layer_utils\u001b[38;5;241m.\u001b[39mv2_dtype_behavior_enabled()\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n\u001b[0;32m    140\u001b[0m ):\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;66;03m# In TF1, the dtype defaults to the input dtype which is typically\u001b[39;00m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;66;03m# int32, so explicitly set it to floatx\u001b[39;00m\n\u001b[0;32m    143\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mfloatx()\n",
            "\u001b[1;31mValueError\u001b[0m: Both `input_dim` and `output_dim` should be positive, Received input_dim = 0 and output_dim = 32"
          ]
        }
      ],
      "source": [
        "if mode == ModelRunMode.TRAIN.value:\n",
        "    # Load data\n",
        "    df_merged = merge_data_by_id(input_csv, output_csv, sep=separator_char)\n",
        "\n",
        "    # Text cleaning\n",
        "    df_merged[\"Text\"] = df_merged[\"Text\"].apply(text_cleaning)\n",
        "\n",
        "    # Convert Label: \"AI\" -> 1, \"Human\" -> 0\n",
        "    y, texts = convert_labels_to_binary_and_get_text(df_merged)\n",
        "\n",
        "    # Creating tokenizer\n",
        "    tokenizer = preprocessing.text.Tokenizer(num_words=vocab_size, oov_token=\"<UNK>\")\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "\n",
        "   # Convert to sequences\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    # Pad sequences\n",
        "    X = preprocessing.sequence.pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "    # Train/test split\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
        "\n",
        "    # Create a GRU-Based classification model\n",
        "    model = build_gru_model(vocab_size=vocab_size, max_len=max_len, embedding_dim=embedding_dim, gru_units=gru_units)\n",
        "\n",
        "    # Display model sumary\n",
        "    model.summary()\n",
        "\n",
        "    # Training\n",
        "    history = model.fit( X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
        "\n",
        "    # Plot model learning curves\n",
        "    plot_learning_curves(history)\n",
        "\n",
        "    # Construct the file paths\n",
        "    model_path = os.path.join(file_path, f\"{model_prefix}_model\")\n",
        "    tokenizer_path = os.path.join(file_path, f\"{model_prefix}_tokenizer\")\n",
        "\n",
        "    # Save the model weights\n",
        "    print(\"Saving model to:\", model_path)\n",
        "    model.save(model_path)\n",
        "\n",
        "    print(\"Saving tokenizer to:\", tokenizer_path)\n",
        "    with open(tokenizer_path, \"wb\") as f:\n",
        "        pickle.dump(tokenizer, f)\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    loss_val, acc_val = model.evaluate(X_val, y_val, verbose=verbose)\n",
        "    print(f\"Validation Loss: {loss_val:.4f}, Validation Accuracy: {acc_val:.4f}\")\n",
        "\n",
        "if mode == ModelRunMode.CLASSIFY.value:\n",
        "    # Construct the file paths\n",
        "    model_path = os.path.join(file_path, f\"{model_prefix}_model\")\n",
        "    tokenizer_path = os.path.join(file_path, f\"{model_prefix}_tokenizer\")\n",
        "\n",
        "    # Loading model\n",
        "    model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "    print(\"Loading tokenizer:\", tokenizer_path)\n",
        "    with open(tokenizer_path, \"rb\") as f:\n",
        "        tokenizer = pickle.load(f)\n",
        "\n",
        "    # Reading input for classification\n",
        "    df_new = pd.read_csv(input_csv, sep=separator_char)\n",
        "    if \"ID\" not in df_new.columns or \"Text\" not in df_new.columns:\n",
        "        raise ValueError(\"Input CSV must have 'ID' and 'Text' columns for classification.\")\n",
        "\n",
        "    # Clean text\n",
        "    df_new[\"Text\"] = df_new[\"Text\"].apply(text_cleaning)\n",
        "    texts = df_new[\"Text\"].tolist()\n",
        "\n",
        "    # Convert to sequences\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "    # Pad\n",
        "    X_new = preprocessing.sequence.pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "    # Predict\n",
        "    preds = model.predict(X_new)\n",
        "    pred_bin = (preds >= 0.5).astype(int).flatten()\n",
        "    pred_label = np.where(pred_bin == 1, \"AI\", \"Human\")\n",
        "\n",
        "    # Save result\n",
        "    df_out = pd.DataFrame({\"ID\": df_new[\"ID\"], \"Label\": pred_label})\n",
        "    df_out.to_csv(output_csv, sep=separator_char, index=False)\n",
        "    print(f\"Predictions saved to {output_csv}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
