{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e117c95a",
   "metadata": {},
   "source": [
    "# Bidirectional Encoder Representations from Transformers (BERT) Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7f3362",
   "metadata": {},
   "source": [
    "```md\n",
    "@authors: Grupo 03\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00553163",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 23:25:22.779449: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-23 23:25:22.792779: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742772322.807279   51676 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742772322.811841   51676 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742772322.823207   51676 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742772322.823227   51676 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742772322.823229   51676 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742772322.823230   51676 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-23 23:25:22.826567: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Notebook Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cee9d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model run enum\n",
    "class ModelRunMode(Enum):\n",
    "    \"\"\"\n",
    "    Enumeration of Model Run Mode.\n",
    "    \"\"\"\n",
    "    TRAIN           = \"train\"           # Train Mode\n",
    "    CLASSIFY        = \"classify\"        # Classify Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc6f6268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model run mode\n",
    "# Options: \n",
    "#   ModelRunMode.TRAIN.value            (Train the model)\n",
    "#   ModelRunMode.CLASSIFY.value         (Classify data)\n",
    "mode = ModelRunMode.CLASSIFY.value\n",
    "# Prefix for saving the model files\n",
    "model_prefix = \"llm_bert_model\"\n",
    "file_path = \"llm_bert_model_weights\"\n",
    "separator_char = \"\\t\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6adb728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters cell\n",
    "if mode == ModelRunMode.TRAIN.value:\n",
    "    # TRAIN mode: Set parameters for training\n",
    "    input_csv = \"../Tarefas/tarefa_1/clean_input_datasets/dataset1_enh_inputs_v2.csv\"           # CSV file with training inputs (ID, Text)\n",
    "    output_csv = \"../Tarefas/tarefa_1/clean_output_datasets/dataset1_enh_outputs_v2.csv\"        # CSV file with training outputs (ID, Label)\n",
    "    test_size = 0.3                                                                             # Proportion of the dataset to use as test data\n",
    "    random_state=42                                                                             # Seed for reproducible dataset splitting\n",
    "elif mode == ModelRunMode.CLASSIFY.value:\n",
    "    # CLASSIFY mode: Set parameters for classification\n",
    "    input_csv = \"classify_input_datasets/dataset3_inputs.csv\"                                   # CSV file with texts for prediction (ID, Text)\n",
    "    output_csv = \"classify_output_datasets/dataset3_outputs_llm_bert_model-s1.csv\"              # CSV file to store prediction result\n",
    "else:\n",
    "    print(\"The selected option is not valid. Options: \\\"train\\\" or \\\"classify\\\"!\")\n",
    "    SystemExit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7713c68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to load and merge two datasets by ID column\n",
    "def merge_data_by_id(input_csv, output_csv, sep=\"\\t\"):\n",
    "    df_in = pd.read_csv(input_csv, sep=sep)\n",
    "    df_out = pd.read_csv(output_csv, sep=sep)\n",
    "\n",
    "    # Remove duplicates or NaNs if needed\n",
    "    df_in.dropna(subset=[\"ID\", \"Text\"], inplace=True)\n",
    "    df_out.dropna(subset=[\"ID\", \"Label\"], inplace=True)\n",
    "    df_in.drop_duplicates(subset=[\"ID\"], inplace=True)\n",
    "    df_out.drop_duplicates(subset=[\"ID\"], inplace=True)\n",
    "\n",
    "    df_merged = pd.merge(df_in, df_out, on=\"ID\", how=\"inner\")\n",
    "    return df_merged\n",
    "\n",
    "# Method for text cleaning\n",
    "def text_cleaning(text, stopwords = False):\n",
    "    def normalize(text):\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove numbers, special characters, e o caractere '\n",
    "        text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "        # Replace multiple spaces with a single space\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        # Add start and end sequence tokens\n",
    "        # text = 'startseq ' + \" \".join([word for word in text.split() if len(word) > 1]) + ' endseq'\n",
    "        return text\n",
    "\n",
    "    def remove_stopwords(text):\n",
    "        stopwords = [\n",
    "        \"the\", \"of\", \"and\", \"in\", \"to\", \"is\", \"a\", \"that\", \"for\", \"are\", \"on\", \"with\", \n",
    "        \"as\", \"at\", \"by\", \"from\", \"this\", \"it\", \"an\", \"be\", \"or\", \"which\", \"was\", \"were\"\n",
    "        ]\n",
    "        text = ' '.join([word for word in text.split() if word not in stopwords])\n",
    "        return text\n",
    "    \n",
    "    text = normalize(text)\n",
    "    if stopwords:\n",
    "        text = remove_stopwords(text)\n",
    "    return text\n",
    "\n",
    "# Method to convert labels to binary\n",
    "def convert_labels_to_binary_and_text(df_merged):\n",
    "    df_merged[\"Label\"] = df_merged[\"Label\"].str.lower().str.strip()\n",
    "    y = np.where(df_merged[\"Label\"] == \"ai\", 1, 0)\n",
    "    texts = df_merged[\"Text\"].tolist()\n",
    "    return y, texts\n",
    "\n",
    "# Method to plot the learning curves\n",
    "def plot_learning_curves(history):\n",
    "    # Loss\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    if 'val_loss' in history.history:\n",
    "        plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "    plt.title(\"Loss\")\n",
    "    plt.legend()\n",
    "    \n",
    "    # Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "    if 'val_accuracy' in history.history:\n",
    "        plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Method to check label distribution\n",
    "def check_label_distribution(df_merged):\n",
    "    label_counts = df_merged[\"Label\"].value_counts(dropna=False)\n",
    "    print(\"Label distribution:\\n\", label_counts)\n",
    "\n",
    "# Method to print the first 5 cleaned texts\n",
    "def debug_text_cleaning(df_merged):\n",
    "    for i in range(min(5, len(df_merged))):\n",
    "        print(df_merged[\"Text\"].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98f21a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bert(input_csv, output_csv, model_ckpt=\"bert-base-uncased\", output_dir=file_path, model_prefix=model_prefix, num_train_epochs=3, batch_size=8, test_size=0.2, learning_rate=1e-3, random_state=48):\n",
    "    print(\"[INFO] Loading data...\")\n",
    "    df_merged = merge_data_by_id(input_csv, output_csv, sep=separator_char)\n",
    "    \n",
    "    print(\"[INFO] Checking label distribution...\")\n",
    "    check_label_distribution(df_merged=df_merged)\n",
    "    \n",
    "    print(\"[INFO] Cleaning text...\")\n",
    "    df_merged[\"Text\"] = df_merged[\"Text\"].apply(text_cleaning)\n",
    "    \n",
    "    print(\"[INFO] Debugging cleaned text...\")\n",
    "    debug_text_cleaning(df_merged)\n",
    "    \n",
    "    print(\"[INFO] Converting labels and text...\")\n",
    "    labels, texts = convert_labels_to_binary_and_text(df_merged)\n",
    "    \n",
    "    print(\"[INFO] Splitting dataset...\")\n",
    "    X_train_texts, X_val_texts, y_train, y_val = train_test_split(texts, labels, test_size=test_size, random_state=random_state, stratify=labels)\n",
    "    \n",
    "    print(\"[INFO] Initializing tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "    \n",
    "    print(\"[INFO] Calculating token lengths...\")\n",
    "    raw_train_encodings = tokenizer(X_train_texts, add_special_tokens=True, truncation=False, padding=False)\n",
    "    token_lengths = [len(seq) for seq in raw_train_encodings[\"input_ids\"]]\n",
    "    max_length = int(np.percentile(token_lengths, 90))\n",
    "    print(f\"[INFO] Selected max_length: {max_length}\")\n",
    "    \n",
    "    print(\"[INFO] Tokenizing train and validation datasets...\")\n",
    "    train_encodings = tokenizer(X_train_texts, truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "    val_encodings = tokenizer(X_val_texts, truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "    \n",
    "    print(\"[INFO] Converting data to TensorFlow datasets...\")\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(batch_size)\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings), y_val)).batch(batch_size)\n",
    "    \n",
    "    print(\"[INFO] Loading model...\")\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=1)\n",
    "    \n",
    "    print(\"[INFO] Compiling model...\")\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    loss = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "    \n",
    "    print(\"[INFO] Setting up early stopping...\")\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "    \n",
    "    print(\"[INFO] Starting training...\")\n",
    "    history = model.fit(train_dataset, validation_data=val_dataset, epochs=num_train_epochs, callbacks=[early_stopping])\n",
    "    \n",
    "    print(\"[INFO] Evaluating model...\")\n",
    "    val_loss, val_acc = model.evaluate(val_dataset)\n",
    "    print(f\"\\n[RESULT] Validation Accuracy: {val_acc:.4f} | Validation Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    print(\"[INFO] Model summary:\")\n",
    "    model.summary()\n",
    "    \n",
    "    print(\"[INFO] Plotting learning curves...\")\n",
    "    plot_learning_curves(history)\n",
    "    \n",
    "    print(\"[INFO] Preparing to save model...\")\n",
    "    if os.path.exists(output_dir):\n",
    "        print(\"[INFO] Removing existing output directory...\")\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    model_path = os.path.join(output_dir, f\"{model_prefix}_model\")\n",
    "    tokenizer_path = os.path.join(output_dir, f\"{model_prefix}_tokenizer\")\n",
    "    config_path = os.path.join(output_dir, f\"{model_prefix}_config.json\")\n",
    "    \n",
    "    print(\"[INFO] Saving model...\")\n",
    "    model.save_pretrained(model_path)\n",
    "    \n",
    "    print(\"[INFO] Saving tokenizer...\")\n",
    "    tokenizer.save_pretrained(tokenizer_path)\n",
    "    \n",
    "    print(\"[INFO] Saving configuration...\")\n",
    "    config_data = {\n",
    "        \"model_ckpt\": model_ckpt,\n",
    "        \"num_train_epochs\": num_train_epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"max_length\": max_length\n",
    "    }\n",
    "    with open(config_path, \"w\") as f:\n",
    "        json.dump(config_data, f)\n",
    "    \n",
    "    print(f\"[INFO] Training completed. Model, tokenizer, and config saved to {output_dir}!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43fa4fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification function\n",
    "def classify_bert(input_csv, output_csv, output_dir=\"llm_bert_model_weights\", separator_char=separator_char):\n",
    "    # Construct the file paths\n",
    "    model_path = os.path.join(output_dir, f\"{model_prefix}_model\")\n",
    "    tokenizer_path = os.path.join(output_dir, f\"{model_prefix}_tokenizer\")\n",
    "    config_path = os.path.join(output_dir, f\"{model_prefix}_config.json\")\n",
    "\n",
    "    # Load the model\n",
    "    print(f\"Loading model from: {output_dir}\")\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "    \n",
    "    # Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "    # Loading configuration\n",
    "    print(\"Loading configuration from:\", config_path)\n",
    "    with open(config_path, \"r\") as f:\n",
    "         config_data = json.load(f)\n",
    "\n",
    "    # Retrieve the saved configuration max_len\n",
    "    max_length = config_data[\"max_length\"]\n",
    "\n",
    "    # Read the input CSV\n",
    "    df_input = pd.read_csv(input_csv, sep=separator_char)\n",
    "    if \"ID\" not in df_input.columns or \"Text\" not in df_input.columns:\n",
    "        raise ValueError(\"Input CSV must have 'ID' and 'Text' columns for classification.\")\n",
    "    \n",
    "    # Extract texts\n",
    "    texts = df_input[\"Text\"].astype(str).tolist()\n",
    "    \n",
    "    # Tokenize\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length)\n",
    "    \n",
    "    # Create tf.data.Dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(dict(encodings))\n",
    "    dataset = dataset.batch(16)\n",
    "    \n",
    "    # Predict probabilities (logits => sigmoid)\n",
    "    outputs = model.predict(dataset)\n",
    "\n",
    "    if isinstance(outputs, dict) and \"logits\" in outputs:\n",
    "        logits = outputs[\"logits\"]\n",
    "    else:\n",
    "        logits = outputs\n",
    "    \n",
    "    # Convert logits to probabilities\n",
    "    probs = tf.nn.sigmoid(logits).numpy()\n",
    "    \n",
    "    # Threshold => \"AI\" vs \"Human\"\n",
    "    pred_bin = (probs >= 0.5).astype(int)\n",
    "    pred_labels = [\"AI\" if val == 1 else \"Human\" for val in pred_bin]\n",
    "\n",
    "\n",
    "    \n",
    "    # Save result\n",
    "    df_out = pd.DataFrame({\"ID\": df_input[\"ID\"], \"Label\": pred_labels})\n",
    "    df_out.to_csv(output_csv, sep=separator_char, index=False)\n",
    "    print(f\"Predictions saved to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e633c6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: llm_bert_model_weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 23:25:25.009872: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at llm_bert_model_weights/llm_bert_model_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuration from: llm_bert_model_weights/llm_bert_model_config.json\n",
      "7/7 [==============================] - 19s 2s/step\n",
      "Predictions saved to classify_output_datasets/dataset3_outputs_llm_bert_model.csv\n"
     ]
    }
   ],
   "source": [
    "if mode == ModelRunMode.TRAIN.value:\n",
    "    # Check if Tensorflow is listing available GPUs (if not, continue with CPU)\n",
    "    print(\"Tensorflow List of GPUs:\", tf.config.list_physical_devices('GPU'))\n",
    "    \n",
    "    # Train model\n",
    "    train_bert(input_csv=input_csv, output_csv=output_csv, model_ckpt=\"bert-base-uncased\", output_dir=file_path, model_prefix=model_prefix, num_train_epochs=5, batch_size=8, test_size=test_size, learning_rate=1e-5, random_state=random_state)\n",
    "\n",
    "if mode == ModelRunMode.CLASSIFY.value:\n",
    "    # Classification\n",
    "    classify_bert(input_csv=input_csv, output_csv=output_csv, output_dir=file_path, separator_char=separator_char)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
